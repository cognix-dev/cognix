"""
CLI ãƒãƒ£ãƒƒãƒˆå‡¦ç†ã¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç®¡ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€ãƒãƒ£ãƒƒãƒˆå‡¦ç†ã¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç®¡ç†æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚
å…·ä½“çš„ã«ã¯ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’å«ã¿ã¾ã™ï¼š

1. ãƒãƒ£ãƒƒãƒˆå¯¾è©±å‡¦ç† (handle_chat)
2. ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç®¡ç† (think, plan, write)
3. ã‚»ãƒŸã‚ªãƒ¼ãƒˆå®Ÿè£…æ©Ÿèƒ½
"""

import os
import sys
import json
import time
from typing import Dict, List, Set, Any, Optional, Union, Tuple
from datetime import datetime
from pathlib import Path

# å¿…é ˆä¾å­˜
from cognix.cli_shared import CLIModuleBase
from cognix.reference_parser import ReferenceParser
from cognix.prompt_templates import prompt_manager
from cognix.ui import StatusIndicator, Icon, FileIconMapper, get_risk_icon  # Phase 1: UI improvements

# Zen Step HUDï¼ˆä»»æ„ãƒ»å­˜åœ¨ã—ãªã‘ã‚Œã°ç„¡åŠ¹ï¼‰
try:
    from cognix.progress_zen import StepHUD
except Exception:
    StepHUD = None

from cognix.logger import console, err_console, logger  # stdout/stderr separation + debug log
from rich.text import Text
from cognix.theme_zen import GREEN, CYAN, YELLOW, MAGENTA, RESET  # ANSI color codes

class ChatWorkflowModule(CLIModuleBase):
    """ãƒãƒ£ãƒƒãƒˆå‡¦ç†ã¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç®¡ç†æ©Ÿèƒ½ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
    
    ãƒ¡ã‚¤ãƒ³ã®ãƒãƒ£ãƒƒãƒˆå¯¾è©±å‡¦ç†ã¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç®¡ç†ï¼ˆthink, plan, writeï¼‰ã‚’æ‹…å½“ã—ã¾ã™ã€‚
    ã¾ãŸã€ã‚»ãƒŸã‚ªãƒ¼ãƒˆå®Ÿè£…æ©Ÿèƒ½ã‚‚æä¾›ã—ã¾ã™ã€‚
    """
    
    def __init__(self):
        """åŸºæœ¬åˆæœŸåŒ– - å¼•æ•°ãªã—
        
        ä¾å­˜ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¯set_dependenciesã§å¾Œã‹ã‚‰è¨­å®šã—ã¾ã™
        """
        # è¦ªã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ– - å¼•æ•°ãªã—ã§å‘¼ã³å‡ºã—
        super().__init__()
        
        # ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å›ºæœ‰ã®åˆæœŸåŒ–
        # (ä¾å­˜ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«ä¾å­˜ã—ãªã„åˆæœŸåŒ–ã®ã¿)
    
    def set_dependencies(self, cli):
        """CLI ã‹ã‚‰ä¾å­˜ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ³¨å…¥ã™ã‚‹
        
        ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹ã§å…±é€šä¾å­˜ã‚’ã‚»ãƒƒãƒˆã—ã€
        è¿½åŠ ã§ repository_analyzer / impact_analyzer / related_finder ã‚‚æ˜ç¤ºçš„ã«ã‚³ãƒ”ãƒ¼
        """
        # ã¾ãšãƒ™ãƒ¼ã‚¹å®Ÿè£…ã§ config, memory, context, llm_manager ãªã©ã‚’ã‚³ãƒ”ãƒ¼
        super().set_dependencies(cli)
        
        # è¿½åŠ ä¾å­˜ã‚’æ˜ç¤ºçš„ã«ã‚³ãƒ”ãƒ¼
        self.repository_analyzer = getattr(cli, 'repository_analyzer', None)
        self.impact_analyzer = getattr(cli, 'impact_analyzer', None)
        self.related_finder = getattr(cli, 'related_finder', None)
        self.session_manager = getattr(cli, 'session_manager', None)  # ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†
    
    def handle_chat(self, user_input: str):
        """ãƒãƒ£ãƒƒãƒˆå…¥åŠ›ã‚’å‡¦ç†ã™ã‚‹
        
        Args:
            user_input: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
        """
        try:
            # è¤‡æ•°è¡Œãƒãƒ£ãƒƒãƒˆå…¥åŠ›ã®ã‚µãƒãƒ¼ãƒˆ
            if user_input.strip().upper() == "MULTI":
                err_console.print("ğŸ’¬ Multi-line chat mode")
                user_input = self.get_multiline_input(
                    "Enter your question or message:",
                    allow_empty=False
                )
                if not user_input:
                    return

            err_console.print("Thinking...")

            # å‚ç…§è¨˜æ³•ã®è§£æã‚’è¿½åŠ 
            reference_parser = ReferenceParser(self.context)
            parsed_refs = reference_parser.parse(user_input)

            # å¤‰æ•°ã‚’äº‹å‰ã«åˆæœŸåŒ–ï¼ˆé‡è¦ï¼šå‚ç…§è¨˜æ³•ãŒãªã„å ´åˆã§ã‚‚å®šç¾©ï¼‰
            has_errors = False
            has_valid_content = False
            error_messages = []

            # å‚ç…§è¨˜æ³•ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã®å‡¦ç†
            if parsed_refs.has_references:
                err_console.print("\nğŸ” Processing references...")
                
                # ãƒ•ã‚¡ã‚¤ãƒ«å‚ç…§ã‚’ãƒã‚§ãƒƒã‚¯
                for file_ref in parsed_refs.files:
                    if not file_ref.exists:
                        error_messages.append(f"âŒ File not found: {file_ref.filename}")
                        has_errors = True
                    else:
                        has_valid_content = True
                
                # é–¢æ•°å‚ç…§ã‚’ãƒã‚§ãƒƒã‚¯
                for func_ref in parsed_refs.functions:
                    if not func_ref.found:
                        error_messages.append(f"âŒ Function not found: #{func_ref.function_name}")
                        has_errors = True
                    else:
                        has_valid_content = True
                                
                # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºï¼ˆãŸã ã—å‡¦ç†ã¯ç¶™ç¶šï¼‰
                if error_messages:
                    for error_msg in error_messages:
                        err_console.print(error_msg)
                
                # æœ‰åŠ¹ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚ã‚‹å ´åˆã¯å‡¦ç†ã‚’ç¶™ç¶š
                if has_valid_content:
                    if parsed_refs.context_text:
                        err_console.print(parsed_refs.context_text)
                        err_console.print()
                elif has_errors and not has_valid_content:
                    # å…¨ã¦ã®å‚ç…§ãŒå¤±æ•—ã—ãŸå ´åˆã®ã¿ä¸­æ–­
                    err_console.print("\nAll references failed. Please check your reference syntax.")
                    return

            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆä¿®æ­£ï¼šæ­£ã—ã„ã‚·ã‚°ãƒãƒãƒ£ã§å‘¼ã³å‡ºã—ï¼‰
            base_context = self.context.generate_context_for_prompt(user_input)
            
            # å‚ç…§è¨˜æ³•ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆï¼ˆæœ‰åŠ¹ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰
            enhanced_context = base_context
            if parsed_refs.has_references and parsed_refs.context_text:
                enhanced_context = f"{base_context}\n\n=== Referenced Content ===\n{parsed_refs.context_text}"

            # ä¼šè©±å±¥æ­´ã®ç®¡ç†ï¼ˆä¿®æ­£ï¼šæ­£ã—ã„ãƒ¡ã‚½ãƒƒãƒ‰åã§å‘¼ã³å‡ºã—ï¼‰
            conversation_history = []
            if hasattr(self, 'memory') and self.memory:
                conversation_history = self.memory.get_conversation_context(5)
                
            # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å¼·åŒ–
            base_system_prompt = self.config.get_system_prompt("default") if hasattr(self, 'config') else ""

            # å‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼æä¾›ã®ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ï¼‰
            reference_context = ""
            if parsed_refs.has_references and has_valid_content:
                reference_context = f"""CRITICAL: The user has provided specific file content below using reference notation.
The following content is the ACTUAL file content from the user's project:
{parsed_refs.context_text}
You MUST base your analysis on this exact content shown above. 
Do NOT make assumptions about what the file might contain based on its name.
The content displayed above is the current, real state of the user's files."""

            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            session_context = ""
            if hasattr(self, 'session_manager') and self.session_manager:
                stats = self.session_manager.get_session_stats()
                if stats and stats.get('total_entries', 0) > 0:
                    session_context = f"""

IMPORTANT COGNIX SESSION CONTEXT:
- You are operating in Cognix, which has advanced session management
- This session has been restored with {stats['total_entries']} previous interactions
- All conversation history and context from previous sessions is available to you
- When users reference past conversations, you can naturally access them from your memory
- Do NOT say information "won't be preserved" - in Cognix, it IS preserved across sessions
- Respond naturally as if this is one continuous conversation"""

            # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰ï¼ˆå‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æœ€å„ªå…ˆé…ç½®ï¼‰
            if reference_context:
                enhanced_system_prompt = reference_context + "\n\n" + base_system_prompt + session_context
            else:
                enhanced_system_prompt = base_system_prompt + session_context

            # LLMå¿œç­”ç”Ÿæˆ
            if hasattr(self, 'llm_manager') and self.llm_manager:
                if self.config.get("stream_responses", True):
                    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”
                    stream_gen = self.llm_manager.stream_response(
                        prompt=user_input,
                        context=enhanced_context,
                        system_prompt=enhanced_system_prompt,
                        conversation_history=conversation_history
                    )
                    
                    model_prefix = self._get_model_prefix()
                    full_response = self._stream_with_typewriter(stream_gen, model_prefix)
                else:
                    # é€šå¸¸å¿œç­”
                    response = self.llm_manager.generate_response(
                        prompt=user_input,
                        context=enhanced_context,
                        system_prompt=enhanced_system_prompt,
                        conversation_history=conversation_history
                    )
                    full_response = response.content if hasattr(response, 'content') else str(response)
                    
                    model_prefix = self._get_model_prefix()
                    self._display_with_typewriter(full_response, model_prefix)
            else:
                full_response = "Error: LLM Manager not available"
                err_console.print(full_response)

            # ãƒ¡ãƒ¢ãƒªã«ä¿å­˜
            if hasattr(self, 'memory') and self.memory:
                self.memory.add_entry(
                    user_prompt=user_input,
                    claude_reply=full_response,
                    model_used=getattr(self.llm_manager, 'current_model', 'unknown'),
                    metadata={
                        "has_references": parsed_refs.has_references,
                        "referenced_files": [f.filename for f in parsed_refs.files if f.exists],
                        "referenced_functions": [f.function_name for f in parsed_refs.functions if f.found],
                        "reference_errors": len(error_messages) if error_messages else 0
                    }
                )

            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜ï¼ˆè¿½åŠ ï¼‰
            if hasattr(self, 'session_manager') and self.session_manager:
                self.session_manager.add_entry(
                    user_input=user_input,
                    ai_response=full_response,
                    model_used=getattr(self.llm_manager, 'current_model', 'unknown'),
                    command_type="chat",
                    target_files=None,
                    metadata={
                        "has_references": parsed_refs.has_references,
                        "referenced_files": [f.filename for f in parsed_refs.files if f.exists],
                        "referenced_functions": [f.function_name for f in parsed_refs.functions if f.found],
                        "reference_errors": len(error_messages) if error_messages else 0
                    },
                    workflow_state=None
                )

        except Exception as e:
            if hasattr(self, 'error_handler') and self.error_handler:
                self.error_handler.handle_error(e, "chat interaction")
            else:
                err_console.print(f"Error in chat: {e}")

    def cmd_think(self, args):
        """Problem analysis (Step 1 of thinkâ†’planâ†’write workflow)"""
        # å¼•æ•°å‡¦ç†
        if isinstance(args, str):
            args = args.strip()
            goal = args if args else None
        elif isinstance(args, list):
            goal = " ".join(args).strip() if args else None
        else:
            goal = None
            
        # ç›®æ¨™ã®å–å¾—
        if not goal:
            goal = self.get_multiline_input(
                "What would you like me to think about?",
                allow_empty=False
            )
            if not goal:
                err_console.print("Operation cancelled.")
                return
        
        err_console.print(f"Analyzing: {goal}")
        
        # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çŠ¶æ…‹ã®åˆæœŸåŒ–
        if hasattr(self, 'shared_state') and self.shared_state:
            self.shared_state.workflow_state["current_goal"] = goal
            self.shared_state.workflow_state["think_result"] = None
            self.shared_state.workflow_state["plan_result"] = None
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆä¿®æ­£ï¼šuser_promptã‚’æ¸¡ã™ï¼‰
        context = self.context.generate_context_for_prompt(goal)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        try:
            prompt_data = prompt_manager.render_prompt(
                "problem_analysis",
                {"goal": goal}
            )
            
            if not prompt_data:
                err_console.print("Error: Failed to generate analysis prompt")
                return
                
            prompt = prompt_data["prompt"]
            system_prompt = prompt_data["system_prompt"]
            
        except Exception as e:
            err_console.print(f"Error generating prompt: {e}")
            return
        
        try:
            # LLMå¿œç­”ç”Ÿæˆ
            if hasattr(self, 'llm_manager') and self.llm_manager:
                response = self.llm_manager.generate_response(
                    prompt=prompt,
                    context=context,
                    system_prompt=system_prompt
                )
                response_content = response.content if hasattr(response, 'content') else str(response)
            else:
                response_content = "Error: LLM Manager not available"
                err_console.print(response_content)
                return
                
            # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çŠ¶æ…‹ã®æ›´æ–°
            if hasattr(self, 'shared_state') and self.shared_state:
                self.shared_state.workflow_state["think_result"] = response_content
            
            # çµæœè¡¨ç¤º
            model_prefix = self._get_model_prefix()
            self._display_with_typewriter(response_content, model_prefix)
            
            # ãƒ¡ãƒ¢ãƒªã«ä¿å­˜
            if hasattr(self, 'memory') and self.memory:
                self.memory.add_entry(
                    user_prompt=f"/think {goal}",
                    claude_reply=response_content,
                    model_used=getattr(self.llm_manager, 'current_model', 'unknown'),
                    metadata={"command": "think", "goal": goal}
                )
                
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜
            if hasattr(self, 'session_manager') and self.session_manager:
                self.session_manager.add_entry(
                    user_input=f"/think {goal}",
                    ai_response=response_content,
                    model_used=getattr(self.llm_manager, 'current_model', 'unknown'),
                    command_type="think",
                    metadata={"goal": goal}
                )
                
            err_console.print(Text.from_ansi(f"\n Next step: {GREEN}/plan{RESET}"))
            
        except Exception as e:
            if hasattr(self, 'error_handler') and self.error_handler:
                self.error_handler.handle_error(e, "think command")
            else:
                err_console.print(f"Error in think command: {e}")

    def cmd_plan(self, args):
        """Implementation planning (Step 2 of thinkâ†’planâ†’write workflow)"""
        # å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã®çµæœã‚’ç¢ºèª
        if hasattr(self, 'shared_state') and self.shared_state:
            if not self.shared_state.workflow_state.get("think_result"):
                err_console.print(StatusIndicator.error("Please run /think first to analyze your goal."))
                return
                
            goal = self.shared_state.workflow_state.get("current_goal", "")
            think_result = self.shared_state.workflow_state.get("think_result", "")
        else:
            err_console.print("Error: Workflow state not available")
            return
            
        err_console.print(f"ğŸ“‹ Creating implementation plan for: {goal}")
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆä¿®æ­£ï¼šuser_promptã‚’æ¸¡ã™ï¼‰
        context = self.context.generate_context_for_prompt(goal)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        prompt_data = prompt_manager.render_prompt(
            "implementation_plan",
            {
                "goal": goal,
                "analysis": think_result
            }
        )
        prompt = prompt_data["prompt"]
        system_prompt = prompt_data["system_prompt"]
        
        try:
            # LLMå¿œç­”ç”Ÿæˆ
            if hasattr(self, 'llm_manager') and self.llm_manager:
                response = self.llm_manager.generate_response(
                    prompt=prompt,
                    context=context,
                    system_prompt=system_prompt
                )
                response_content = response.content if hasattr(response, 'content') else str(response)
            else:
                response_content = "Error: LLM Manager not available"
                err_console.print(response_content)
                return
                
            # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çŠ¶æ…‹ã®æ›´æ–°
            if hasattr(self, 'shared_state') and self.shared_state:
                self.shared_state.workflow_state["plan_result"] = response_content
            
            # çµæœè¡¨ç¤º
            model_prefix = self._get_model_prefix()
            self._display_with_typewriter(response_content, model_prefix)
            
            # ãƒ¡ãƒ¢ãƒªã«ä¿å­˜
            if hasattr(self, 'memory') and self.memory:
                self.memory.add_entry(
                    user_prompt="/plan",
                    claude_reply=response_content,
                    model_used=getattr(self.llm_manager, 'current_model', 'unknown'),
                    metadata={"command": "plan", "goal": goal}
                )
                
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜
            if hasattr(self, 'session_manager') and self.session_manager:
                self.session_manager.add_entry(
                    user_input="/plan",
                    ai_response=response_content,
                    model_used=getattr(self.llm_manager, 'current_model', 'unknown'),
                    command_type="plan",
                    metadata={"goal": goal}
                )
                
            err_console.print(Text.from_ansi(f"\n Next step: {GREEN}/write [filename]{RESET}"))
            
        except Exception as e:
            if hasattr(self, 'error_handler') and self.error_handler:
                self.error_handler.handle_error(e, "plan command")
            else:
                err_console.print(f"Error in plan command: {e}")

    def cmd_write(self, args):
        """Code generation (Step 3 of thinkâ†’planâ†’write workflow)"""
        # å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã®çµæœã‚’ç¢ºèª
        if hasattr(self, 'shared_state') and self.shared_state:
            if not self.shared_state.workflow_state.get("plan_result"):
                err_console.print(StatusIndicator.error("Please run /plan first to create an implementation plan."))
                return
                
            goal = self.shared_state.workflow_state.get("current_goal", "")
            think_result = self.shared_state.workflow_state.get("think_result", "")
            plan_result = self.shared_state.workflow_state.get("plan_result", "")
        else:
            err_console.print("Error: Workflow state not available")
            return
            
        # ãƒ•ã‚¡ã‚¤ãƒ«åã®å–å¾—
        if isinstance(args, str):
            filename = args.strip()
        elif isinstance(args, list):
            filename = " ".join(args).strip()
        else:
            filename = ""
            
        if not filename:
            filename = input("Enter filename to write to: ")
            if not filename:
                err_console.print("Operation cancelled.")
                return
                
        err_console.print(f"âœï¸ Writing implementation to: {filename}")
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆä¿®æ­£ï¼šuser_promptã‚’æ¸¡ã™ï¼‰
        context = self.context.generate_context_for_prompt(f"{goal}\n{filename}")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        prompt_data = prompt_manager.render_prompt(
            "code_generation",
            {
                "goal": goal,
                "analysis": think_result,
                "plan": plan_result,
                "additional_context": f"Target file: {filename}"
            }
        )
        prompt = prompt_data["prompt"]
        system_prompt = prompt_data["system_prompt"]
        
        try:
            # LLMå¿œç­”ç”Ÿæˆ
            if hasattr(self, 'llm_manager') and self.llm_manager:
                response = self.llm_manager.generate_response(
                    prompt=prompt,
                    context=context,
                    system_prompt=system_prompt
                )
                response_content = response.content if hasattr(response, 'content') else str(response)
            else:
                response_content = "Error: LLM Manager not available"
                err_console.print(response_content)
                return
                
            # çµæœè¡¨ç¤º
            model_prefix = self._get_model_prefix()
            self._display_with_typewriter(response_content, model_prefix)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ä¿å­˜ç¢ºèª
            confirmation = input(f"\nSave this implementation to {filename}? (y/n): ")
            if confirmation.lower() == 'y':
                # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å‡¦ç†
                file_path = Path(filename)
                file_path.parent.mkdir(parents=True, exist_ok=True)
                
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(response_content)
                    
                err_console.print(f"âœ… Implementation saved to {filename}")
                
                # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çŠ¶æ…‹ã®ã‚¯ãƒªã‚¢ï¼ˆä¸€é€£ã®ä½œæ¥­ãŒå®Œäº†ï¼‰
                if hasattr(self, 'shared_state') and self.shared_state:
                    self.cmd_clear_workflow([])
            else:
                err_console.print("File not saved.")
            
            # ãƒ¡ãƒ¢ãƒªã«ä¿å­˜
            if hasattr(self, 'memory') and self.memory:
                self.memory.add_entry(
                    user_prompt=f"/write {filename}",
                    claude_reply=response_content,
                    model_used=getattr(self.llm_manager, 'current_model', 'unknown'),
                    metadata={"command": "write", "filename": filename, "goal": goal}
                )
                
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜
            if hasattr(self, 'session_manager') and self.session_manager:
                self.session_manager.add_entry(
                    user_input=f"/write {filename}",
                    ai_response=response_content,
                    model_used=getattr(self.llm_manager, 'current_model', 'unknown'),
                    command_type="write",
                    metadata={"filename": filename, "goal": goal}
                )
                
        except Exception as e:
            if hasattr(self, 'error_handler') and self.error_handler:
                self.error_handler.handle_error(e, "write command")
            else:
                err_console.print(f"Error in write command: {e}")

    def cmd_workflow_status(self, args):
        """Display workflow status"""
        if hasattr(self, 'shared_state') and self.shared_state:
            goal = self.shared_state.workflow_state.get("current_goal")
            think_done = bool(self.shared_state.workflow_state.get("think_result"))
            plan_done = bool(self.shared_state.workflow_state.get("plan_result"))
            
            err_console.print("\nğŸ“Š Current Workflow Status:")
            err_console.print(f"Goal: {goal if goal else 'Not set'}")
            err_console.print(f"Step 1 (Think): {'âœ… Completed' if think_done else 'âŒ Not completed'}")
            err_console.print(f"Step 2 (Plan): {'âœ… Completed' if plan_done else 'âŒ Not completed'}")
            err_console.print(f"Step 3 (Write): {'â³ Ready to start' if plan_done else 'âŒ Not ready'}")
            
            # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹
            if not goal:
                err_console.print(Text.from_ansi(f"\nNext step: {GREEN}/think [your goal]{RESET}"))
            elif not think_done:
                err_console.print(Text.from_ansi(f"\nNext step: {GREEN}/think {goal}{RESET}"))
            elif not plan_done:
                err_console.print(Text.from_ansi(f"\nNext step: {GREEN}/plan{RESET}"))
            else:
                err_console.print(Text.from_ansi(f"\nNext step: {GREEN}/write [filename]{RESET}"))
        else:
            err_console.print("Error: Workflow state not available")

    def cmd_clear_workflow(self, args):
        """Clear workflow state"""
        if hasattr(self, 'shared_state') and self.shared_state:
            # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
            self.shared_state.workflow_state["current_goal"] = None
            self.shared_state.workflow_state["think_result"] = None
            self.shared_state.workflow_state["plan_result"] = None
            
            err_console.print("âœ… Workflow state cleared. You can start a new workflow with /think.")
        else:
            err_console.print("Error: Workflow state not available")

    def cmd_semi_auto(self, args):
        """Automatic implementation with AI assistance
        - Two-step verification prompt (empowering users to make decisions)
        - Detailed intermediate output (process visualization)
        - Supports @filename syntax to load spec from file
        """

        # å¼•æ•°å‡¦ç†
        if isinstance(args, str):
            goal = args.strip() if args else None
        elif isinstance(args, list):
            goal = " ".join(args).strip() if args else None
        else:
            goal = None
            
        if not goal:
            goal = self.get_multiline_input(
                "What would you like me to implement automatically?",
                allow_empty=False
            )
            if not goal:
                err_console.print("Operation cancelled.")
                return
        
        # @ãƒ•ã‚¡ã‚¤ãƒ«å‚ç…§ã®å‡¦ç†
        if goal.startswith('@'):
            spec_filepath = goal[1:].strip()  # @ã‚’é™¤å» + å‰å¾Œç©ºç™½é™¤å»
            logger.debug(f"[@File Reference] Detected: {spec_filepath}")
            
            # ç©ºãƒ‘ã‚¹ãƒã‚§ãƒƒã‚¯
            if not spec_filepath:
                err_console.print("âŒ No spec file specified. Usage: /make @filename.md")
                logger.debug("[@File Reference] ERROR: Empty filepath")
                return
            
            if os.path.exists(spec_filepath):
                try:
                    with open(spec_filepath, 'r', encoding='utf-8') as f:
                        goal = f.read()
                    
                    # ç©ºãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯
                    if not goal.strip():
                        err_console.print(f"âŒ Spec file is empty: {spec_filepath}")
                        logger.debug(f"[@File Reference] ERROR: File is empty: {spec_filepath}")
                        return
                    
                    err_console.print(Text.from_ansi(f"{GREEN}âœ“ Loaded: {spec_filepath} ({len(goal):,} chars) successfully!{RESET}"))
                    logger.debug(f"[@File Reference] SUCCESS: Loaded {spec_filepath} ({len(goal):,} chars)")
                except UnicodeDecodeError:
                    err_console.print(f"âŒ Failed to read spec file (encoding issue, UTF-8 expected): {spec_filepath}")
                    logger.debug(f"[@File Reference] ERROR: UnicodeDecodeError for {spec_filepath}")
                    return
                except Exception as e:
                    err_console.print(f"âŒ Failed to read spec file: {e}")
                    logger.debug(f"[@File Reference] ERROR: {e}")
                    return
            else:
                err_console.print(f"âŒ Spec file not found: {spec_filepath}")
                logger.debug(f"[@File Reference] ERROR: File not found: {spec_filepath}")
                return
        
        try:
            # Import semi-auto engine
            from cognix.semi_auto_engine import SemiAutoEngine, SemiAutoResult
            
            # Initialize engine
            self.engine = SemiAutoEngine(
                llm_manager=self.llm_manager,
                context=self.context,
                impact_analyzer=getattr(self, 'impact_analyzer', None),
                related_finder=getattr(self, 'related_finder', None),
                diff_engine=getattr(self, 'diff_engine', None),
                repository_analyzer=getattr(self, 'repository_analyzer', None),
                config=self.config
            )
            
            # é–‹å§‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆã‚¢ã‚¤ã‚³ãƒ³çµ±ä¸€ + æ”¹è¡Œè¿½åŠ ï¼‰
            err_console.print()  # ç©ºè¡Œ
            err_console.print(f"{Icon.ROBOT.value} Starting automated implementation...")
            
            # â­ Zen HUD: ã‚´ãƒ¼ãƒ«è¡¨ç¤ºã‚’çŸ­ç¸®ï¼ˆæœ€åˆã®è¡Œã€æœ€å¤§100æ–‡å­—ï¼‰
            goal_first_line = goal.split('\n')[0].strip()
            if len(goal_first_line) > 100:
                goal_display = goal_first_line[:97] + "..."
            else:
                goal_display = goal_first_line
            err_console.print(f"{Icon.SEARCH.value} Goal: {goal_display}")
            
            err_console.print()  # ç©ºè¡Œ
            
            # Execute implementation
            result = self.engine.execute_semi_auto_implementation(goal)
            
            # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯ï¼ˆã‚¢ã‚¤ã‚³ãƒ³çµ±ä¸€ï¼‰
            if not result.success:
                err_console.print(f"{Icon.ERROR.value} auto implementation failed: {result.error}")
                return
            
            
            # Phase 2: Analysis Resultså‰Šé™¤ï¼ˆZen HUDè¨­è¨ˆï¼‰
            # åˆ†æçµæœã¯å†…éƒ¨ã§å‡¦ç†ã•ã‚Œã€æœ€çµ‚çš„ãªQuality/Recommendationsã®ã¿è¡¨ç¤º
            
            
            
            # Phase 2: Zen HUDè¦ç´„è¡¨ç¤ºã®ã¿ï¼ˆDiffè©³ç´°ã¯å‰Šé™¤ï¼‰
            # è¨­è¨ˆæ›¸: å®Œäº†å¾Œã¯è¦ç´„ã ã‘ï¼ˆLint/Diffæ¨å®š/Secretsï¼‰
            
            
            # Zen HUD: Qualityè¦ç´„ (æ–°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ)
            if result.quality_scores:
                file_count = len(result.quality_scores)
                avg_score = int(sum(result.quality_scores.values()) * 100 / len(result.quality_scores))
                console.print()  # ç©ºè¡Œ
                console.print(Text.from_ansi(f"{GREEN}âœ“ Code generation completed successfully!{RESET}"))
                console.print()  # ç©ºè¡Œ
                
                # â­ Zen HUD: Lint/Review ã‚µãƒãƒªãƒ¼
                if result.zen_summary:
                    zen = result.zen_summary
                    
                    # Lintè¡Œ
                    lint_info = zen.get("lint", {})
                    if lint_info.get("initial", 0) > 0:
                        if lint_info.get("final", 0) == 0:
                            console.print(Text.from_ansi(f"â“˜ Lint check : {lint_info['initial']} error(s) â†’ {GREEN}âœ“ auto-fixed{RESET}"))
                        else:
                            console.print(Text.from_ansi(f"â“˜ Lint check : {lint_info['initial']} error(s) â†’ {lint_info['final']} remaining"))
                    else:
                        console.print(Text.from_ansi(f"â“˜ Lint check : {GREEN}âœ“ no issues{RESET}"))
                    
                    # Reviewè¡Œï¼ˆæ”¹å–„ç‰ˆï¼šå¢—ãˆãŸå ´åˆã‚‚è€ƒæ…®ï¼‰
                    review_info = zen.get("review", {})
                    initial = review_info.get("initial", 0)
                    final = review_info.get("final", 0)
                    
                    if initial == 0 and final == 0:
                        console.print(Text.from_ansi(f"â“˜ Code review : {GREEN}âœ“ no issues{RESET}"))
                    elif final == 0:
                        # å…¨ã¦ä¿®æ­£æ¸ˆã¿
                        console.print(Text.from_ansi(f"â“˜ Code review : {initial} issue(s) â†’ {GREEN}âœ“ auto-fixed{RESET}"))
                    elif final < initial:
                        # ä¸€éƒ¨ä¿®æ­£ï¼ˆæ¸›ã£ãŸï¼‰
                        fixed_count = initial - final
                        console.print(Text.from_ansi(f"â“˜ Code review : {initial} issue(s) â†’ {GREEN}âœ“ {fixed_count} fixed{RESET} ({final} remaining)"))
                    else:
                        # åŒã˜ã‹å¢—ãˆãŸå ´åˆã¯ã‚·ãƒ³ãƒ—ãƒ«ã«è¡¨ç¤º
                        console.print(Text.from_ansi(f"â“˜ Code review : {final} issue(s) remaining"))
                
                # ğŸ†• Testè¡Œï¼ˆLintâ†’Reviewâ†’Testâ†’ãƒ•ã‚¡ã‚¤ãƒ«æ•° ã®é †åºï¼‰
                if result.zen_summary:
                    test_info = result.zen_summary.get("test", {})
                    if not test_info.get("skipped") and test_info.get("total", 0) > 0:
                        t_passed = test_info.get("passed", 0)
                        t_failed_initial = test_info.get("failed_initial", 0)
                        t_failed_final = test_info.get("failed_final", 0)
                        
                        if t_failed_final == 0:
                            if t_failed_initial > 0:
                                console.print(Text.from_ansi(
                                    f"â“˜ Test check  : {t_passed} passed, {t_failed_initial} failed â†’ {GREEN}âœ“ auto-fixed{RESET}"
                                ))
                            else:
                                console.print(Text.from_ansi(
                                    f"â“˜ Test check  : {GREEN}âœ“ {t_passed} passed{RESET}"
                                ))
                        else:
                            console.print(Text.from_ansi(
                                f"â“˜ Test check  : {t_passed} passed, {t_failed_final} failed"
                            ))
                
                # ãƒ•ã‚¡ã‚¤ãƒ«æ•°è¡¨ç¤º
                file_word = "file" if file_count == 1 else "files"
                console.print(Text.from_ansi(f"{GREEN}âœ“ Generated {file_count} {file_word}{RESET}"))
                
                console.print()  # ç©ºè¡Œ
                
            # Zen HUD: Recommendations 1è¡Œãƒ»æœ€å¤§3ä»¶
            if result.recommendations:
                from cognix import hud_components as hud
                compact = hud.recommendations_compact(result.recommendations, max_items=3)
                if compact:
                    console.print(Text.from_ansi(f"{YELLOW}âŠ¹ Recommendations:{RESET} {compact}"))
                    console.print()  # ç©ºè¡Œ
            
            # ==========================================
            # æ”¹å–„æ¡ˆA: å“è³ªã‚µãƒãƒªãƒ¼è¡¨ç¤ºï¼ˆæ‰¿èªå‰ï¼‰
            # ==========================================
            if result.quality_scores:
                console.print(Text.from_ansi(f"Quality Scores:"))
                
                for filename, score in result.quality_scores.items():
                    # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆè¡¨ç¤ºï¼ˆ0.90 â†’ 90%ï¼‰
                    score_percent = int(score * 100)
                    
                    # è‰²ã¨ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ¤å®š
                    if score >= 0.9:
                        color = GREEN
                        grade = "Excellent"
                    elif score >= 0.75:
                        color = GREEN
                        grade = "Good"
                    elif score >= 0.6:
                        color = YELLOW
                        grade = "Fair"
                    else:
                        color = YELLOW
                        grade = "Needs Review"
                    
                    console.print(Text.from_ansi(
                        f"  {color}{filename}{RESET}: {score_percent}% {color}{grade}{RESET}"
                    ))
                
                console.print()  # ç©ºè¡Œ
            
            # ==========================================
            # æ”¹å–„æ¡ˆA: å½±éŸ¿åˆ†æã‚µãƒãƒªãƒ¼è¡¨ç¤ºï¼ˆæ‰¿èªå‰ï¼‰
            # ==========================================
            if result.impact_analysis:
                console.print(Text.from_ansi(f"{GREEN}Impact Analysis:{RESET}"))
                
                for filename, impact in result.impact_analysis.items():
                    risk_level = impact.get('risk_level', 'LOW')
                    
                    if risk_level == 'HIGH':
                        color = YELLOW
                        icon = "â—¼"  # é«˜ãƒªã‚¹ã‚¯
                    elif risk_level == 'MEDIUM':
                        color = YELLOW
                        icon = "â—†"  # ä¸­ãƒªã‚¹ã‚¯
                    else:
                        color = GREEN
                        icon = "â—‡"  # ä½ãƒªã‚¹ã‚¯
                    
                    console.print(Text.from_ansi(
                        f"  {color}{icon} {filename}: {risk_level} RISK{RESET}"
                    ))
                
                console.print()  # ç©ºè¡Œ
            
            
            
            # Phase 2: æœ€çµ‚ãƒ¡ãƒ‹ãƒ¥ãƒ¼ï¼ˆãƒ«ãƒ¼ãƒ—ç‰ˆ - [t] Try againå¯¾å¿œï¼‰
            max_lint_attempts = 3
            lint_attempt = 0
            prev_lint_fail_count = None
            current_lint_result = getattr(result, 'lint_result', None)  # â† åˆå›ã®Lintçµæœã‚’å–å¾—
            
            while True:  # â† ãƒ«ãƒ¼ãƒ—æ§‹é€ ã«å¤‰æ›´
                console.print("Next:")
                console.print(Text.from_ansi(f"  [{CYAN}a{RESET}] Apply"))
                console.print(Text.from_ansi(f"  [{CYAN}r{RESET}] Reject"))
                console.print(Text.from_ansi(f"  [{CYAN}v{RESET}] View details"))
                
                # [t] Try againé¸æŠè‚¢ï¼ˆSyntax errors + Quality issues + Code review issues ãƒã‚§ãƒƒã‚¯ï¼‰
                # Lintçµæœã‹ã‚‰Syntax erroræ•°ã‚’å–å¾—
                lint_error_count = 0
                if current_lint_result:
                    lint_error_count = len(current_lint_result.get('errors', []))
                
                # Quality issues ã®ã‚«ã‚¦ãƒ³ãƒˆ
                quality_issue_count = 0
                if hasattr(result, 'quality_scores') and result.quality_scores:
                    for filename, score in result.quality_scores.items():
                        if score < 0.70:  # 70%æœªæº€ã¯å•é¡Œã‚ã‚Š
                            quality_issue_count += 1
                
                # â­ Code review remaining issues ã®ã‚«ã‚¦ãƒ³ãƒˆï¼ˆZen HUDå¯¾å¿œï¼‰
                review_remaining_count = 0
                if hasattr(result, 'zen_summary') and result.zen_summary:
                    review_remaining_count = result.zen_summary.get("review", {}).get("final", 0)
                
                # Syntax errors ã‚‚ Quality issues ã‚‚ Code review issues ã‚‚ 0 ã®å ´åˆã®ã¿ [t] éè¡¨ç¤º
                show_try_again = True
                total_issues = lint_error_count + quality_issue_count + review_remaining_count
                if current_lint_result is not None and total_issues == 0:
                    # Lintå®Ÿè¡Œæ¸ˆã¿ã€ã‹ã¤ã€å…¨ã¦0ã®å ´åˆã®ã¿éè¡¨ç¤º
                    show_try_again = False
                
                if not show_try_again:
                    # [t]ã‚’è¡¨ç¤ºã—ãªã„
                    pass
                elif lint_attempt >= max_lint_attempts:
                    # æœ€å¤§è©¦è¡Œå›æ•°åˆ°é”æ™‚ã¯ disabledè¡¨ç¤º
                    console.print(Text.from_ansi(f"  [{CYAN}t{RESET}] Try again (disabled - max attempts)"))
                elif lint_attempt > 0:
                    # è©¦è¡Œå›æ•°è¡¨ç¤º
                    console.print(Text.from_ansi(f"  [{CYAN}t{RESET}] Try again ({lint_attempt}/{max_lint_attempts} attempts used)"))
                else:
                    # åˆå›è¡¨ç¤ºï¼ˆä½•ã‚’ä¿®æ­£ã™ã‚‹ã‹æ˜ç¤ºï¼‰
                    # â­ Zen HUDå¯¾å¿œ: review_remaining_countã‚‚å«ã‚ã‚‹
                    issue_parts = []
                    if lint_error_count > 0:
                        issue_parts.append(f"{lint_error_count} syntax error(s)")
                    if quality_issue_count > 0:
                        issue_parts.append(f"{quality_issue_count} quality issue(s)")
                    if review_remaining_count > 0:
                        issue_parts.append(f"{review_remaining_count} review issue(s)")
                    
                    if issue_parts:
                        console.print(Text.from_ansi(f"  [{CYAN}t{RESET}] Try again (fix {' + '.join(issue_parts)})"))
                    else:
                        console.print(Text.from_ansi(f"  [{CYAN}t{RESET}] Try again"))
                
                # éå¯¾è©±ãƒ¢ãƒ¼ãƒ‰: auto_mode=True ãªã‚‰è‡ªå‹•Apply
                _auto = getattr(getattr(self, '_cli_instance', None), 'auto_mode', False)
                if _auto:
                    err_console.print(f"\n[auto] Auto-applying (non-interactive mode)...")
                    choice = "a"
                else:
                    choice = input("\nYour choice: ").strip().lower()
                
                if choice == "r":
                    err_console.print(Text.from_ansi(f"{MAGENTA}âœ• Implementation rejected by user{RESET}"))
                    err_console.print()  # æ”¹è¡Œã‚’è¿½åŠ 
                    return
                
                elif choice == "v":
                    # View details ãƒ•ãƒ­ãƒ¼
                    detail_result = self._handle_review_details(result)
                    
                    if detail_result == "applied":
                        # Applyæ¸ˆã¿: çµ‚äº†
                        return
                    elif detail_result == "rejected":
                        # Rejectæ¸ˆã¿: çµ‚äº†
                        return
                    elif detail_result == "back":
                        # Back: ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã«æˆ»ã‚‹
                        continue
                    else:
                        # äºˆæœŸã—ãªã„æˆ»ã‚Šå€¤
                        continue
                
                elif choice == "t":
                    # â­ å…¨ã¦ã®å•é¡ŒãŒ0ã®å ´åˆã®ã¿ã€ŒNo need to retryã€
                    if current_lint_result is not None and total_issues == 0:
                        err_console.print(f"{Icon.SUCCESS.value} All issues are already resolved. No need to retry.")
                        continue
                    
                    # Lintå†å®Ÿè¡Œå‡¦ç†
                    if lint_attempt >= max_lint_attempts:
                        err_console.print(f"{Icon.WARNING.value} Max attempts reached ({max_lint_attempts}/{max_lint_attempts}). Please choose [a]pply or [r]eject.")
                        continue
                    
                    lint_attempt += 1
                    err_console.print()  # ç©ºè¡Œï¼ˆStepHUDè¡¨ç¤ºå‰ã®ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
                    
                    # å…¨ãƒã‚§ãƒƒã‚¯å†å®Ÿè¡Œï¼ˆGãƒã‚§ãƒƒã‚¯ â†’ Import â†’ Lint â†’ Quality â†’ ç·åˆãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰
                    # â­ StepHUDãŒé€²æ—è¡¨ç¤ºã‚’æ‹…å½“ï¼ˆCode Generationã‚¹ã‚¿ã‚¤ãƒ«ï¼‰
                    updated_code, new_lint_result, new_quality_scores = self.engine.retry_full_validation(
                        result.generated_code,
                        goal,  # ç·åˆãƒ¬ãƒ“ãƒ¥ãƒ¼ã§ä½¿ç”¨
                        current_lint_result,
                        attempt=lint_attempt,
                        max_attempts=max_lint_attempts
                    )
                    
                    # current_lint_resultã‚’æ›´æ–°ï¼ˆæ¬¡ã®ãƒ«ãƒ¼ãƒ—ã§ä½¿ç”¨ï¼‰
                    current_lint_result = new_lint_result
                    
                    # çµæœã‚’æ›´æ–°ï¼ˆSemiAutoResultã¯ä¸å¤‰ãªã®ã§æ–°è¦ä½œæˆï¼‰
                    # SemiAutoResult is already imported at the top
                    # â­ zen_summaryã‚’engineã‹ã‚‰å–å¾—
                    new_zen_summary = getattr(self.engine, '_zen_summary', None)
                    result = SemiAutoResult(
                        success=True,
                        analysis=result.analysis,
                        generated_code=updated_code,
                        quality_scores=new_quality_scores if new_quality_scores else result.quality_scores,
                        recommendations=result.recommendations,
                        error=result.error,
                        impact_analysis=result.impact_analysis,
                        lint_result=current_lint_result,
                        zen_summary=new_zen_summary,  # â­ Zen HUDå¯¾å¿œ
                        test_result=new_zen_summary.get("test") if new_zen_summary else None  # ğŸ†• ãƒ†ã‚¹ãƒˆçµæœå¼•ãç¶™ã
                    )
                    
                    # â­ Try againå¾Œã®Zen HUDã‚µãƒãƒªãƒ¼è¡¨ç¤º
                    if new_zen_summary:
                        console.print()  # ç©ºè¡Œ
                        
                        # Lintè¡Œ
                        lint_info = new_zen_summary.get("lint", {})
                        if lint_info.get("initial", 0) > 0:
                            if lint_info.get("final", 0) == 0:
                                console.print(Text.from_ansi(f"â“˜ Lint check : {lint_info['initial']} error(s) â†’ {GREEN}âœ“ auto-fixed{RESET}"))
                            else:
                                console.print(Text.from_ansi(f"â“˜ Lint check : {lint_info['initial']} error(s) â†’ {lint_info['final']} remaining"))
                        else:
                            console.print(Text.from_ansi(f"â“˜ Lint check : {GREEN}âœ“ no issues{RESET}"))
                        
                        # Reviewè¡Œï¼ˆæ”¹å–„ç‰ˆï¼šå¢—ãˆãŸå ´åˆã‚‚è€ƒæ…®ï¼‰
                        review_info = new_zen_summary.get("review", {})
                        initial = review_info.get("initial", 0)
                        final = review_info.get("final", 0)
                        
                        if initial == 0 and final == 0:
                            console.print(Text.from_ansi(f"â“˜ Code review : {GREEN}âœ“ no issues{RESET}"))
                        elif final == 0:
                            # å…¨ã¦ä¿®æ­£æ¸ˆã¿
                            console.print(Text.from_ansi(f"â“˜ Code review : {initial} issue(s) â†’ {GREEN}âœ“ auto-fixed{RESET}"))
                        elif final < initial:
                            # ä¸€éƒ¨ä¿®æ­£ï¼ˆæ¸›ã£ãŸï¼‰
                            fixed_count = initial - final
                            console.print(Text.from_ansi(f"â“˜ Code review : {initial} issue(s) â†’ {GREEN}âœ“ {fixed_count} fixed{RESET} ({final} remaining)"))
                        else:
                            # åŒã˜ã‹å¢—ãˆãŸå ´åˆã¯ã‚·ãƒ³ãƒ—ãƒ«ã«è¡¨ç¤º
                            console.print(Text.from_ansi(f"â“˜ Code review : {final} issue(s) remaining"))
                        
                        # ğŸ†• Testè¡Œï¼ˆretryå¾Œï¼‰ï¼ˆLintâ†’Reviewâ†’Testâ†’ãƒ•ã‚¡ã‚¤ãƒ«æ•° ã®é †åºï¼‰
                        if new_zen_summary:
                            test_info = new_zen_summary.get("test", {})
                            if not test_info.get("skipped") and test_info.get("total", 0) > 0:
                                t_passed = test_info.get("passed", 0)
                                t_failed_initial = test_info.get("failed_initial", 0)
                                t_failed_final = test_info.get("failed_final", 0)
                                
                                if t_failed_final == 0:
                                    if t_failed_initial > 0:
                                        console.print(Text.from_ansi(
                                            f"â“˜ Test check  : {t_passed} passed, {t_failed_initial} failed â†’ {GREEN}âœ“ auto-fixed{RESET}"
                                        ))
                                    else:
                                        console.print(Text.from_ansi(
                                            f"â“˜ Test check  : {GREEN}âœ“ {t_passed} passed{RESET}"
                                        ))
                                else:
                                    console.print(Text.from_ansi(
                                        f"â“˜ Test check  : {t_passed} passed, {t_failed_final} failed"
                                    ))
                        
                        # ãƒ•ã‚¡ã‚¤ãƒ«æ•°è¡¨ç¤º
                        if result.quality_scores:
                            file_count = len(result.quality_scores)
                            file_word = "file" if file_count == 1 else "files"
                            console.print(Text.from_ansi(f"{GREEN}âœ“ Validated {file_count} {file_word}{RESET}"))
                        
                        console.print()  # ç©ºè¡Œ
                    
                    # ãƒ«ãƒ¼ãƒ—ç¶™ç¶š
                    continue
                
                elif choice == "a":
                    # Applyå‡¦ç†ã¸
                    break  # â† ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
                
                else:
                    err_console.print(f"{Icon.ERROR.value} Invalid choice. Please try again.")
                    continue
            
            # choice == "a" ã®å ´åˆã®ã¿ã“ã“ã«åˆ°é”
            # choice == "a" ã®å ´åˆã®ã¿ç¶šè¡Œ
            
            # ã‚³ãƒ¼ãƒ‰é©ç”¨ï¼ˆã‚¢ã‚¤ã‚³ãƒ³çµ±ä¸€ï¼‰
            err_console.print(f"\n{Icon.GEAR.value} Applying implementation...")
            final_result = self.engine.apply_generated_code(result)
            
            if final_result.success:
                err_console.print(Text.from_ansi(f"{GREEN}{Icon.SUCCESS.value} Auto implementation completed successfully!{RESET}"))
                err_console.print()  # ç©ºè¡Œã‚’è¿½åŠ 
                
                if final_result.applied_files:
                    err_console.print(f"{Icon.FOLDER.value} Files created/modified: {len(final_result.applied_files)}")
                    for file in final_result.applied_files:
                        err_console.print(f"   â€¢ {file}")
                    err_console.print()  # ç©ºè¡Œã‚’è¿½åŠ 
                
                if final_result.backup_paths:
                    err_console.print(f"{Icon.PACKAGE.value} Backups created: {len(final_result.backup_paths)}")
                    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã®å€‹åˆ¥ãƒ‘ã‚¹è¡¨ç¤ºï¼ˆFiles created/modifiedã¨åŒã˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼‰
                    for backup_path in final_result.backup_paths:
                        err_console.print(f"   â€¢ {backup_path}")
                    err_console.print()  # ç©ºè¡Œã‚’è¿½åŠ 
                                    
                # ãƒ¡ãƒ¢ãƒªä¿å­˜
                if hasattr(self, 'memory') and self.memory:
                    self.memory.add_entry(
                        user_prompt=f"Semi-auto implementation: {goal}",
                        claude_reply=f"Implementation completed with {len(final_result.applied_files or [])} files",
                        model_used=getattr(self.llm_manager, 'current_model', 'unknown'),
                        metadata={
                            "command_type": "semi_auto",
                            "files_generated": final_result.applied_files,
                            "quality_scores": final_result.quality_scores
                        }
                    )
                
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜
                if hasattr(self, 'session_manager') and self.session_manager:
                    self.session_manager.add_entry(
                        user_input=f"/semi-auto {goal}",
                        ai_response=f"Implementation completed successfully",
                        model_used=getattr(self.llm_manager, 'current_model', 'unknown'),
                        command_type="semi_auto",
                        target_files=final_result.applied_files or [],
                        metadata={
                            "goal": goal,
                            "files_count": len(final_result.applied_files or []),
                            "quality_average": sum(final_result.quality_scores.values()) / len(final_result.quality_scores) if final_result.quality_scores else 0
                        }
                    )
                
            else:
                err_console.print(f"{Icon.ERROR.value} Failed to apply implementation: {final_result.error}")
                    
        except ImportError:
            err_console.print(f"{Icon.ERROR.value} Semi-auto engine not available. Please check installation.")

        except Exception as e:
            if hasattr(self, 'error_handler') and self.error_handler:
                self.error_handler.handle_error(e, "semi-auto command")
            else:
                err_console.print(f"Error in semi-auto command: {e}")
        finally:
            pass  # HUDç®¡ç†ã¯semi_auto_engineã«å§”è­²


    def _handle_review_details(self, result):
        """Review details ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ•ãƒ­ãƒ¼ï¼ˆè¨­è¨ˆæ›¸æº–æ‹ ï¼‰
        
        Args:
            result: ç”Ÿæˆçµæœã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        from cognix.diff_viewer import DiffViewer
        

        # files_dataã‚’äº‹å‰ç”Ÿæˆï¼ˆImpact Analysisç”¨ï¼‰
        files_data = []
        for filename in result.generated_code.keys():
            file_path = Path(filename)
            if file_path.exists():
                old_content = file_path.read_text(encoding='utf-8')
                new_content = result.generated_code[filename]
                
                old_lines = old_content.splitlines()
                new_lines = new_content.splitlines()
                
                additions = len([l for l in new_lines if l not in old_lines])
                deletions = len([l for l in old_lines if l not in new_lines])
                
                files_data.append({
                    'name': filename,
                    'added': additions,
                    'removed': deletions
                })
            else:
                line_count = len(result.generated_code[filename].splitlines())
                files_data.append({
                    'name': filename,
                    'added': line_count,
                    'removed': 0
                })
        
        # Impact AnalysisãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
        has_impact_analysis = bool(result.impact_analysis if hasattr(result, 'impact_analysis') else False)

        while True:
            # Zen: ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ï¼ˆä½™ç™½ã§åŒºåˆ‡ã‚Šï¼‰
            console.print()
            file_count = len(result.generated_code)
            console.print(f"[bold]{file_count} files changed[/bold]")
            console.print()
            
            file_list = list(result.generated_code.items())
            for idx, (filename, _) in enumerate(file_list, 1):
                # Diffæ¨å®šã‚’è¨ˆç®—
                file_path = Path(filename)
                if file_path.exists():
                    old_content = file_path.read_text(encoding='utf-8')
                    new_content = result.generated_code[filename]
                    
                    old_lines = old_content.splitlines()
                    new_lines = new_content.splitlines()
                    
                    additions = len([l for l in new_lines if l not in old_lines])
                    deletions = len([l for l in old_lines if l not in new_lines])
                    
                    # Zen: ã‚·ãƒ³ãƒ—ãƒ«ãªå·®åˆ†è¡¨ç¤ºï¼ˆCognix green + ç™½ï¼‰
                    console.print(Text.from_ansi(f"  {idx}  {GREEN}{filename}{RESET}  +{additions} / -{deletions}"))
                else:
                    line_count = len(result.generated_code[filename].splitlines())
                    console.print(Text.from_ansi(f"  {idx}  {GREEN}{filename}{RESET}  new, {line_count} lines"))
            
            # Zen: ãƒ¡ãƒ‹ãƒ¥ãƒ¼ï¼ˆã‚­ãƒ¼ã¯ã‚·ã‚¢ãƒ³è‰²ï¼‰
            console.print()
            console.print(Text.from_ansi(f"  [{CYAN}1{RESET}]-[{CYAN}{len(file_list)}{RESET}]  Preview / diff"))
            # Impact AnalysisãŒã‚ã‚‹å ´åˆã®ã¿ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã«è¡¨ç¤º
            if has_impact_analysis:
                console.print(Text.from_ansi(f"  [{CYAN}i{RESET}]      Impact analysis"))
            console.print()
            console.print(Text.from_ansi(f"  [{CYAN}a{RESET}] Apply    [{CYAN}r{RESET}] Reject    [{CYAN}b{RESET}] Back"))
            
            choice = input("\nYour choice: ").strip().lower()
            
            # æ•°å€¤é¸æŠ: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
            if choice.isdigit():
                file_idx = int(choice) - 1
                if 0 <= file_idx < len(file_list):
                    filename, content = file_list[file_idx]
                    self._show_file_preview(filename, content, result)
                else:
                    err_console.print(f"{Icon.ERROR.value} Invalid file number")
                continue
            
            # i: Impact analysis
            if choice == "i":
                if has_impact_analysis:
                    self._show_impact_analysis(result, files_data)
                else:
                    console.print("\nImpact analysis is not available.")
                    console.print("No existing files to analyze.")
                    input("\nPress Enter to continue...")
                continue
            
            # a: Apply
            if choice == "a":
                err_console.print(f"\n{Icon.GEAR.value} Applying implementation...")
                final_result = self.engine.apply_generated_code(result)
                
                if final_result.success:
                    err_console.print(Text.from_ansi(f"{GREEN}âœ“ Implementation applied successfully!{RESET}"))
                    self._show_apply_summary(final_result)
                else:
                    err_console.print(f"{Icon.ERROR.value} Failed to apply: {final_result.error}")
                return "applied"
            
            # r: Reject
            if choice == "r":
                err_console.print(Text.from_ansi(f"{MAGENTA}âœ• Implementation rejected by user{RESET}"))
                err_console.print()  # æ”¹è¡Œã‚’è¿½åŠ 
                return "rejected"
            
            # b: Back
            if choice == "b":
                return "back"
            
            err_console.print(f"{Icon.ERROR.value} Invalid choice")
    
    def _show_file_preview(self, filename: str, content: str, result=None):
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤ºï¼ˆPreview â†’ Diffåˆ‡æ›¿å¯¾å¿œï¼‰
        
        Args:
            filename: ãƒ•ã‚¡ã‚¤ãƒ«å
            content: ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹
            result: ç”Ÿæˆçµæœã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼ˆã‚µãƒãƒªãƒ¼è¡¨ç¤ºç”¨ã€çœç•¥å¯èƒ½ï¼‰
        """
        from rich.syntax import Syntax
        from rich.panel import Panel
        
        file_path = Path(filename)
        icon = FileIconMapper.get_icon(filename)
        
        # åˆæ‰‹: Previewè¡¨ç¤º
        console.print(f"\n{icon} Preview: {filename}")
        
        # è¨€èªæ¨æ¸¬
        ext = file_path.suffix.lstrip('.')
        language = ext if ext else "text"
        
        # å…¨å†…å®¹ã‚’è¡¨ç¤ºï¼ˆ50è¡Œåˆ¶é™ã‚’å‰Šé™¤ï¼‰
        syntax = Syntax(content, language, theme="monokai", line_numbers=True)
        panel = Panel(syntax, border_style="cyan", padding=(0, 1))
        console.print(panel)
        
        # ==========================================
        # ã€æ–°è¦è¿½åŠ ã€‘ãƒ•ã‚¡ã‚¤ãƒ«è©³ç´°ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        # ==========================================
        if result is not None:
            self._show_file_details_summary(filename, result)
        
        # Diffåˆ‡æ›¿ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        console.print()
        console.print(Text.from_ansi(f"[{CYAN}d{RESET}] Show diff   [{CYAN}b{RESET}] Back to file list"))
        choice = input("Your choice: ").strip().lower()
        
        if choice == "d":
            self._show_file_diff(filename, content)
    
    def _show_file_details_summary(self, filename: str, result):
        """ãƒ•ã‚¡ã‚¤ãƒ«è©³ç´°ã‚µãƒãƒªãƒ¼è¡¨ç¤ºï¼ˆ[v] View detailsç”¨ï¼‰
        
        ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰è¡¨ç¤ºå¾Œã«Qualityã€Review Issuesã€Lintçµæœã‚’è¡¨ç¤º
        
        Args:
            filename: ãƒ•ã‚¡ã‚¤ãƒ«å
            result: ç”Ÿæˆçµæœã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        try:
            from semi_auto_engine import get_file_details_summary, format_file_details_for_display
        except ImportError:
            try:
                from cognix.semi_auto_engine import get_file_details_summary, format_file_details_for_display
            except ImportError:
                # ã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ããªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                return
        
        # ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        details = get_file_details_summary(
            generated_code=result.generated_code if hasattr(result, 'generated_code') else {},
            quality_scores=result.quality_scores if hasattr(result, 'quality_scores') else {},
            lint_result=result.lint_result if hasattr(result, 'lint_result') else None,
            zen_summary=result.zen_summary if hasattr(result, 'zen_summary') else None,
            filename=filename
        )
        
        # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã—ã¦è¡¨ç¤º
        summary_text = format_file_details_for_display(details)
        console.print(summary_text)
    
    def _show_file_diff(self, filename: str, new_content: str):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®Diffè¡¨ç¤º
        
        Args:
            filename: ãƒ•ã‚¡ã‚¤ãƒ«å
            new_content: æ–°ã—ã„å†…å®¹
        """
        from rich.syntax import Syntax
        from rich.panel import Panel
        import difflib
        
        file_path = Path(filename)
        
        if not file_path.exists():
            # æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«: å…¨å†…å®¹ã‚’è¡¨ç¤ºï¼ˆdiffã§ã¯ãªãï¼‰
            icon = FileIconMapper.get_icon(filename)
            console.print(f"\n{icon} New file: {filename} (full content)")
            
            # è¨€èªæ¨æ¸¬
            ext = file_path.suffix.lstrip('.')
            language = ext if ext else "text"
            
            # å…¨å†…å®¹ã‚’è¡¨ç¤ºï¼ˆ50è¡Œåˆ¶é™ãªã—ï¼‰
            syntax = Syntax(new_content, language, theme="monokai", line_numbers=True)
            panel = Panel(syntax, border_style="cyan", padding=(0, 1))
            console.print(panel)
            
            input("\nPress Enter to continue...")
            return
        
        old_content = file_path.read_text(encoding='utf-8')
        
        # Unified diffç”Ÿæˆ
        diff_lines = list(difflib.unified_diff(
            old_content.splitlines(keepends=True),
            new_content.splitlines(keepends=True),
            fromfile=f"a/{filename}",
            tofile=f"b/{filename}",
            lineterm=""
        ))
        
        diff_text = "".join(diff_lines)
        
        # ã‚·ãƒ³ã‚¿ãƒƒã‚¯ã‚¹ãƒã‚¤ãƒ©ã‚¤ãƒˆ
        syntax = Syntax(diff_text, "diff", theme="monokai", line_numbers=False)
        panel = Panel(syntax, border_style="cyan", padding=(0, 1))
        console.print(panel)
        
        input("\nPress Enter to continue...")
    
    def _show_impact_analysis(self, result: Dict, files_data: List[Dict]) -> None:
        """Impact Analysisçµæœã‚’è¡¨ç¤ºï¼ˆZenç‰ˆï¼‰"""
        
        if not getattr(result, 'impact_analysis', {}):
            console.print("\nNo impact analysis available")
            console.print("\nPress Enter to return...")
            input()
            return
        
        # Zen: ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆä½™ç™½ã§åŒºåˆ‡ã‚Šï¼‰
        console.print()
        console.print("[bold]Impact Analysis[/bold]")
        console.print()
        
        diff_info = {}
        for file_info in files_data:
            filename = file_info['name']
            diff_info[filename] = {
                'added': file_info['added'],
                'removed': file_info['removed']
            }
        
        for filename, impact_data in result.impact_analysis.items():
            added = diff_info.get(filename, {}).get('added', 0)
            removed = diff_info.get(filename, {}).get('removed', 0)
            
            # Zen: ãƒ•ã‚¡ã‚¤ãƒ«åã¨å·®åˆ†ï¼ˆCognix green + ç™½ï¼‰
            console.print(Text.from_ansi(f"{GREEN}{filename}{RESET}  +{added} / -{removed}"))
            
            # Zen: ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«ï¼ˆç™½ãƒ©ãƒ™ãƒ« + è‰²ä»˜ãã®å€¤ï¼‰
            risk_level = impact_data.get('risk_level', 'unknown').upper()
            if risk_level == 'LOW':
                console.print(Text.from_ansi(f"  Risk         {GREEN}{risk_level}{RESET}"))
            elif risk_level == 'HIGH':
                console.print(f"  Risk         [red]{risk_level}[/red]")
            elif risk_level == 'MEDIUM':
                console.print(f"  Risk         [yellow]{risk_level}[/yellow]")
            else:
                console.print(f"  Risk         {risk_level}")
            
            # Zen: è¤‡é›‘åº¦ï¼ˆç™½ï¼‰
            complexity = impact_data.get('complexity', 0)
            console.print(f"  Complexity   {complexity}/10")
            
            # Zen: ä¾å­˜é–¢ä¿‚ï¼ˆç™½ï¼‰
            dependencies = impact_data.get('dependencies', [])
            dep_count = len(dependencies)
            console.print(f"  Dependencies {dep_count} files")
            
            # Zen: å½±éŸ¿ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆç™½ï¼‰
            affected_files = impact_data.get('affected_files', [])
            if affected_files:
                # ãƒ•ãƒ«ãƒ‘ã‚¹ â†’ ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿ã«å¤‰æ›
                affected_names = [Path(f).name for f in affected_files[:3]]
                affected_str = ', '.join(affected_names)
                if len(affected_files) > 3:
                    affected_str += f" +{len(affected_files) - 3}"
                console.print(f"  Affects      : {affected_str}")
            else:
                console.print(f"  Affects      none")
            
            # Zen: è­¦å‘Šã¯ã‚·ãƒ³ãƒ—ãƒ«ã«ï¼ˆHIGHã®ã¿ï¼‰
            if risk_level == 'HIGH' and affected_files:
                console.print(f"  [yellow]âš  Used by {len(affected_files)} files[/yellow]")
            
            console.print()
        
        # Zen: ã‚µãƒãƒªãƒ¼ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ã«ï¼‰
        risk_counts = {'high': 0, 'medium': 0, 'low': 0}
        
        for impact_data in result.impact_analysis.values():
            risk_level = impact_data.get('risk_level', 'unknown').lower()
            if risk_level in risk_counts:
                risk_counts[risk_level] += 1
        
        # ã‚µãƒãƒªãƒ¼æ–‡å­—åˆ—ã‚’æ§‹ç¯‰
        summary_parts = []
        if risk_counts['high'] > 0:
            summary_parts.append(f"[red]{risk_counts['high']} high[/red]")
        if risk_counts['medium'] > 0:
            summary_parts.append(f"[yellow]{risk_counts['medium']} medium[/yellow]")
        if risk_counts['low'] > 0:
            # Cognix greenã‚’ä½¿ç”¨
            low_text = Text.from_ansi(f"{GREEN}{risk_counts['low']} low{RESET}")
            summary_parts.append(low_text)
        
        if summary_parts:
            # summary_partsã«Textã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒå«ã¾ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚å€‹åˆ¥ã«å‡¦ç†
            console.print(Text.from_ansi(f"{GREEN}{risk_counts['low']} low{RESET} risk") if risk_counts['low'] > 0 and risk_counts['high'] == 0 and risk_counts['medium'] == 0 else ' Â· '.join([str(p) for p in summary_parts]) + ' risk')
        
        console.print("\nPress Enter to return...")
        input()
    
    def _show_lints_summary(self, result):
        """Lintsè¦ç´„è¡¨ç¤ºï¼ˆTop3 + ... +N moreï¼‰
        
        Args:
            result: ç”Ÿæˆçµæœã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        console.print("\n" + "="*60)
        console.print("Lints Summary (Top 3)")
        console.print("="*60)
        
        # ã€æ¨æ¸¬ã€‘result.lint_results ãŒå­˜åœ¨ã™ã‚‹ã¨ä»®å®š
        if hasattr(result, 'lint_results') and result.lint_results:
            issues = result.lint_results[:3]
            for issue in issues:
                console.print(f"  â€¢ {issue}")
            
            remaining = len(result.lint_results) - 3
            if remaining > 0:
                console.print(f"\n  ... +{remaining} more")
        else:
            console.print("No lint issues found")
        
        input("\nPress Enter to continue...")
    
    def _show_apply_summary(self, final_result):
        """Applyå¾Œã®ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        
        Args:
            final_result: é©ç”¨çµæœã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        if final_result.applied_files:
            err_console.print(f"{Icon.FOLDER.value} Files created/modified: {len(final_result.applied_files)}")
            for file in final_result.applied_files:
                err_console.print(f"   â€¢ {file}")
            err_console.print()  # ç©ºè¡Œã‚’è¿½åŠ 
        
        if hasattr(final_result, 'backup_paths') and final_result.backup_paths:
            err_console.print()  # ç©ºè¡Œã‚’è¿½åŠ 
            err_console.print(f"{Icon.PACKAGE.value} Backups created: {len(final_result.backup_paths)}")
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã®å€‹åˆ¥ãƒ‘ã‚¹è¡¨ç¤ºï¼ˆFiles created/modifiedã¨åŒã˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼‰
            for backup_path in final_result.backup_paths:
                err_console.print(f"   â€¢ {backup_path}")
        
        # ==========================================
        # æ”¹å–„æ¡ˆC: å“è³ªã‚¹ã‚³ã‚¢è¡¨ç¤ºï¼ˆApplyå¾Œï¼‰
        # ==========================================
        if hasattr(final_result, 'quality_scores') and final_result.quality_scores:
            err_console.print(Text.from_ansi(f"{Icon.CHART.value} Quality Scores:"))
            
            total_score = 0
            count = 0
            
            for filename, score in final_result.quality_scores.items():
                # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆè¡¨ç¤ºï¼ˆ0.90 â†’ 90%ï¼‰
                score_percent = int(score * 100)
                
                # è‰²ã¨ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ¤å®š
                if score >= 0.9:
                    color = GREEN
                    grade = "Excellent"
                elif score >= 0.75:
                    color = GREEN
                    grade = "Good"
                elif score >= 0.6:
                    color = YELLOW
                    grade = "Fair"
                else:
                    color = YELLOW
                    grade = "Needs Review"
                
                err_console.print(Text.from_ansi(
                    f"   {color}{filename}{RESET}: {score_percent}% {color}{grade}{RESET}"
                ))
                
                total_score += score
                count += 1
            
            # å¹³å‡ã‚¹ã‚³ã‚¢è¡¨ç¤º
            if count > 0:
                avg_score = total_score / count
                avg_percent = int(avg_score * 100)
                err_console.print()
                err_console.print(Text.from_ansi(
                    f"   Average Quality: {GREEN}{avg_percent}%{RESET}"
                ))
                err_console.print()


    def cmd_make(self, args):
        """Alias for /semi-auto command
        
        Short command for semi-automatic code generation.
        Usage: /make <goal>
        Example: /make create a calculator
        """
        # cmd_semi_autoã«å‡¦ç†ã‚’å§”è­²
        return self.cmd_semi_auto(args)