"""
Semi-Auto Engine for Cognix
Handles semi-automatic code generation and file modifications
"""

import os
import re
import sys
import ast
import json
import asyncio
import tempfile
import subprocess
from cognix.logger import logger, err_console
from pathlib import Path
from typing import List, Dict, Optional, Any, Tuple, NamedTuple
from dataclasses import dataclass
import logging
from datetime import datetime
from cognix.ui import Icon, StatusIndicator, print_success_banner, print_error_banner, print_warning_banner
from cognix.diff_viewer import DiffViewer
from cognix.progress_zen import StepHUD
from cognix.utils import Spinner, ProgressBar

# â­ File Requirement Detection Pipeline
from cognix.file_detection import FileRequirementDetector

# Local imports
try:
    from .stylecode_extractor import StyleCodeExtractor
    # Note: PromptTemplateManager import removed - prompts are handled directly in _get_code_generation_system_prompt()
    from .linter_integration import LinterIntegration
except ImportError:
    from stylecode_extractor import StyleCodeExtractor
    # Note: PromptTemplateManager import removed - prompts are handled directly in _get_code_generation_system_prompt()
    from linter_integration import LinterIntegration

# Phase 1: Linter Integration
try:
    from cognix.linter_integration import LinterIntegration
except ImportError:
    LinterIntegration = None
    logger.debug("âš  LinterIntegration not available (optional feature)")

# ============================================
# Diff Sanitizer Configuration
# ============================================
# LLMã¸ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã€Œâ‰¤5è¡Œã€ã¨æŒ‡ç¤ºã—ã¦ã„ã‚‹ãŸã‚ã€
# Sanitizerã®marginã‚‚Â±5è¡Œã«çµ±ä¸€ã™ã‚‹ã€‚
# å¤‰æ›´æ™‚ã¯ã€ã“ã®å®šæ•°ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…ã®è¨˜è¿°ã‚’åŒæ™‚ã«æ›´æ–°ã™ã‚‹ã“ã¨ã€‚
DIFF_SANITIZER_MARGIN = 5

# ============================================
# Project Structure (LLM-based detection result)
# ============================================

@dataclass
class ProjectStructure:
    """
    LLMåˆ¤å®šã«ã‚ˆã‚‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 
    
    Attributes:
        stage: "single" (ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã®ã¿) or "multi" (ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰å¿…è¦)
        file_types: å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã®ãƒªã‚¹ãƒˆ (ä¾‹: ["html", "css", "js"])
        reason: åˆ¤å®šç†ç”±
    """
    stage: str           # "single" | "multi"
    file_types: List[str]  # ["html", "css", "js"] ãªã©
    reason: str          # åˆ¤å®šç†ç”±


# ============================================
# Differential Scan: Issue Cache & Modified Range
# ============================================

@dataclass
class CachedIssue:
    """
    ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸIssueæƒ…å ±
    
    å·®åˆ†ã‚¹ã‚­ãƒ£ãƒ³æ©Ÿèƒ½ã§ä½¿ç”¨ã€‚å¤‰æ›´ã•ã‚Œã¦ã„ãªã„è¡Œã®Issueã‚’
    å†æ¤œå‡ºã›ãšã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã€‚
    
    Attributes:
        file: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        line: è¡Œç•ªå·ï¼ˆNoneã®å ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã«é–¢ã™ã‚‹Issueï¼‰
        line_end: çµ‚äº†è¡Œï¼ˆç¯„å›²æŒ‡å®šæ™‚ã€Noneã®å ´åˆã¯å˜ä¸€è¡Œï¼‰
        issue_type: Issueç¨®åˆ¥ï¼ˆ1-25ï¼‰
        description: Issueèª¬æ˜
        fix: ä¿®æ­£ææ¡ˆ
        content_hash: è©²å½“è¡Œã®ãƒãƒƒã‚·ãƒ¥ï¼ˆå¤‰æ›´æ¤œçŸ¥ç”¨ï¼‰
    """
    file: str
    line: Optional[int]
    line_end: Optional[int]
    issue_type: int
    description: str
    fix: str
    content_hash: str


@dataclass
class ModifiedRange:
    """
    å¤‰æ›´ã•ã‚ŒãŸè¡Œç¯„å›²
    
    unified diffã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸå¤‰æ›´ç¯„å›²ã€‚
    å·®åˆ†ã‚¹ã‚­ãƒ£ãƒ³ã§å†æ¤œå‡ºå¯¾è±¡ã‚’ç‰¹å®šã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã€‚
    
    Attributes:
        file: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        start_line: å¤‰æ›´é–‹å§‹è¡Œï¼ˆ1-indexedï¼‰
        end_line: å¤‰æ›´çµ‚äº†è¡Œï¼ˆ1-indexedã€inclusiveï¼‰
        diff_content: è©²å½“éƒ¨åˆ†ã®å·®åˆ†å†…å®¹
    """
    file: str
    start_line: int
    end_line: int
    diff_content: str


@dataclass
class SICError:
    """
    SICæ¤œè¨¼ã‚¨ãƒ©ãƒ¼ã®æ§‹é€ åŒ–æƒ…å ±
    
    3è€…æ¯”è¼ƒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ä½¿ç”¨ã€‚ã‚¨ãƒ©ãƒ¼ã®ç¨®é¡ã¨è©³ç´°ã‚’
    LLMã«æ˜ç¢ºã«ä¼ãˆã‚‹ãŸã‚ã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã€‚
    
    Attributes:
        error_type: ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—
            - "TOP_LEVEL_POLLUTION": å±æ€§ãŒã‚¯ãƒ©ã‚¹å¤–ã«æ¼å‡º
            - "SYMBOL_DELETION": ã‚¯ãƒ©ã‚¹/é–¢æ•°ã®å‰Šé™¤
            - "ATTRIBUTE_DRIFT": é‡è¦å±æ€§ã®æ¶ˆå¤±
            - "CLASS_DELETION": ã‚¯ãƒ©ã‚¹ã®æ¶ˆå¤±
            - "TYPE_ANNOTATION_DRIFT": å‹ãƒ’ãƒ³ãƒˆã®æ¶ˆå¤±
            - "SYNTAX_ERROR": æ§‹æ–‡ã‚¨ãƒ©ãƒ¼
        affected_class: å½±éŸ¿ã‚’å—ã‘ãŸã‚¯ãƒ©ã‚¹å
        affected_items: å½±éŸ¿ã‚’å—ã‘ãŸå±æ€§/ã‚·ãƒ³ãƒœãƒ«/é–¢æ•°ã®ãƒªã‚¹ãƒˆ
        human_readable: äººé–“å¯èª­ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆå¾“æ¥å½¢å¼ï¼‰
    """
    error_type: str
    affected_class: Optional[str]
    affected_items: List[str]
    human_readable: str


# ============================================
# Complexity Assessment
# ============================================

class ComplexityAssessor:
    """è¤‡é›‘åº¦åˆ¤å®šã‚¯ãƒ©ã‚¹(3å±¤é˜²å¾¡)"""
    
    def __init__(self):
        self.simple_keywords = self._init_simple_keywords()
        self.medium_keywords = self._init_medium_keywords()
        self.complex_keywords = self._init_complex_keywords()
        self.exclusion_patterns = self._init_exclusion_patterns()
        
        # âœ… æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        self.existing_files = {}
        
        # ã‚µãƒãƒ¼ãƒˆã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã®å®šç¾©
        # repository_analyzer.pyã®SUPPORTED_LANGUAGESã«å¯¾å¿œ + è¿½åŠ ã®ä¸€èˆ¬çš„ãªè¨€èª
        supported_extensions = {
            # Python
            '.py', '.pyi',
            # JavaScript/TypeScript
            '.js', '.jsx', '.ts', '.tsx', '.mjs', '.cjs',
            # HTML/CSS
            '.html', '.htm', '.css', '.scss', '.sass', '.less',
            # è¨­å®šãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«
            '.json', '.yml', '.yaml', '.toml', '.ini', '.cfg', '.xml',
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
            '.md', '.mdx', '.txt', '.rst',
            # ãã®ä»–ã®è¨€èª
            '.rb',      # Ruby
            '.php',     # PHP
            '.java',    # Java
            '.c', '.cpp', '.cc', '.cxx', '.h', '.hpp',  # C/C++
            '.cs',      # C#
            '.go',      # Go
            '.rs',      # Rust
            '.swift',   # Swift
            '.kt',      # Kotlin
            '.scala',   # Scala
            '.sh', '.bash',  # Shell
            '.ps1',     # PowerShell
            '.sql',     # SQL
            '.vue',     # Vue
            '.svelte',  # Svelte
        }
        
        try:
            scanned_count = 0
            loaded_count = 0
            
            # ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³
            for file_path in Path.cwd().glob("*.*"):
                scanned_count += 1
                
                # ã‚µãƒãƒ¼ãƒˆã•ã‚Œã‚‹æ‹¡å¼µå­ã®ã¿èª­ã¿è¾¼ã‚€
                if file_path.suffix.lower() in supported_extensions:
                    try:
                        content = file_path.read_text(encoding='utf-8')
                        self.existing_files[file_path.name] = content
                        loaded_count += 1
                        logger.debug(f"[ComplexityAssessor] Loaded: {file_path.name} ({len(content)} chars)")
                    except Exception as read_error:
                        logger.debug(f"[ComplexityAssessor] Failed to read {file_path.name}: {read_error}")
                        pass
            
            logger.debug(f"[ComplexityAssessor] Scanned {scanned_count} files, loaded {loaded_count} files")
            logger.debug(f"[ComplexityAssessor] Loaded files: {list(self.existing_files.keys())}")
            
        except Exception as e:
            logger.debug(f"âš  Warning: Failed to scan some files: {e}")
            logger.debug(f"[ComplexityAssessor] Scan error: {e}")
        

        
    def _init_simple_keywords(self) -> List[str]:
        """Simple indicators (40+ keywords)"""
        return [
            # åŸºæœ¬çš„ãªæŒ¨æ‹¶ãƒ»è¡¨ç¤º
            'hello world', 'hello', 'hi', 'greet', 'welcome',
            'display', 'print', 'show', 'output',
            
            # åŸºæœ¬çš„ãªè¨ˆç®—ãƒ»å¤‰æ›
            'simple calculator', 'basic calculator', 'add subtract',
            'calculator', 'math', 'arithmetic',
            'converter', 'conversion', 'convert',
            'temperature', 'currency', 'unit',
            
            # åŸºæœ¬çš„ãªç”Ÿæˆ
            'random number', 'random', 'generate number',
            'dice', 'coin flip', 'lottery',
            
            # åŸºæœ¬çš„ãªã‚¿ã‚¤ãƒãƒ¼ãƒ»æ™‚è¨ˆ
            'timer', 'stopwatch', 'countdown', 'counter',
            'clock', 'time display', 'current time',
            
            # åŸºæœ¬çš„ãªã‚²ãƒ¼ãƒ ãƒ»ãƒ‘ã‚ºãƒ«
            'fizzbuzz', 'guess number', 'guessing game',
            'tic tac toe', 'rock paper scissors',
            
            # åŸºæœ¬çš„ãªæ–‡å­—åˆ—æ“ä½œ
            'reverse string', 'palindrome', 'word count',
            'character count', 'string manipulation',
            
            # ä¿®é£¾èª
            'simple', 'basic', 'minimal', 'quick', 'easy',
            'beginner', 'starter', 'demo', 'example',
            'prototype', 'proof of concept', 'poc'
        ]
    
    def _init_medium_keywords(self) -> List[str]:
        """Medium indicators (30+ keywords)"""
        return [
            # æ¨™æº–çš„ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
            'todo app', 'todo list', 'task manager',
            'note taking', 'notepad', 'markdown editor',
            'file manager', 'file browser', 'file explorer',
            'image gallery', 'photo gallery',
            'contact manager', 'address book',
            'bookmark manager', 'url shortener',
            
            # æ¨™æº–çš„ãªæ©Ÿèƒ½
            'crud', 'form validation', 'user input',
            'file upload', 'file download',
            'search function', 'filter', 'sort',
            'pagination', 'infinite scroll',
            
            # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ä½¿ç”¨(Mediumç›¸å½“)
            'react app', 'vue app', 'angular app',
            'express server', 'flask app', 'django app',
            'fastapi', 'spring boot',
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å˜ä½“ä½¿ç”¨
            'sqlite', 'local database', 'json storage',
            'csv file', 'data persistence'
        ]
    
    def _init_complex_keywords(self) -> List[str]:
        """Complex indicators (80+ keywords)"""
        return [
            # ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ‘ã‚¿ãƒ¼ãƒ³
            'microservice', 'monolith', 'service mesh',
            'event driven', 'event sourcing', 'cqrs',
            'hexagonal architecture', 'clean architecture',
            'domain driven design', 'ddd',
            
            # APIè¨­è¨ˆ
            'api gateway', 'rest api', 'restful api',
            'graphql', 'grpc', 'websocket server',
            'api versioning', 'api documentation',
            
            # èªè¨¼ãƒ»èªå¯
            'authentication system', 'authorization',
            'oauth', 'oauth2', 'openid', 'saml',
            'jwt', 'session management', 'sso',
            'multi factor', 'mfa', '2fa',
            'rbac', 'role based', 'permission system',
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ»æ°¸ç¶šåŒ–
            'database system', 'database design',
            'database migration', 'schema design',
            'orm design', 'data modeling',
            'transaction management', 'acid',
            'eventual consistency', 'consistency model',
            
            # åˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ 
            'distributed system', 'distributed',
            'message queue', 'message broker',
            'pub sub', 'event bus',
            'service discovery', 'load balancing',
            'circuit breaker', 'retry logic',
            'saga pattern', 'distributed transaction',
            
            # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£
            'scalable', 'horizontal scaling', 'vertical scaling',
            'auto scaling', 'load balancer',
            'caching strategy', 'cache layer',
            'cdn', 'content delivery',
            
            # ã‚¤ãƒ³ãƒ•ãƒ©ãƒ»ãƒ‡ãƒ—ãƒ­ã‚¤
            'kubernetes', 'k8s', 'docker swarm',
            'container orchestration',
            'ci cd', 'deployment pipeline',
            'blue green', 'canary deployment',
            'terraform', 'infrastructure as code',
            'ansible', 'chef', 'puppet',
            
            # ç›£è¦–ãƒ»é‹ç”¨
            'monitoring system', 'observability',
            'distributed tracing', 'log aggregation',
            'metrics collection', 'alerting system',
            'health check', 'readiness probe',
            
            # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£
            'security framework', 'encryption system',
            'key management', 'secrets management',
            'api security', 'penetration test',
            'vulnerability scan', 'security audit',
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
            'performance optimization', 'query optimization',
            'connection pooling', 'thread pool',
            'async processing', 'batch processing',
            
            # ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚º
            'enterprise', 'production grade',
            'high availability', 'disaster recovery',
            'backup strategy', 'failover',
            'multi tenant', 'multi region',
            
            # ãƒ‡ãƒ¼ã‚¿å‡¦ç†
            'data pipeline', 'etl', 'data warehouse',
            'stream processing', 'real time processing',
            'big data', 'data lake',
            
            # å…·ä½“çš„ãªæŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯(Complexç¢ºå®š)
            'elasticsearch', 'kafka', 'rabbitmq',
            'redis cluster', 'mongodb cluster',
            'postgres replication', 'mysql cluster'
        ]
    
    def _init_exclusion_patterns(self) -> Dict[str, List[str]]:
        """é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©"""
        return {
            'architecture': ['simple', 'basic', 'folder', 'file', 'directory'],
            'production': ['simple', 'basic'],
            'system': ['simple', 'basic'],
            'distributed': ['simple', 'basic'],
            'microservice': ['simple', 'basic', 'demo'],
            'enterprise': ['simple', 'basic', 'demo', 'example']
        }
    
    def assess_complexity(self, goal: str) -> Tuple[str, Dict[str, Any]]:
        """
        è¤‡é›‘åº¦ã‚’3å±¤é˜²å¾¡ã§åˆ¤å®š
        
        Args:
            goal: å®Ÿè£…ç›®æ¨™ã®èª¬æ˜
            
        Returns:
            (complexity, details) ã®ã‚¿ãƒ—ãƒ«
            - complexity: "simple" | "medium" | "complex"
            - details: åˆ¤å®šè©³ç´°æƒ…å ±(ãƒ‡ãƒãƒƒã‚°ç”¨)
        """
        goal_lower = goal.lower()
        
        # Layer 1: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º
        simple_matches = [kw for kw in self.simple_keywords if kw in goal_lower]
        medium_matches = [kw for kw in self.medium_keywords if kw in goal_lower]
        complex_matches = [kw for kw in self.complex_keywords if kw in goal_lower]
        
        # Layer 2: é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³é©ç”¨
        complex_matches_filtered = self._apply_exclusions(complex_matches, goal_lower)
        
        # Layer 3: ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        score = self._calculate_score(
            simple_matches, 
            medium_matches, 
            complex_matches_filtered,
            goal
        )
        
        # æœ€çµ‚åˆ¤å®š
        if score >= 2.0:
            complexity = "complex"
        elif score <= -0.5:
            complexity = "simple"
        else:
            complexity = "medium"
        
        # åˆ¤å®šè©³ç´°(ãƒ‡ãƒãƒƒã‚°ç”¨)
        details = {
            'score': score,
            'simple_matches': simple_matches,
            'medium_matches': medium_matches,
            'complex_matches_original': complex_matches,
            'complex_matches_filtered': complex_matches_filtered,
            'word_count': len(goal.split()),
            'goal_length': len(goal)
        }
        
        return complexity, details
    
    def _apply_exclusions(self, matches: List[str], goal: str) -> List[str]:
        """é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é©ç”¨ã—ã¦false positiveã‚’å‰Šæ¸›"""
        filtered = []
        
        for match in matches:
            should_exclude = False
            
            if match in self.exclusion_patterns:
                for excl_word in self.exclusion_patterns[match]:
                    if excl_word in goal:
                        should_exclude = True
                        break
            
            if not should_exclude:
                filtered.append(match)
        
        return filtered
    
    def _calculate_score(
        self, 
        simple_matches: List[str],
        medium_matches: List[str],
        complex_matches: List[str],
        goal: str
    ) -> float:
        """ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°æ–¹å¼ã§è¤‡é›‘åº¦ã‚’æ•°å€¤åŒ–"""
        score = 0.0
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒã«ã‚ˆã‚‹ã‚¹ã‚³ã‚¢
        score += len(complex_matches) * 2.0
        score += len(medium_matches) * 0.5
        score -= len(simple_matches) * 1.0
        
        # ãƒ¯ãƒ¼ãƒ‰æ•°ã«ã‚ˆã‚‹ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯
        words = goal.split()
        word_count = len(words)
        
        if word_count <= 3:
            score -= 0.5
        elif word_count >= 20:
            score += 1.0
        elif word_count >= 10:
            score += 0.3
        
        # æŠ€è¡“ç”¨èªå¯†åº¦ãƒã‚§ãƒƒã‚¯
        technical_terms = [
            'api', 'database', 'server', 'client',
            'frontend', 'backend', 'fullstack',
            'framework', 'library', 'module',
            'interface', 'integration', 'deployment',
            'service', 'component', 'middleware'
        ]
        
        goal_lower = goal.lower()
        tech_count = sum(1 for term in technical_terms if term in goal_lower)
        
        if tech_count >= 4:
            score += 1.5
        elif tech_count >= 2:
            score += 0.5
        
        # â­ ãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯æ¤œå‡º: ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰+ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã®çµ„ã¿åˆã‚ã›
        # ã“ã®çµ„ã¿åˆã‚ã›ã¯æœ¬è³ªçš„ã«è¤‡é›‘ã§ã‚ã‚Šã€complexã¨ã—ã¦æ‰±ã†ã¹ã
        backend_keywords = [
            # Pythonãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
            'flask', 'django', 'fastapi', 'bottle', 'pyramid',
            'tornado', 'sanic', 'starlette',
            # Node.jsãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
            'express', 'nestjs', 'koa', 'hapi', 'fastify',
            # ä¸€èˆ¬ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰
            'backend', 'server side', 'server-side',
            'rest api', 'graphql', 'websocket',
        ]
        
        frontend_keywords = [
            # æ˜ç¤ºçš„ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰
            'frontend', 'front-end', 'front end',
            'html', 'css', 'canvas',
            # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
            'react', 'vue', 'angular', 'svelte',
            # UIé–¢é€£
            'ui', 'user interface', 'web interface',
            'browser', 'client side', 'client-side',
        ]
        
        has_backend = any(kw in goal_lower for kw in backend_keywords)
        has_frontend = any(kw in goal_lower for kw in frontend_keywords)
        is_fullstack = has_backend and has_frontend
        
        if is_fullstack:
            # ãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯ã¯+2.0ã§complexç¢ºå®šï¼ˆé–¾å€¤2.0ï¼‰
            score += 2.0
        
        return score


class SemiAutoResult(NamedTuple):
    """Semi-auto implementation result
    
    Attributes:
        success: å®Ÿè£…ãŒæˆåŠŸã—ãŸã‹ã©ã†ã‹
        analysis: å®Ÿè£…ç›®æ¨™ã®åˆ†æçµæœ
        generated_code: ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ {filename: code}
        quality_scores: å“è³ªã‚¹ã‚³ã‚¢ {filename: score}
        recommendations: æ¨å¥¨äº‹é …ã®ãƒªã‚¹ãƒˆ
        error: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸(å¤±æ•—æ™‚)
        impact_analysis: å½±éŸ¿åˆ†æçµæœ {filename: impact_data} (â­ ãƒ•ã‚§ãƒ¼ã‚º1ã§è¿½åŠ )
        lint_result: Lintå®Ÿè¡Œçµæœ {has_errors, errors, warnings, ...} (â­ [t] Try againå¯¾å¿œ)
        zen_summary: Zen HUDç”¨ã‚µãƒãƒªãƒ¼ {lint: {...}, review: {...}} (â­ Zen HUDå¯¾å¿œ)
    """
    success: bool
    analysis: str = ""
    generated_code: Dict[str, str] = {}
    quality_scores: Dict[str, float] = {}
    recommendations: List[str] = []
    error: str = ""
    impact_analysis: Dict[str, Any] = {}  # â­ æ–°è¦è¿½åŠ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
    lint_result: Optional[Dict[str, Any]] = None  # â­ [t] Try againå¯¾å¿œ
    zen_summary: Optional[Dict[str, Any]] = None  # â­ Zen HUDç”¨ã‚µãƒãƒªãƒ¼


class ApplyResult(NamedTuple):
    """Code application result"""
    success: bool
    applied_files: List[str] = []
    backup_paths: List[str] = []
    quality_scores: Dict[str, float] = {}
    error: str = ""


# ============================================
# File Details Summary for [v] View details
# ============================================

def get_file_details_summary(
    generated_code: Dict[str, str],
    quality_scores: Dict[str, float],
    lint_result: Optional[Dict[str, Any]],
    zen_summary: Optional[Dict[str, Any]],
    filename: str
) -> Dict[str, Any]:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°ã‚µãƒãƒªãƒ¼ã‚’å–å¾—ï¼ˆ[v] View detailsç”¨ï¼‰
    
    Args:
        generated_code: ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ {filename: code}
        quality_scores: å“è³ªã‚¹ã‚³ã‚¢ {filename: score}
        lint_result: Lintå®Ÿè¡Œçµæœ
        zen_summary: Zen HUDç”¨ã‚µãƒãƒªãƒ¼
        filename: å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«å
        
    Returns:
        ãƒ•ã‚¡ã‚¤ãƒ«è©³ç´°ã‚µãƒãƒªãƒ¼ dict
    """
    # è¡Œæ•°å–å¾—
    code = generated_code.get(filename, "")
    lines = len(code.split('\n')) if code else 0
    
    # å“è³ªã‚¹ã‚³ã‚¢ã¨ã‚°ãƒ¬ãƒ¼ãƒ‰
    score = quality_scores.get(filename, 0.0)
    if score >= 0.9:
        grade = "Excellent"
    elif score >= 0.75:
        grade = "Good"
    elif score >= 0.6:
        grade = "Fair"
    else:
        grade = "Needs Review"
    
    # Review Issuesï¼ˆã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ï¼‰
    review_issues = []
    if zen_summary:
        all_issues = zen_summary.get("review", {}).get("issues", [])
        for issue in all_issues:
            issue_file = issue.get("file", "")
            # ãƒ•ã‚¡ã‚¤ãƒ«åãƒãƒƒãƒãƒ³ã‚°ï¼ˆWindows/Unixä¸¡å¯¾å¿œï¼‰
            # ãƒãƒƒã‚¯ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã¨ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã®ä¸¡æ–¹ã‚’ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿ã¨ã—ã¦æ‰±ã†
            issue_basename = issue_file.replace("\\", "/").split("/")[-1]
            target_basename = filename.replace("\\", "/").split("/")[-1]
            if issue_basename == target_basename or issue_file == filename:
                review_issues.append({
                    "line": issue.get("line", "?"),
                    "type": issue.get("type", 0),  # Issue Typeç•ªå·
                    "description": issue.get("description", ""),
                    "fix": issue.get("fix", "")
                })
    
    # Lint Errors/Warningsï¼ˆã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ï¼‰
    lint_errors = []
    lint_warnings = []
    if lint_result:
        for err in lint_result.get("errors", []):
            err_file = err.get("file", "")
            err_basename = err_file.replace("\\", "/").split("/")[-1]
            target_basename = filename.replace("\\", "/").split("/")[-1]
            if err_basename == target_basename or err_file == filename:
                lint_errors.append({
                    "line": err.get("line", "?"),
                    "message": err.get("message", ""),
                    "rule": err.get("rule", "")
                })
        
        for warn in lint_result.get("warnings", []):
            warn_file = warn.get("file", "")
            warn_basename = warn_file.replace("\\", "/").split("/")[-1]
            target_basename = filename.replace("\\", "/").split("/")[-1]
            if warn_basename == target_basename or warn_file == filename:
                lint_warnings.append({
                    "line": warn.get("line", "?"),
                    "message": warn.get("message", ""),
                    "rule": warn.get("rule", "")
                })
    
    return {
        "filename": filename,
        "lines": lines,
        "quality_score": score,
        "quality_percent": int(score * 100),
        "quality_grade": grade,
        "review_issues": review_issues,
        "lint_errors": lint_errors,
        "lint_warnings": lint_warnings
    }


def format_file_details_for_display(details: Dict[str, Any]) -> str:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«è©³ç´°ã‚’è¡¨ç¤ºç”¨æ–‡å­—åˆ—ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆ[v] View detailsç”¨ï¼‰
    
    ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰è¡¨ç¤ºå¾Œã«è¿½åŠ è¡¨ç¤ºã™ã‚‹ã‚µãƒãƒªãƒ¼éƒ¨åˆ†ã‚’ç”Ÿæˆ
    
    Args:
        details: get_file_details_summaryã®æˆ»ã‚Šå€¤
        
    Returns:
        è¡¨ç¤ºç”¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿æ–‡å­—åˆ—
    """
    output_lines = []
    separator = "â”€" * 67
    
    output_lines.append("")
    output_lines.append(separator)
    output_lines.append(f"Quality: {details['quality_percent']}% {details['quality_grade']}")
    output_lines.append("")
    
    # Review Issues
    # CRITICAL Types: ãã‚‚ãã‚‚èµ·å‹•ã—ãªã„ãƒã‚°
    # - Type 18: Circular imports â†’ ImportErrorç™ºç”Ÿã€ãƒ—ãƒ­ã‚»ã‚¹å³çµ‚äº†
    # â€» SyntaxError/IndentationErrorã¯Linterã§æ¤œå‡º
    # â€» æ‹¬å¼§ã‚¢ãƒ³ãƒãƒ©ãƒ³ã‚¹ï¼ˆHTML/JSï¼‰ã¯SICã§æ¤œå‡º
    CRITICAL_TYPES = {18}  # Circular imports ã®ã¿
    
    review_issues = details.get("review_issues", [])
    if review_issues:
        output_lines.append(f"â“˜ Review Issues ({len(review_issues)}):")
        
        # è¡Œç•ªå·ã‚ã‚Š/ãªã—ã§åˆ†é›¢ã—ã¦ã‚½ãƒ¼ãƒˆ
        # è¡Œç•ªå·ã‚ã‚Š â†’ å…ˆã«è¡¨ç¤ºï¼ˆå…·ä½“çš„ã§ç›´ã—ã‚„ã™ã„ï¼‰
        # è¡Œç•ªå·ãªã— â†’ å¾Œã«è¡¨ç¤ºï¼ˆ[MISSING]ã‚¿ã‚°ä»˜ãï¼‰
        def has_line_number(issue):
            line = issue.get("line")
            if line is None or line == "?" or line == "None":
                return False
            # æ•°å€¤ã¾ãŸã¯æ•°å­—æ–‡å­—åˆ—ã‹ã¤0ã‚ˆã‚Šå¤§ãã„
            try:
                return int(line) > 0
            except (ValueError, TypeError):
                return False
        
        issues_with_line = [i for i in review_issues if has_line_number(i)]
        issues_without_line = [i for i in review_issues if not has_line_number(i)]
        
        # è¡Œç•ªå·ã‚ã‚Šã¯è¡Œç•ªå·é †ã«ã‚½ãƒ¼ãƒˆ
        issues_with_line.sort(key=lambda x: int(x.get("line", 0)))
        
        # çµåˆï¼ˆè¡Œç•ªå·ã‚ã‚Š â†’ è¡Œç•ªå·ãªã—ï¼‰
        sorted_issues = issues_with_line + issues_without_line
        
        for issue in sorted_issues[:10]:  # æœ€å¤§10ä»¶è¡¨ç¤º
            line_num = issue.get("line")
            issue_type = issue.get("type", 0)
            desc = issue.get("description", "")
            
            # è¡Œç•ªå·ã®æœ‰ç„¡ã‚’åˆ¤å®š
            has_line = has_line_number(issue)
            
            # ã‚¿ã‚°æ±ºå®š
            if issue_type in CRITICAL_TYPES:
                tag = "[CRITICAL] "
            elif not has_line:
                tag = "[MISSING] "
            else:
                tag = ""
            
            # è¡¨ç¤ºå½¢å¼
            if has_line:
                prefix = f"  Line {line_num}: "
            else:
                prefix = f"  "
            
            # é•·ã„èª¬æ˜ã¯æŠ˜ã‚Šè¿”ã—ï¼ˆ55æ–‡å­—ã§åˆ†å‰²ï¼‰
            first_line_max = 55 - len(tag)
            if len(desc) > first_line_max:
                output_lines.append(f"{prefix}{tag}{desc[:first_line_max]}")
                # æ®‹ã‚Šã‚’æŠ˜ã‚Šè¿”ã—è¡¨ç¤ºï¼ˆã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆèª¿æ•´ï¼‰
                indent = "           " if has_line else "            "
                remaining = desc[first_line_max:]
                while remaining:
                    output_lines.append(f"{indent}{remaining[:55]}")
                    remaining = remaining[55:]
            else:
                output_lines.append(f"{prefix}{tag}{desc}")
    else:
        output_lines.append("âœ“ Review: No issues")
    
    output_lines.append("")
    
    # Lint Errors
    lint_errors = details.get("lint_errors", [])
    lint_warnings = details.get("lint_warnings", [])
    
    if lint_errors:
        output_lines.append(f"âš  Lint Errors ({len(lint_errors)}):")
        for err in lint_errors[:5]:  # æœ€å¤§5ä»¶è¡¨ç¤º
            line_num = err.get("line", "?")
            message = err.get("message", "")
            rule = err.get("rule", "")
            rule_suffix = f" ({rule})" if rule else ""
            output_lines.append(f"  Line {line_num}: {message}{rule_suffix}")
    
    if lint_warnings:
        output_lines.append(f"âš  Lint Warnings ({len(lint_warnings)}):")
        for warn in lint_warnings[:5]:  # æœ€å¤§5ä»¶è¡¨ç¤º
            line_num = warn.get("line", "?")
            message = warn.get("message", "")
            rule = warn.get("rule", "")
            rule_suffix = f" ({rule})" if rule else ""
            output_lines.append(f"  Line {line_num}: {message}{rule_suffix}")
    
    if not lint_errors and not lint_warnings:
        output_lines.append("âœ“ Lint: No errors")
    
    output_lines.append(separator)
    
    return "\n".join(output_lines)

from typing import Dict, List, Optional, Set, NamedTuple
from pathlib import Path
import re

class FileRequirement(NamedTuple):
    """ãƒ•ã‚¡ã‚¤ãƒ«è¦ä»¶ã®å®šç¾©"""
    filepath: str           # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    file_type: str         # 'html', 'js', 'css', 'python' ãªã©
    reason: str            # å¿…è¦ãªç†ç”±
    priority: str          # 'required', 'optional'


class ValidationResult(NamedTuple):
    """æ¤œè¨¼çµæœ"""
    is_complete: bool                    # å®Œå…¨ã‹
    missing_files: List[str]             # ä¸è¶³ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    missing_requirements: List[FileRequirement]  # ä¸è¶³è¦ä»¶
    reasons: List[str]                   # ä¸è¶³ç†ç”±


class PostGenerationValidator:
    """
    äº‹å¾Œæ¤œè¨¼ã‚¯ãƒ©ã‚¹
    ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦ä»¶ã‚’æº€ãŸã—ã¦ã„ã‚‹ã‹æ¤œè¨¼ã—ã€
    ä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯è¿½åŠ ç”Ÿæˆã‚’å®Ÿè¡Œ
    """
    
    def __init__(self, llm_manager, repository_analyzer, logger):
        """
        Args:
            llm_manager: LLMãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
            repository_analyzer: ãƒªãƒã‚¸ãƒˆãƒªã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼
            logger: ãƒ­ã‚¬ãƒ¼
        """
        self.llm_manager = llm_manager
        self.repository_analyzer = repository_analyzer
        self.logger = logger
        self.no_framework_mode = False  # ğŸ†• ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ç¦æ­¢ãƒ¢ãƒ¼ãƒ‰
    
    def validate_and_complete(
        self,
        generated_files: Dict[str, str],
        goal: str,
        multi_file_detection: dict
    ) -> Dict[str, str]:
        """
        ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œè¨¼ã—ã€ä¸è¶³ãŒã‚ã‚Œã°è£œå®Œ
        
        Args:
            generated_files: LLMãŒç”Ÿæˆã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ç¾¤ {filename: code}
            goal: ãƒ¦ãƒ¼ã‚¶ãƒ¼ç›®æ¨™
            multi_file_detection: è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡ºçµæœ
        
        Returns:
            å®Œå…¨ãªãƒ•ã‚¡ã‚¤ãƒ«ç¾¤ (å¿…è¦ã«å¿œã˜ã¦è¿½åŠ ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å«ã‚€)
        """
        # è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ä¸è¦ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if not multi_file_detection.get('required', False):
            self.logger.debug("[Post-Validation] Multi-file not required, skipping validation")
            return generated_files
        
        self.logger.debug("[Post-Validation] Starting post-generation validation...")
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: æœŸå¾…ã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’äºˆæ¸¬
        expected_files = self._predict_required_files(
            goal=goal,
            multi_file_detection=multi_file_detection
        )
        
        if not expected_files:
            self.logger.debug("[Post-Validation] No expected files predicted, skipping validation")
            return generated_files
        
        self.logger.debug(f"[Post-Validation] Expected files: {[f.filepath for f in expected_files]}")
        self.logger.debug(f"[Post-Validation] Generated files: {list(generated_files.keys())}")
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: å®Œå…¨æ€§ã‚’æ¤œè¨¼
        validation_result = self._validate_completeness(
            generated_files=generated_files,
            expected_files=expected_files,
            goal=goal
        )
        
        if validation_result.is_complete:
            self.logger.debug("[Post-Validation] âœ… All required files generated")
            return generated_files
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: ä¸è¶³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ ç”Ÿæˆ
        self.logger.debug(f"[Post-Validation] âš   Incomplete generation detected")
        self.logger.debug(f"[Post-Validation] Missing files: {validation_result.missing_files}")
        self.logger.debug(f"[Post-Validation] Reasons: {validation_result.reasons}")
        
        completed_files = self._generate_missing_files(
            generated_files=generated_files,
            missing_requirements=validation_result.missing_requirements,
            goal=goal
        )
        
        
        # ==========================================
        # ã€æ–°è¦è¿½åŠ ã€‘Importä¾å­˜é–¢ä¿‚ã®æ¤œè¨¼ï¼ˆå¯¾ç­–4ï¼‰
        # ==========================================
        self.logger.debug("[Post-Validation] Validating import dependencies...")
        
        missing_imports = self._validate_import_dependencies(completed_files)
        
        if missing_imports:
            self.logger.debug(f"[Post-Validation] âš   Missing {len(missing_imports)} imported files")
            self.logger.debug(f"[Post-Validation] Missing imports: {[req.filepath for req in missing_imports]}")
            
            # æ¬ è½ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
            completed_files = self._generate_missing_files(
                generated_files=completed_files,
                missing_requirements=missing_imports,
                goal=goal
            )
            
            self.logger.debug(f"[Post-Validation] Import validation completed: {len(completed_files)} total files")
        else:
            self.logger.debug("[Post-Validation] âœ… All imports are satisfied")
        
        # ğŸ†• requirements.txt äº’æ›æ€§ãƒã‚§ãƒƒã‚¯
        self.logger.debug("[Post-Validation] Checking requirements.txt compatibility...")
        requirements_check = self._validate_requirements_compatibility(completed_files)
        
        if requirements_check['has_issues']:
            self.logger.warning(f"[Post-Validation] âš ï¸ Requirements compatibility issues found:")
            for issue in requirements_check['issues']:
                self.logger.warning(f"[Post-Validation]   - {issue}")
            
            # ä¿®æ­£ç‰ˆã§ç½®ãæ›ãˆ
            if requirements_check['fixed_content']:
                for filepath in list(completed_files.keys()):
                    if filepath.endswith('requirements.txt'):
                        self.logger.info(f"[Post-Validation] ğŸ”§ Auto-fixing {filepath} for Python compatibility")
                        completed_files[filepath] = requirements_check['fixed_content']
                        break
        else:
            self.logger.debug("[Post-Validation] âœ… Requirements.txt compatibility OK")
        
        return completed_files
    
    def _validate_import_dependencies(self, generated_files: Dict[str, str]) -> List[FileRequirement]:
        """
        ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®importæ–‡ã‚’è§£æã—ã€æ¬ è½ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡º
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            æ¬ è½ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã® FileRequirement ãƒªã‚¹ãƒˆ
        
        Examples:
            # JavaScript: src/main.jsx ãŒä»¥ä¸‹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ:
            import { store } from "./store/index.js"
            # ã—ã‹ã— store/index.js ãŒç”Ÿæˆã•ã‚Œã¦ã„ãªã„å ´åˆ:
            â†’ [FileRequirement(filepath='src/store/index.js', ...)] ã‚’è¿”ã™
            
            # Python: main.py ãŒä»¥ä¸‹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ:
            from services import ProductService
            # ã—ã‹ã— services/ ãŒç”Ÿæˆã•ã‚Œã¦ã„ãªã„å ´åˆ:
            â†’ [FileRequirement(filepath='services/__init__.py', ...)] ã‚’è¿”ã™
        """
        import os
        
        missing_requirements = []
        
        self.logger.debug("[Import Validation] Analyzing import dependencies...")
        
        # ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’æŠ½å‡º
        generated_modules = self._extract_python_modules(generated_files)
        self.logger.debug(f"[Import Validation] Generated Python modules: {generated_modules}")
        
        for filepath, content in generated_files.items():
            # ==========================================
            # Python ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ¤œè¨¼
            # ==========================================
            if filepath.endswith('.py'):
                missing_requirements.extend(
                    self._validate_python_imports(filepath, content, generated_files, generated_modules)
                )
                continue
            
            # ==========================================
            # HTML ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚½ãƒ¼ã‚¹å‚ç…§æ¤œè¨¼
            # ==========================================
            if filepath.endswith(('.html', '.htm')):
                missing_requirements.extend(
                    self._validate_html_resources(filepath, content, generated_files)
                )
                continue
            
            # ==========================================
            # JavaScript/TypeScript/JSX ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ¤œè¨¼
            # ==========================================
            if not filepath.endswith(('.js', '.jsx', '.ts', '.tsx')):
                continue
            
            # importæ–‡ã‚’æŠ½å‡º
            # ãƒ‘ã‚¿ãƒ¼ãƒ³1: import { X } from "path"
            # ãƒ‘ã‚¿ãƒ¼ãƒ³2: import X from "path"
            # ãƒ‘ã‚¿ãƒ¼ãƒ³3: import "path"
            import_pattern = r"import\s+(?:.*?\s+from\s+)?[\"'](.+?)[\"']"
            imports = re.findall(import_pattern, content)
            
            for import_path in imports:
                # node_modules ã‚„ çµ¶å¯¾ãƒ‘ã‚¹ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¯ã‚¹ã‚­ãƒƒãƒ—
                if not (import_path.startswith('./') or import_path.startswith('../')):
                    continue
                
                # ç›¸å¯¾ãƒ‘ã‚¹ã‚’çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›
                file_dir = os.path.dirname(filepath)
                
                # ãƒ‘ã‚¹ã‚’æ­£è¦åŒ–
                if import_path.startswith('./'):
                    resolved_path = os.path.join(file_dir, import_path[2:])
                elif import_path.startswith('../'):
                    resolved_path = os.path.join(file_dir, import_path)
                else:
                    resolved_path = import_path
                
                # ãƒ‘ã‚¹ã‚’æ­£è¦åŒ–ï¼ˆ../ ã‚’è§£æ±ºï¼‰
                resolved_path = os.path.normpath(resolved_path)
                
                # Windowsãƒ‘ã‚¹åŒºåˆ‡ã‚Šã‚’Unixå½¢å¼ã«å¤‰æ›
                resolved_path = resolved_path.replace('\\', '/')
                
                # æ‹¡å¼µå­ã®è£œå®Œ
                if not resolved_path.endswith(('.js', '.jsx', '.ts', '.tsx', '.json', '.css')):
                    # æ‹¡å¼µå­ãªã—ã®å ´åˆã€è¤‡æ•°ã®å€™è£œã‚’è©¦ã™
                    candidates = [
                        resolved_path + '.js',
                        resolved_path + '.jsx',
                        resolved_path + '.ts',
                        resolved_path + '.tsx',
                        resolved_path + '/index.js',
                        resolved_path + '/index.jsx',
                    ]
                    
                    found = False
                    for candidate in candidates:
                        if candidate in generated_files:
                            found = True
                            break
                    
                    if not found:
                        # æœ€ã‚‚ä¸€èˆ¬çš„ãª .js ã‚’ä½¿ç”¨
                        missing_path = resolved_path + '.js'
                        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                        if not any(req.filepath == missing_path for req in missing_requirements):
                            missing_requirements.append(FileRequirement(
                                filepath=missing_path,
                                file_type='js',
                                reason=f'Imported by {filepath}',
                                priority='required'
                            ))
                            self.logger.debug(f"[Import Validation] Missing: {missing_path} (imported from {filepath})")
                else:
                    # æ‹¡å¼µå­ã‚ã‚Šã®å ´åˆã€ãã®ã¾ã¾ç¢ºèª
                    if resolved_path not in generated_files:
                        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                        if not any(req.filepath == resolved_path for req in missing_requirements):
                            file_type = resolved_path.split('.')[-1]
                            if file_type not in ['js', 'jsx', 'ts', 'tsx', 'json', 'css']:
                                file_type = 'js'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
                            
                            missing_requirements.append(FileRequirement(
                                filepath=resolved_path,
                                file_type=file_type,
                                reason=f'Imported by {filepath}',
                                priority='required'
                            ))
                            self.logger.debug(f"[Import Validation] Missing: {resolved_path} (imported from {filepath})")
        
        self.logger.debug(f"[Import Validation] Found {len(missing_requirements)} missing imports")
        return missing_requirements
    
    def _extract_python_modules(self, generated_files: Dict[str, str]) -> set:
        """
        ç”Ÿæˆã•ã‚ŒãŸPythonãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åã‚’æŠ½å‡º
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åã®ã‚»ãƒƒãƒˆ (ä¾‹: {'models', 'views', 'services', 'config'})
        """
        modules = set()
        
        for filepath in generated_files.keys():
            if not filepath.endswith('.py'):
                continue
            
            # ãƒ‘ã‚¹ã‹ã‚‰ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åã‚’æŠ½å‡º
            # ä¾‹: "models/product.py" â†’ "models"
            # ä¾‹: "services/__init__.py" â†’ "services"
            # ä¾‹: "main.py" â†’ "main"
            parts = filepath.replace('\\', '/').split('/')
            
            if len(parts) >= 2:
                # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒã‚ã‚‹å ´åˆã€æœ€åˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã—ã¦ç™»éŒ²
                modules.add(parts[0])
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ‹¡å¼µå­ãªã—ï¼‰ã‚‚ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã—ã¦ç™»éŒ²
            filename = parts[-1].replace('.py', '')
            if filename != '__init__':
                modules.add(filename)
        
        return modules
    
    def _validate_html_resources(
        self,
        filepath: str,
        content: str,
        generated_files: Dict[str, str]
    ) -> List[FileRequirement]:
        """
        HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚½ãƒ¼ã‚¹å‚ç…§ï¼ˆCSS, JavaScriptï¼‰ã‚’è§£æã—ã€æ¬ è½ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡º
        
        Args:
            filepath: HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            content: HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            æ¬ è½ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã® FileRequirement ãƒªã‚¹ãƒˆ
        
        Examples:
            # HTML: index.html ãŒä»¥ä¸‹ã‚’å‚ç…§:
            <link rel="stylesheet" href="css/styles.css">
            <script src="js/app.js"></script>
            # ã—ã‹ã— css/styles.css ãŒç”Ÿæˆã•ã‚Œã¦ã„ãªã„å ´åˆ:
            â†’ [FileRequirement(filepath='css/styles.css', ...)] ã‚’è¿”ã™
        """
        import os
        import re
        
        missing = []
        file_dir = os.path.dirname(filepath)
        
        self.logger.debug(f"[HTML Resource Validation] Analyzing: {filepath}")
        
        # ==========================================
        # CSSå‚ç…§ã®æ¤œå‡º: <link rel="stylesheet" href="...">
        # ==========================================
        # ãƒ‘ã‚¿ãƒ¼ãƒ³: <link rel="stylesheet" href="path">
        # ãƒ‘ã‚¿ãƒ¼ãƒ³: <link href="path" rel="stylesheet">
        css_patterns = [
            r'<link[^>]+rel=["\']stylesheet["\'][^>]+href=["\']([^"\']+)["\']',
            r'<link[^>]+href=["\']([^"\']+)["\'][^>]+rel=["\']stylesheet["\']',
            r'<link[^>]+href=["\']([^"\']+\.css)["\']'
        ]
        
        css_refs = set()
        for pattern in css_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            css_refs.update(matches)
        
        for css_path in css_refs:
            # å¤–éƒ¨URLï¼ˆhttp://, https://, //ï¼‰ã¯ã‚¹ã‚­ãƒƒãƒ—
            if css_path.startswith(('http://', 'https://', '//', 'data:')):
                continue
            
            # CDNå‚ç…§ã¯ã‚¹ã‚­ãƒƒãƒ—
            if 'cdn' in css_path.lower() or 'unpkg' in css_path.lower() or 'jsdelivr' in css_path.lower():
                continue
            
            # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ§‹æ–‡ï¼ˆJinja2/Django/ERB/PHPï¼‰ã¯ã‚¹ã‚­ãƒƒãƒ—
            # ã“ã‚Œã‚‰ã¯ã‚µãƒ¼ãƒãƒ¼ã‚µã‚¤ãƒ‰ã§å‹•çš„ã«è§£æ±ºã•ã‚Œã‚‹ãƒ‘ã‚¹ã§ã‚ã‚Šã€é™çš„ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦æ¤œè¨¼ã™ã¹ãã§ã¯ãªã„
            if '{{' in css_path or '{%' in css_path or '<%' in css_path or '<?' in css_path:
                continue
            
            # ç›¸å¯¾ãƒ‘ã‚¹ã‚’è§£æ±º
            if css_path.startswith('/'):
                # çµ¶å¯¾ãƒ‘ã‚¹ï¼ˆãƒ«ãƒ¼ãƒˆã‹ã‚‰ã®ç›¸å¯¾ï¼‰
                resolved_path = css_path.lstrip('/')
            elif css_path.startswith('./'):
                resolved_path = os.path.join(file_dir, css_path[2:])
            elif css_path.startswith('../'):
                resolved_path = os.path.join(file_dir, css_path)
            else:
                # ç›¸å¯¾ãƒ‘ã‚¹
                if file_dir:
                    resolved_path = os.path.join(file_dir, css_path)
                else:
                    resolved_path = css_path
            
            # ãƒ‘ã‚¹ã‚’æ­£è¦åŒ–
            resolved_path = os.path.normpath(resolved_path).replace('\\', '/')
            
            # ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ã«å­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
            if resolved_path not in generated_files:
                # static/ ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ä»˜ãã‚‚ç¢ºèª
                alt_path = f"static/{resolved_path}"
                if alt_path not in generated_files:
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    if not any(req.filepath == resolved_path for req in missing):
                        missing.append(FileRequirement(
                            filepath=resolved_path,
                            file_type='css',
                            reason=f'Referenced by {filepath}: <link href="{css_path}">',
                            priority='required'
                        ))
                        self.logger.debug(f"[HTML Resource Validation] Missing CSS: {resolved_path}")
        
        # ==========================================
        # JavaScriptå‚ç…§ã®æ¤œå‡º: <script src="...">
        # ==========================================
        js_pattern = r'<script[^>]+src=["\']([^"\']+)["\']'
        js_refs = re.findall(js_pattern, content, re.IGNORECASE)
        
        for js_path in js_refs:
            # å¤–éƒ¨URLï¼ˆhttp://, https://, //ï¼‰ã¯ã‚¹ã‚­ãƒƒãƒ—
            if js_path.startswith(('http://', 'https://', '//', 'data:')):
                continue
            
            # CDNå‚ç…§ã¯ã‚¹ã‚­ãƒƒãƒ—
            if 'cdn' in js_path.lower() or 'unpkg' in js_path.lower() or 'jsdelivr' in js_path.lower():
                continue
            
            # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ§‹æ–‡ï¼ˆJinja2/Django/ERB/PHPï¼‰ã¯ã‚¹ã‚­ãƒƒãƒ—
            # ã“ã‚Œã‚‰ã¯ã‚µãƒ¼ãƒãƒ¼ã‚µã‚¤ãƒ‰ã§å‹•çš„ã«è§£æ±ºã•ã‚Œã‚‹ãƒ‘ã‚¹ã§ã‚ã‚Šã€é™çš„ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦æ¤œè¨¼ã™ã¹ãã§ã¯ãªã„
            if '{{' in js_path or '{%' in js_path or '<%' in js_path or '<?' in js_path:
                continue
            
            # ç›¸å¯¾ãƒ‘ã‚¹ã‚’è§£æ±º
            if js_path.startswith('/'):
                resolved_path = js_path.lstrip('/')
            elif js_path.startswith('./'):
                resolved_path = os.path.join(file_dir, js_path[2:])
            elif js_path.startswith('../'):
                resolved_path = os.path.join(file_dir, js_path)
            else:
                if file_dir:
                    resolved_path = os.path.join(file_dir, js_path)
                else:
                    resolved_path = js_path
            
            # ãƒ‘ã‚¹ã‚’æ­£è¦åŒ–
            resolved_path = os.path.normpath(resolved_path).replace('\\', '/')
            
            # ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ã«å­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
            if resolved_path not in generated_files:
                # static/ ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ä»˜ãã‚‚ç¢ºèª
                alt_path = f"static/{resolved_path}"
                if alt_path not in generated_files:
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    if not any(req.filepath == resolved_path for req in missing):
                        missing.append(FileRequirement(
                            filepath=resolved_path,
                            file_type='js',
                            reason=f'Referenced by {filepath}: <script src="{js_path}">',
                            priority='required'
                        ))
                        self.logger.debug(f"[HTML Resource Validation] Missing JS: {resolved_path}")
        
        self.logger.debug(f"[HTML Resource Validation] Found {len(missing)} missing resources in {filepath}")
        return missing
    
    def _validate_python_imports(
        self, 
        filepath: str, 
        content: str, 
        generated_files: Dict[str, str],
        generated_modules: set
    ) -> List[FileRequirement]:
        """
        Pythonãƒ•ã‚¡ã‚¤ãƒ«ã®importæ–‡ã‚’è§£æã—ã€æ¬ è½ã—ãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’æ¤œå‡º
        
        Args:
            filepath: æ¤œè¨¼å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            content: ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“
            generated_modules: ç”Ÿæˆã•ã‚ŒãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åã®ã‚»ãƒƒãƒˆ
        
        Returns:
            æ¬ è½ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã® FileRequirement ãƒªã‚¹ãƒˆ
        """
        missing = []
        
        # Pythonæ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆä¸»è¦ãªã‚‚ã®ï¼‰
        STDLIB_MODULES = {
            'os', 'sys', 're', 'json', 'typing', 'datetime', 'time', 'math',
            'collections', 'itertools', 'functools', 'pathlib', 'logging',
            'unittest', 'abc', 'dataclasses', 'enum', 'copy',
            'hashlib', 'uuid', 'random', 'string', 'io', 'contextlib',
            'threading', 'multiprocessing', 'asyncio', 'concurrent',
            'http', 'urllib', 'socket', 'email', 'html', 'xml',
            'csv', 'sqlite3', 'pickle', 'shelve', 'dbm',
            'importlib', 'inspect', 'traceback', 'warnings', 'argparse',
            'configparser', 'shutil', 'tempfile', 'glob', 'fnmatch',
            'base64', 'binascii', 'struct', 'codecs', 'unicodedata',
            'locale', 'gettext', 'calendar', 'zoneinfo',
            'decimal', 'fractions', 'statistics', 'cmath',
            'array', 'bisect', 'heapq', 'queue', 'types', 'weakref',
            'textwrap', 'difflib', 'pprint', 'reprlib',
            'operator', 'keyword', 'token', 'tokenize', 'ast',
            'dis', 'code', 'codeop', 'compileall', 'py_compile',
            'zipfile', 'tarfile', 'gzip', 'bz2', 'lzma', 'zlib',
            'secrets', 'hmac', 'ssl',
            'subprocess', 'sched', 'signal', 'mmap', 'ctypes', 'platform',
            '__future__', 'builtins', 'atexit', 'gc', 'resource',
        }
        
        # ã‚ˆãä½¿ã‚ã‚Œã‚‹ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
        THIRD_PARTY_PACKAGES = {
            # Web Frameworks
            'flask', 'flask_sqlalchemy', 'flask_cors', 'flask_login', 'flask_wtf',
            'flask_migrate', 'flask_restful', 'flask_jwt_extended', 'flask_mail',
            # â­ Flask Extensions (è¿½åŠ )
            'flask_bcrypt', 'flask_session', 'flask_socketio', 'flask_caching',
            'flask_limiter', 'flask_admin', 'flask_babel', 'flask_security',
            'flask_marshmallow', 'flask_debugtoolbar', 'flask_principal',
            'flask_compress', 'flask_talisman', 'flask_moment', 'flask_assets',
            'django', 'fastapi', 'starlette', 'uvicorn', 'gunicorn', 'werkzeug',
            'bottle', 'tornado', 'aiohttp', 'sanic', 'quart', 'falcon',
            # â­ Django Extensions (è¿½åŠ )
            'rest_framework', 'djangorestframework', 'django_cors_headers',
            'django_filter', 'django_extensions', 'django_debug_toolbar',
            'django_allauth', 'django_celery_beat', 'django_redis',
            'django_storages', 'django_crispy_forms', 'crispy_forms',
            # Database
            'sqlalchemy', 'alembic', 'pymysql', 'psycopg2', 'pymongo', 'redis',
            'peewee', 'tortoise', 'databases', 'motor', 'asyncpg', 'aiomysql',
            # â­ Database Extensions (è¿½åŠ )
            'sqlmodel', 'mongoengine', 'elasticsearch', 'elasticsearch_dsl', 'cassandra',
            # Data Validation
            'marshmallow', 'pydantic', 'cerberus', 'voluptuous', 'attrs',
            # HTTP/API
            'requests', 'httpx', 'aiohttp', 'urllib3', 'httplib2',
            # â­ GraphQL (è¿½åŠ )
            'graphene', 'graphene_django', 'strawberry', 'ariadne',
            # Data Science
            'numpy', 'pandas', 'scipy', 'matplotlib', 'seaborn', 'plotly',
            'sklearn', 'scikit_learn', 'tensorflow', 'torch', 'keras',
            # Testing
            'pytest', 'pytest_flask', 'pytest_cov', 'unittest', 'mock', 'faker',
            'hypothesis', 'coverage', 'tox', 'nox',
            # â­ Testing Extensions (è¿½åŠ )
            'pytest_asyncio', 'pytest_django', 'pytest_mock', 'responses',
            'vcrpy', 'factory_boy',
            # Utilities
            'python_dotenv', 'dotenv', 'click', 'typer', 'rich', 'colorama',
            'pyyaml', 'yaml', 'toml', 'tomllib', 'configparser',
            'celery', 'rq', 'apscheduler', 'schedule',
            'pillow', 'PIL', 'opencv', 'cv2',
            'jinja2', 'mako', 'chameleon',
            'boto3', 'botocore', 'google', 'azure',
            'jwt', 'pyjwt', 'passlib', 'bcrypt', 'cryptography',
            'beautifulsoup4', 'bs4', 'lxml', 'scrapy', 'selenium',
            'paramiko', 'fabric', 'invoke',
            'loguru', 'structlog',
            'tenacity', 'backoff', 'retrying',
            # â­ Authentication (è¿½åŠ )
            'authlib', 'python_jose', 'jose', 'oauthlib',
            # â­ WebSocket (è¿½åŠ )
            'websockets', 'python_socketio', 'socketio', 'channels',
            # â­ Date/Time (è¿½åŠ )
            'arrow', 'pendulum', 'python_dateutil', 'dateutil',
            # â­ Configuration (è¿½åŠ )
            'dynaconf', 'python_decouple', 'decouple', 'environs',
            # â­ File Processing (è¿½åŠ )
            'openpyxl', 'xlrd', 'xlwt', 'python_docx', 'docx',
            'pypdf2', 'pypdf', 'reportlab', 'python_pptx', 'pptx',
            # â­ Serialization (è¿½åŠ )
            'orjson', 'ujson', 'simplejson',
            # â­ Caching (è¿½åŠ )
            'cachetools', 'diskcache', 'aiocache',
            # â­ Monitoring (è¿½åŠ )
            'sentry_sdk', 'prometheus_client', 'opentelemetry', 'newrelic', 'ddtrace',
            # â­ Message Queue (è¿½åŠ )
            'pika', 'kafka_python', 'confluent_kafka', 'kombu',
            # â­ Async (è¿½åŠ )
            'anyio', 'trio', 'httpcore',
            # â­ CLI (è¿½åŠ )
            'fire', 'argcomplete',
            # â­ Other (è¿½åŠ )
            'imageio', 'dramatiq', 'huey', 'minio', 's3fs', 'python_json_logger',
            'certifi', 'email_validator',
            # Type checking
            'mypy', 'pyright', 'typeguard',
        }
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: from X import Y
        from_import_pattern = r'^from\s+([\w.]+)\s+import'
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: import X
        import_pattern = r'^import\s+([\w.]+)'
        
        lines = content.split('\n')
        
        for line in lines:
            line = line.strip()
            
            # ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
            if line.startswith('#'):
                continue
            
            module_name = None
            full_module_path = None
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³1: from X import Y ã¾ãŸã¯ from X.Y import Z
            match = re.match(from_import_pattern, line)
            if match:
                full_module_path = match.group(1)  # å®Œå…¨ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ‘ã‚¹ï¼ˆä¾‹: services.user_serviceï¼‰
                module_name = full_module_path.split('.')[0]  # æœ€ä¸Šä½ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³2: import X ã¾ãŸã¯ import X.Y
            if not module_name:
                match = re.match(import_pattern, line)
                if match:
                    full_module_path = match.group(1)
                    module_name = full_module_path.split('.')[0]
            
            if not module_name:
                continue
            
            # ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆfrom . import Xï¼‰
            if module_name == '':
                continue
            
            # æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯ã‚¹ã‚­ãƒƒãƒ—
            if module_name.lower() in STDLIB_MODULES:
                continue
            
            # ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¯ã‚¹ã‚­ãƒƒãƒ—
            if module_name.lower().replace('-', '_') in THIRD_PARTY_PACKAGES:
                continue
            
            # æ—¢ã«ç”Ÿæˆã•ã‚Œã¦ã„ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæœ€ä¸Šä½ã®ã¿ãƒã‚§ãƒƒã‚¯ï¼‰
            if module_name in generated_modules:
                # ã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒã‚ã‚‹å ´åˆã¯è¿½åŠ ãƒã‚§ãƒƒã‚¯
                module_parts = full_module_path.split('.')
                if len(module_parts) > 1:
                    # ã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒ‘ã‚¹ï¼ˆä¾‹: services/user_service.pyï¼‰
                    submodule_path = '/'.join(module_parts) + '.py'
                    
                    if submodule_path not in generated_files:
                        # ã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒæ¬ è½
                        if not any(req.filepath == submodule_path for req in missing):
                            missing.append(FileRequirement(
                                filepath=submodule_path,
                                file_type='python',
                                reason=f'Imported by {filepath}: "{line.strip()}"',
                                priority='required'
                            ))
                            self.logger.debug(f"[Import Validation] Missing Python submodule: {submodule_path} (imported from {filepath})")
                continue
            
            # ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã«ç›´æ¥å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            # ä¾‹: "services" â†’ "services/__init__.py" ã¾ãŸã¯ "services.py"
            possible_paths = [
                f"{module_name}/__init__.py",
                f"{module_name}.py",
            ]
            
            found = False
            for path in possible_paths:
                if path in generated_files:
                    found = True
                    break
            
            if not found:
                # æ¬ è½ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã—ã¦ç™»éŒ²ï¼ˆ__init__.pyï¼‰
                missing_path = f"{module_name}/__init__.py"
                
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                if not any(req.filepath == missing_path for req in missing):
                    missing.append(FileRequirement(
                        filepath=missing_path,
                        file_type='python',
                        reason=f'Imported by {filepath}: "{line.strip()}"',
                        priority='required'
                    ))
                    self.logger.debug(f"[Import Validation] Missing Python module: {module_name} (imported from {filepath})")
                
                # ã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒã‚ã‚‹å ´åˆã¯ã€ã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚‚ç™»éŒ²
                module_parts = full_module_path.split('.')
                if len(module_parts) > 1:
                    submodule_path = '/'.join(module_parts) + '.py'
                    if not any(req.filepath == submodule_path for req in missing):
                        missing.append(FileRequirement(
                            filepath=submodule_path,
                            file_type='python',
                            reason=f'Imported by {filepath}: "{line.strip()}"',
                            priority='required'
                        ))
                        self.logger.debug(f"[Import Validation] Missing Python submodule: {submodule_path} (imported from {filepath})")
        
        return missing
    
    def _validate_requirements_compatibility(self, generated_files: Dict[str, str]) -> Dict[str, Any]:
        """
        requirements.txtã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒPythonãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨äº’æ›æ€§ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            {
                'has_issues': bool,
                'issues': List[str],
                'fixed_content': Optional[str]
            }
        """
        import sys
        import re
        
        result = {
            'has_issues': False,
            'issues': [],
            'fixed_content': None
        }
        
        # requirements.txt ã‚’æ¢ã™
        requirements_content = None
        requirements_filepath = None
        for filepath, content in generated_files.items():
            if filepath.endswith('requirements.txt'):
                requirements_content = content
                requirements_filepath = filepath
                break
        
        if not requirements_content:
            self.logger.debug("[Requirements Validation] No requirements.txt found")
            return result
        
        python_version = (sys.version_info.major, sys.version_info.minor)
        python_version_str = f"{python_version[0]}.{python_version[1]}"
        self.logger.debug(f"[Requirements Validation] Checking compatibility for Python {python_version_str}")
        
        # configã‹ã‚‰äº’æ›æ€§ãƒ«ãƒ¼ãƒ«ã‚’å–å¾—
        compatibility_rules = {}
        if self.config:
            python_compat = self.config.get("python_compatibility", {})
            rules = python_compat.get("rules", {})
            compatibility_rules = rules.get(python_version_str, {})
            self.logger.debug(f"[Requirements Validation] Loaded {len(compatibility_rules)} rules from config")
        
        if not compatibility_rules:
            self.logger.debug(f"[Requirements Validation] No compatibility rules for Python {python_version_str}")
            return result
        
        lines = requirements_content.split('\n')
        fixed_lines = []
        
        for line in lines:
            original_line = line
            line_stripped = line.strip()
            
            # ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã‚„ç©ºè¡Œã¯ã‚¹ã‚­ãƒƒãƒ—
            if not line_stripped or line_stripped.startswith('#'):
                fixed_lines.append(original_line)
                continue
            
            # ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸åã¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’æŠ½å‡º
            # ãƒ‘ã‚¿ãƒ¼ãƒ³: package==version, package>=version, package<=version, package~=version
            match = re.match(r'^([a-zA-Z0-9_-]+)\s*([=<>~!]+)\s*([0-9.]+)', line_stripped)
            
            if not match:
                fixed_lines.append(original_line)
                continue
            
            package_name_original = match.group(1)
            package_name = package_name_original.lower().replace('-', '_')
            operator = match.group(2)
            version = match.group(3)
            
            # äº’æ›æ€§å•é¡ŒãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆconfigã®ã‚­ãƒ¼ã¯-åŒºåˆ‡ã‚Šï¼‰
            package_key = package_name_original.lower()
            rule = compatibility_rules.get(package_key)
            
            # _åŒºåˆ‡ã‚Šã§ã‚‚æ¤œç´¢
            if not rule:
                rule = compatibility_rules.get(package_name)
            
            if rule:
                min_version = rule.get('min_version')
                recommended = rule.get('recommended', min_version)
                reason = rule.get('reason', 'Compatibility issue')
                
                if min_version and operator == '==' and self._version_less_than(version, min_version):
                    result['has_issues'] = True
                    result['issues'].append(
                        f"{package_name_original}=={version} is incompatible with Python {python_version_str}: {reason}. "
                        f"Minimum required: {min_version}"
                    )
                    self.logger.warning(f"[Requirements Validation] âš ï¸ {package_name_original}=={version} -> needs >={min_version}")
                    
                    # ä¿®æ­£ç‰ˆã‚’ç”Ÿæˆï¼ˆrecommendedãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ä½¿ç”¨ï¼‰
                    fixed_line = f"{package_name_original}>={recommended}"
                    fixed_lines.append(fixed_line)
                    continue
            
            fixed_lines.append(original_line)
        
        if result['has_issues']:
            result['fixed_content'] = '\n'.join(fixed_lines)
            self.logger.info(f"[Requirements Validation] Found {len(result['issues'])} compatibility issue(s)")
        
        return result
    
    def _version_less_than(self, version1: str, version2: str) -> bool:
        """
        ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ¯”è¼ƒ: version1 < version2 ã‹ã©ã†ã‹
        
        Args:
            version1: æ¯”è¼ƒå…ƒãƒãƒ¼ã‚¸ãƒ§ãƒ³ (ä¾‹: "2.0.23")
            version2: æ¯”è¼ƒå…ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ (ä¾‹: "2.0.25")
        
        Returns:
            version1 < version2 ã®å ´åˆ True
        """
        try:
            v1_parts = [int(x) for x in version1.split('.')]
            v2_parts = [int(x) for x in version2.split('.')]
            
            # æ¡æ•°ã‚’æƒãˆã‚‹
            while len(v1_parts) < len(v2_parts):
                v1_parts.append(0)
            while len(v2_parts) < len(v1_parts):
                v2_parts.append(0)
            
            return v1_parts < v2_parts
        except (ValueError, AttributeError):
            return False
    
    def _extract_explicit_filenames(self, goal: str) -> List[str]:
        """
        ä»•æ§˜æ›¸/ã‚´ãƒ¼ãƒ«ã‹ã‚‰æ˜ç¤ºçš„ã«æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«åã‚’æŠ½å‡º
        
        å¯¾å¿œãƒ‘ã‚¿ãƒ¼ãƒ³:
        1. ãƒãƒƒã‚¯ã‚¯ã‚©ãƒ¼ãƒˆ: `filename.ext`
        2. File Output ã‚»ã‚¯ã‚·ãƒ§ãƒ³: "- Single file: `neon_bubble_pop.html`"
        3. å‡ºåŠ›æŒ‡å®š: "Output: filename.ext", "Generate: filename.ext"
        4. ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³: è¡Œé ­ã® "- filename.ext"
        
        Args:
            goal: ãƒ¦ãƒ¼ã‚¶ãƒ¼ç›®æ¨™/ä»•æ§˜æ›¸ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            æ¤œå‡ºã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒªã‚¹ãƒˆï¼ˆé‡è¤‡ãªã—ã€æ¤œå‡ºé †ï¼‰
        """
        import re
        
        filenames = []
        
        # æœ‰åŠ¹ãªæ‹¡å¼µå­ãƒªã‚¹ãƒˆ
        VALID_EXTENSIONS = (
            'html', 'htm', 'css', 'js', 'jsx', 'ts', 'tsx',
            'py', 'json', 'yaml', 'yml', 'md', 'txt',
            'svg', 'xml'
        )
        ext_pattern = '|'.join(VALID_EXTENSIONS)
        
        # ============================================
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1ï¼ˆæœ€å„ªå…ˆï¼‰: File Output / Output file ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        # ============================================
        # "- Single file: `neon_bubble_pop.html`"
        # "## File Output\n- Single file: `filename.ext`"
        pattern1 = rf'(?:Single file|File Output|Output file|Output)[:\s]+[`"\']?([a-zA-Z0-9_\-\.]+\.(?:{ext_pattern}))[`"\']?'
        matches1 = re.findall(pattern1, goal, re.IGNORECASE)
        filenames.extend(matches1)
        
        # ============================================
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ãƒãƒƒã‚¯ã‚¯ã‚©ãƒ¼ãƒˆã§å›²ã¾ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«å
        # ============================================
        # `neon_bubble_pop.html`
        pattern2 = rf'`([a-zA-Z0-9_\-]+\.(?:{ext_pattern}))`'
        matches2 = re.findall(pattern2, goal, re.IGNORECASE)
        filenames.extend(matches2)
        
        # ============================================
        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: ç”ŸæˆæŒ‡ç¤º
        # ============================================
        # "Generate: app.js", "Create: index.html"
        pattern3 = rf'(?:Generate|Create|Produce)[:\s]+[`"\']?([a-zA-Z0-9_\-\.]+\.(?:{ext_pattern}))[`"\']?'
        matches3 = re.findall(pattern3, goal, re.IGNORECASE)
        filenames.extend(matches3)
        
        # ============================================
        # ãƒ‘ã‚¿ãƒ¼ãƒ³4: ãƒªã‚¹ãƒˆé …ç›®ã®ãƒ•ã‚¡ã‚¤ãƒ«å
        # ============================================
        # "- neon_bubble_pop.html"
        # "* index.js"
        pattern4 = rf'^[\-\*]\s+[`"\']?([a-zA-Z0-9_\-]+\.(?:{ext_pattern}))[`"\']?\s*$'
        matches4 = re.findall(pattern4, goal, re.MULTILINE | re.IGNORECASE)
        filenames.extend(matches4)
        
        # é‡è¤‡é™¤å»ï¼ˆæ¤œå‡ºé †ã‚’ä¿æŒï¼‰
        seen = set()
        unique_filenames = []
        for fn in filenames:
            fn_lower = fn.lower()
            if fn_lower not in seen:
                seen.add(fn_lower)
                unique_filenames.append(fn)
        
        if unique_filenames:
            self.logger.debug(f"[Explicit Files] Detected: {unique_filenames}")
        
        return unique_filenames
    
    def _predict_required_files(
        self,
        goal: str,
        multi_file_detection: dict
    ) -> List[FileRequirement]:
        """
        Repository Mapã¨æ¤œå‡ºçµæœã‹ã‚‰æœŸå¾…ã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’äºˆæ¸¬
        
        Args:
            goal: ãƒ¦ãƒ¼ã‚¶ãƒ¼ç›®æ¨™
            multi_file_detection: è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡ºçµæœ
        
        Returns:
            æœŸå¾…ã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«è¦ä»¶ã®ãƒªã‚¹ãƒˆ
        """
        requirements = []
        
        # ==========================================
        # ã€æœ€å„ªå…ˆã€‘Multi-File Detection ã®çµæœã‚’ä½¿ç”¨
        # ==========================================
        
        detected_files = multi_file_detection.get("detected_files", [])
        if detected_files:
            self.logger.debug(f"[Post-Validation] Using detected files from Multi-File Detection: {detected_files}")
            
            for filename in detected_files:
                # æ‹¡å¼µå­ã‚’å–å¾—
                ext = filename.split('.')[-1] if '.' in filename else 'unknown'
                
                requirements.append(FileRequirement(
                    filepath=filename,
                    file_type=ext,
                    reason=f'Explicitly mentioned in goal (detected by Multi-File Detection)',
                    priority='required'
                ))
            
            self.logger.debug(f"[Post-Validation] Predicted {len(requirements)} required files from Multi-File Detection")
            return requirements  # æ˜ç¤ºçš„ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°ã€ã“ã“ã§çµ‚äº†ï¼ˆæ…£ç¿’çš„æ¨æ¸¬ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰
        
        # ==========================================
        # ã€æ¬¡å„ªå…ˆã€‘æ–°è¦ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡ºï¼ˆå¯¾ç­–1-Aï¼‰
        # ==========================================
        goal_lower = goal.lower()
        
        # Reactãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æ¤œå‡ºï¼ˆæ˜ç¤ºçš„ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯æŒ‡å®šã®ã¿ï¼‰
        react_keywords = [
            'react app', 'react application', 'create react',
            'react component', 'react project', 'using react',
            'with react', 'in react', 'react.js', 'reactjs'
        ]
        is_react_project = any(kw in goal_lower for kw in react_keywords)
        
        # ğŸ†• no_framework_modeã®å ´åˆã€Reactæ¤œå‡ºã‚’ã‚¹ã‚­ãƒƒãƒ—
        if is_react_project and self.no_framework_mode:
            self.logger.debug("[Post-Validation] React keywords found but SKIPPED due to no-framework mode")
            is_react_project = False
        
        if is_react_project:
            self.logger.debug("[Post-Validation] React project detected from goal keywords")
            self.logger.debug(f"[Post-Validation] Goal: {goal[:100]}...")
            
            # Reactå¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ 
            # Note: .js ã¨ .jsx ã®ä¸¡æ–¹ã‚’ã‚µãƒãƒ¼ãƒˆï¼ˆ_validate_completeness ãŒæ‹¡å¼µå­ãªã—ã§ãƒãƒƒãƒãƒ³ã‚°ï¼‰
            requirements.append(FileRequirement(
                filepath='src/App.js',  # ã¾ãŸã¯ src/App.jsx
                file_type='js',
                reason='Main React component (REQUIRED for all React projects)',
                priority='required'
            ))
            
            requirements.append(FileRequirement(
                filepath='src/index.js',  # ã¾ãŸã¯ src/index.jsx
                file_type='js',
                reason='React entry point',
                priority='required'
            ))
            
            # Vite: index.html, Create React App: public/index.html
            # ã©ã¡ã‚‰ã‚‚æ¤œå‡ºã•ã‚Œã‚‹ã‚ˆã†ã«ã€public/index.htmlã‚’å„ªå…ˆ
            requirements.append(FileRequirement(
                filepath='public/index.html',
                file_type='html',
                reason='HTML entry point (CRA) or index.html (Vite)',
                priority='required'
            ))
            
            self.logger.debug(f"[Post-Validation] Added {len(requirements)} React project requirements")
            self.logger.debug(f"[Post-Validation] Required files: {[r.filepath for r in requirements]}")
            return requirements
        
        # ==========================================
        # ã€æ–°è¦è¿½åŠ ã€‘æ˜ç¤ºçš„ãƒ•ã‚¡ã‚¤ãƒ«åã®æ¤œå‡ºï¼ˆæœ€å„ªå…ˆï¼‰
        # file_typesã‹ã‚‰ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¨æ¸¬ã‚ˆã‚Šå„ªå…ˆ
        # ==========================================
        explicit_files = self._extract_explicit_filenames(goal)
        if explicit_files:
            self.logger.debug(f"[Post-Validation] Using explicit filenames from goal: {explicit_files}")
            
            for filename in explicit_files:
                ext = filename.split('.')[-1] if '.' in filename else 'unknown'
                requirements.append(FileRequirement(
                    filepath=filename,
                    file_type=ext,
                    reason=f'Explicitly specified in goal/spec',
                    priority='required'
                ))
            
            self.logger.debug(f"[Post-Validation] Predicted {len(requirements)} required files from explicit filenames")
            return requirements  # æ˜ç¤ºçš„ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¨æ¸¬ã‚’ã‚¹ã‚­ãƒƒãƒ—
        
        # ==========================================
        # ã€æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ã€‘file_types ã‹ã‚‰ã®æ¨æ¸¬
        # Goal ã«æ˜ç¤ºçš„ãªãƒ•ã‚¡ã‚¤ãƒ«åãŒãªãã€React ã§ã‚‚ãªã„å ´åˆã€
        # file_types ã‹ã‚‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¨æ¸¬ã™ã‚‹
        # ==========================================
        file_types = multi_file_detection.get('file_types', [])
        if file_types and not requirements:
            self.logger.debug(f"[Post-Validation] Building requirements from file_types: {file_types}")
            
            # file_types ã«åŸºã¥ã„ã¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ 
            default_files_for_types = {
                'html': ('index.html', 'html'),
                'css': ('styles.css', 'css'),
                'js': ('app.js', 'js'),
                'jsx': ('src/App.jsx', 'jsx'),
                'ts': ('src/main.ts', 'ts'),
                'tsx': ('src/App.tsx', 'tsx'),
                'python': ('main.py', 'py'),
            }
            
            for file_type in file_types:
                if file_type in default_files_for_types:
                    default_file, ext = default_files_for_types[file_type]
                    requirements.append(FileRequirement(
                        filepath=default_file,
                        file_type=ext,
                        reason=f"Required for web project (detected file_type: {file_type})",
                        priority='required'
                    ))
            
            if requirements:
                self.logger.debug(f"[Post-Validation] Predicted {len(requirements)} required files from file_types")
                self.logger.debug(f"[Post-Validation] Required files: {[r.filepath for r in requirements]}")
                return requirements
        
        # ==========================================
        # ã€æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ã€‘Repository Analyzerä¾å­˜
        # ==========================================
        if not self.repository_analyzer:
            self.logger.debug("[Post-Validation] No repository analyzer, cannot predict files")
            return requirements
        
        try:
            repo_data = self.repository_analyzer.memory.repository_data
            
            # goalã‹ã‚‰æ“ä½œã‚¿ã‚¤ãƒ—ã‚’æ¤œå‡º
            goal_lower = goal.lower()
            is_add_feature = any(kw in goal_lower for kw in ['add', 'create', 'implement', 'build'])
            is_modify = any(kw in goal_lower for kw in ['modify', 'update', 'change', 'extend', 'support'])
            
            # Repository Mapã‹ã‚‰æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
            for filepath in repo_data.keys():
                file_type = self._get_file_type(filepath)
                
                if not file_type:
                    continue
                
                # HTML + JS ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆ
                if file_type in ['html', 'js', 'css']:
                    # UIå¤‰æ›´ã‚’ä¼´ã†æ“ä½œã®å ´åˆã€HTML/JSä¸¡æ–¹ãŒå¿…è¦
                    if is_add_feature or is_modify:
                        # HTML: UIè¦ç´ ãŒå¿…è¦
                        if file_type == 'html':
                            requirements.append(FileRequirement(
                                filepath=filepath,
                                file_type='html',
                                reason='UI changes required (buttons, forms, or display elements)',
                                priority='required'
                            ))
                        
                        # JS: ãƒ­ã‚¸ãƒƒã‚¯ãŒå¿…è¦
                        elif file_type == 'js':
                            requirements.append(FileRequirement(
                                filepath=filepath,
                                file_type='js',
                                reason='Logic implementation required (functions, event handlers)',
                                priority='required'
                            ))
                        
                        # CSS: ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚°ãŒå¿…è¦ãªå ´åˆ
                        elif file_type == 'css' and 'style' in goal_lower:
                            requirements.append(FileRequirement(
                                filepath=filepath,
                                file_type='css',
                                reason='Styling changes required',
                                priority='optional'
                            ))
                
                # Pythonè¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆ
                elif file_type == 'python':
                    # æ©Ÿèƒ½è¿½åŠ ã®å ´åˆã€é–¢é€£ã™ã‚‹å…¨ã¦ã®Pythonãƒ•ã‚¡ã‚¤ãƒ«ãŒå¯¾è±¡ã«ãªã‚Šã†ã‚‹
                    if is_add_feature or is_modify:
                        requirements.append(FileRequirement(
                            filepath=filepath,
                            file_type='python',
                            reason='May require changes for feature implementation',
                            priority='optional'  # Pythonã¯æ˜ç¢ºãªä¾å­˜é–¢ä¿‚ãŒå¿…è¦
                        ))
            
            self.logger.debug(f"[Post-Validation] Predicted {len(requirements)} required files")
            
        except Exception as e:
            self.logger.debug(f"[Post-Validation] Failed to predict required files: {e}")
            import traceback
            self.logger.debug(traceback.format_exc())
        
        return requirements
    
    def _get_file_type(self, filepath: str) -> Optional[str]:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã‚’å–å¾—
        
        Args:
            filepath: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        
        Returns:
            'html', 'js', 'css', 'python' ãªã©ã€ã¾ãŸã¯ None
        """
        if filepath.endswith(('.html', '.htm')):
            return 'html'
        elif filepath.endswith(('.js', '.jsx')):
            return 'js'
        elif filepath.endswith(('.css', '.scss', '.sass')):
            return 'css'
        elif filepath.endswith('.py'):
            return 'python'
        elif filepath.endswith(('.ts', '.tsx')):
            return 'typescript'
        elif filepath.endswith('.java'):
            return 'java'
        else:
            return None

    def _validate_completeness(
        self,
        generated_files: Dict[str, str],
        expected_files: List[FileRequirement],
        goal: str
    ) -> ValidationResult:
        """ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒæœŸå¾…ã•ã‚Œã‚‹è¦ä»¶ã‚’æº€ãŸã—ã¦ã„ã‚‹ã‹æ¤œè¨¼"""
        generated_filenames = set(generated_files.keys())
        missing_files = []
        missing_requirements = []
        reasons = []
        
        for requirement in expected_files:
            filepath = requirement.filepath
            file_found = False
            
            if filepath in generated_filenames:
                file_found = True
            else:
                base_name = filepath.rsplit('.', 1)[0] if '.' in filepath else filepath
                for gen_file in generated_filenames:
                    gen_base = gen_file.rsplit('.', 1)[0] if '.' in gen_file else gen_file
                    if base_name == gen_base:
                        file_found = True
                        break
            
            if not file_found:
                missing_files.append(filepath)
                missing_requirements.append(requirement)
                reasons.append(f"{filepath}: {requirement.reason}")
        
        is_complete = len(missing_files) == 0
        
        return ValidationResult(
            is_complete=is_complete,
            missing_files=missing_files,
            missing_requirements=missing_requirements,
            reasons=reasons
        )
    
    
    def _extract_code_from_response(self, response_text: str, filepath: str) -> str:
        """
        LLMå¿œç­”ã‹ã‚‰ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã™ã‚‹ï¼ˆãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»ï¼‰
        
        å¯¾å¿œã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³:
        1. ```language\n...\n```  (æ¨™æº–)
        2. ```language\n...       (çµ‚ã‚ã‚ŠãŒãªã„ - ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™ã§åˆ‡ã‚ŒãŸå ´åˆ)
        3. [ã‚³ãƒ¼ãƒ‰]               (ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ãªã—)
        
        Args:
            response_text: LLMã®å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ
            filepath: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆãƒ­ã‚°ç”¨ï¼‰
        
        Returns:
            æŠ½å‡ºã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ï¼ˆä¸å®Œå…¨ãªå ´åˆã¯ __INCOMPLETE__ ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ä»˜ãï¼‰
        """
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: æ¨™æº–çš„ãªã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ ```language\n...\n```
        pattern1 = r'```(?:[a-z]+)?\s*\n(.*?)\n```'
        match1 = re.search(pattern1, response_text, re.DOTALL)
        if match1:
            self.logger.debug(f"[Code Extraction] âœ… Standard code block: {filepath}")
            return match1.group(1).strip()
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: é–‹å§‹ã®ã¿ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ ```language\n... (çµ‚ã‚ã‚ŠãŒãªã„)
        pattern2 = r'```(?:[a-z]+)?\s*\n(.*?)$'
        match2 = re.search(pattern2, response_text, re.DOTALL)
        if match2:
            self.logger.debug(f"[Code Extraction] âš ï¸  Incomplete code block detected: {filepath}")
            self.logger.debug(f"  This file needs regeneration (LLM response was truncated)")
            # ä¸å®Œå…¨ãƒ•ãƒ©ã‚°ã‚’ä»˜ã‘ã¦è¿”ã™
            extracted = match2.group(1).strip()
            return f"__INCOMPLETE__\n{extracted}"
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: å…ˆé ­ã« ```language ãªã©ãŒã‚ã‚‹å ´åˆã¯å‰Šé™¤
        if response_text.strip().startswith('```'):
            lines = response_text.strip().split('\n')
            code = '\n'.join(lines[1:]).strip()
            self.logger.debug(f"[Code Extraction] âš   Removed markdown header: {filepath}")
            return code
        
        # ãã‚Œä»¥å¤–ã¯ãã®ã¾ã¾
        self.logger.debug(f"[Code Extraction] â„¹ï¸  No code block markers: {filepath}")
        return response_text.strip()

    
    def _extract_embedded_javascript(self, html_content: str) -> str:
        """
        HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰<script>ã‚¿ã‚°å†…ã®JavaScriptã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        
        Args:
            html_content: HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹
        
        Returns:
            æŠ½å‡ºã•ã‚ŒãŸJavaScriptã‚³ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°ã®<script>ã‚¿ã‚°ãŒã‚ã‚‹å ´åˆã¯çµåˆï¼‰
        """
        import re
        
        # <script>ã‚¿ã‚°å†…ã®ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡ºï¼ˆå¤–éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«å‚ç…§ã¯é™¤å¤–ï¼‰
        # (?![^>]*\bsrc\s*=) â†’ srcã‚¢ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ãƒˆãŒãªã„<script>ã‚¿ã‚°ã®ã¿ãƒãƒƒãƒ
        pattern = r'<script(?![^>]*\bsrc\s*=)[^>]*>(.*?)</script>'
        scripts = re.findall(pattern, html_content, re.DOTALL | re.IGNORECASE)
        
        if not scripts:
            return ""
        
        # è¤‡æ•°ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ–ãƒ­ãƒƒã‚¯ã‚’çµåˆ
        return '\n\n'.join(scripts)
    
    
    def _extract_embedded_css(self, html_content: str) -> str:
        """
        HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰<style>ã‚¿ã‚°å†…ã®CSSã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        
        Args:
            html_content: HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹
        
        Returns:
            æŠ½å‡ºã•ã‚ŒãŸCSSã‚³ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°ã®<style>ã‚¿ã‚°ãŒã‚ã‚‹å ´åˆã¯çµåˆï¼‰
        """
        import re
        
        # <style>ã‚¿ã‚°å†…ã®ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        pattern = r'<style[^>]*>(.*?)</style>'
        styles = re.findall(pattern, html_content, re.DOTALL | re.IGNORECASE)
        
        if not styles:
            return ""
        
        # è¤‡æ•°ã®ã‚¹ã‚¿ã‚¤ãƒ«ãƒ–ãƒ­ãƒƒã‚¯ã‚’çµåˆ
        return '\n\n'.join(styles)
    
    
    def _validate_html_tags(self, html_content: str) -> Tuple[bool, List[str]]:
        """
        HTMLã‚¿ã‚°ã®é–‹é–‰ä¸€è‡´ã‚’ãƒã‚§ãƒƒã‚¯
        
        Args:
            html_content: HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹
        
        Returns:
            (is_valid, errors): æ¤œè¨¼çµæœã¨ã‚¨ãƒ©ãƒ¼ãƒªã‚¹ãƒˆ
        """
        import re
        
        errors = []
        
        # ãƒã‚§ãƒƒã‚¯å¯¾è±¡ã‚¿ã‚°
        critical_tags = ['html', 'head', 'body', 'script', 'style']
        
        for tag in critical_tags:
            # é–‹ãã‚¿ã‚°ã¨é–‰ã˜ã‚¿ã‚°ã®æ•°ã‚’æ•°ãˆã‚‹
            # (?i) = case-insensitive
            open_pattern = f'<{tag}(?:\\s|>)'
            close_pattern = f'</{tag}>'
            
            open_count = len(re.findall(open_pattern, html_content, re.IGNORECASE))
            close_count = len(re.findall(close_pattern, html_content, re.IGNORECASE))
            
            if open_count != close_count:
                errors.append(
                    f"Tag mismatch: <{tag}> found {open_count} times, "
                    f"</{tag}> found {close_count} times"
                )
        
        is_valid = len(errors) == 0
        
        if not is_valid:
            self.logger.debug(f"[HTML Tag Check] âŒ Tag mismatch detected")
            for error in errors:
                self.logger.debug(f"  - {error}")
        else:
            self.logger.debug(f"[HTML Tag Check] âœ… All tags matched")
        
        return is_valid, errors
    
    
    def _check_javascript_brackets(self, js_code: str) -> List[str]:
        """
        JavaScriptã‚³ãƒ¼ãƒ‰ã®æ‹¬å¼§ãƒ»ãƒ–ãƒ¬ãƒ¼ã‚¹ãƒ»ãƒ–ãƒ©ã‚±ãƒƒãƒˆã®æ•°ã‚’ãƒã‚§ãƒƒã‚¯
        
        Args:
            js_code: JavaScriptã‚³ãƒ¼ãƒ‰
        
        Returns:
            ã‚¨ãƒ©ãƒ¼ãƒªã‚¹ãƒˆï¼ˆã‚¨ãƒ©ãƒ¼ãŒãªã‘ã‚Œã°ç©ºãƒªã‚¹ãƒˆï¼‰
        """
        errors = []
        
        # æ‹¬å¼§ã®ã‚«ã‚¦ãƒ³ãƒˆ
        open_braces = js_code.count('{')
        close_braces = js_code.count('}')
        if open_braces != close_braces:
            errors.append(f"Unmatched braces: {open_braces} open, {close_braces} close")
        
        open_parens = js_code.count('(')
        close_parens = js_code.count(')')
        if open_parens != close_parens:
            errors.append(f"Unmatched parentheses: {open_parens} open, {close_parens} close")
        
        open_brackets = js_code.count('[')
        close_brackets = js_code.count(']')
        if open_brackets != close_brackets:
            errors.append(f"Unmatched brackets: {open_brackets} open, {close_brackets} close")
        
        return errors
    
    
    def _validate_json_syntax(self, filepath: str, content: str) -> Tuple[bool, str]:
        """
        JSONæ§‹æ–‡ãƒã‚§ãƒƒã‚¯
        
        Args:
            filepath: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            content: ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹
        
        Returns:
            (is_valid, error_message): æ§‹æ–‡ã®å¦¥å½“æ€§ã¨ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        """
        if not filepath.endswith('.json'):
            return True, ""
        
        try:
            import json
            json.loads(content)
            self.logger.debug(f"[JSON Syntax Check] âœ… Valid JSON: {filepath}")
            return True, ""
        except json.JSONDecodeError as e:
            error_msg = f"Line {e.lineno}, Column {e.colno}: {e.msg}"
            self.logger.debug(f"[JSON Syntax Check] âŒ Invalid JSON: {filepath}")
            self.logger.debug(f"  Error: {error_msg}")
            return False, error_msg
    
    
    def _validate_yaml_syntax(self, filepath: str, content: str) -> Tuple[bool, str]:
        """
        YAMLæ§‹æ–‡ãƒã‚§ãƒƒã‚¯
        
        Args:
            filepath: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            content: ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹
        
        Returns:
            (is_valid, error_message): æ§‹æ–‡ã®å¦¥å½“æ€§ã¨ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        """
        if not filepath.endswith(('.yaml', '.yml')):
            return True, ""
        
        try:
            import yaml
            yaml.safe_load(content)
            self.logger.debug(f"[YAML Syntax Check] âœ… Valid YAML: {filepath}")
            return True, ""
        except yaml.YAMLError as e:
            error_msg = str(e)
            self.logger.debug(f"[YAML Syntax Check] âŒ Invalid YAML: {filepath}")
            self.logger.debug(f"  Error: {error_msg}")
            return False, error_msg
        except Exception as e:
            # yamlãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒåˆ©ç”¨ä¸å¯ã®å ´åˆ
            self.logger.debug(f"[YAML Syntax Check] âš   YAML module not available, skipping: {filepath}")
            return True, ""
    
    
    def _validate_markdown_syntax(self, filepath: str, content: str) -> Tuple[bool, str]:
        """
        Markdownæ§‹æ–‡ãƒã‚§ãƒƒã‚¯ï¼ˆåŸºæœ¬çš„ãªãƒã‚§ãƒƒã‚¯ã®ã¿ï¼‰
        
        Args:
            filepath: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            content: ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹
        
        Returns:
            (is_valid, error_message): æ§‹æ–‡ã®å¦¥å½“æ€§ã¨ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        """
        if not filepath.endswith(('.md', '.markdown')):
            return True, ""
        
        errors = []
        
        # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã®é–‹é–‰ãƒã‚§ãƒƒã‚¯
        import re
        code_blocks = re.findall(r'```', content)
        if len(code_blocks) % 2 != 0:
            errors.append("Unclosed code block (unmatched ```)")
        
        # ãƒªãƒ³ã‚¯å½¢å¼ã®åŸºæœ¬ãƒã‚§ãƒƒã‚¯
        # [text](url) ã®å½¢å¼
        link_pattern = r'\[([^\]]+)\]\(([^\)]*)\)'
        links = re.findall(link_pattern, content)
        for text, url in links:
            if not url.strip():
                errors.append(f"Empty link URL for text: [{text}]")
        
        if errors:
            error_msg = "; ".join(errors)
            self.logger.debug(f"[Markdown Syntax Check] âŒ Invalid Markdown: {filepath}")
            self.logger.debug(f"  Errors: {error_msg}")
            return False, error_msg
        
        self.logger.debug(f"[Markdown Syntax Check] âœ… Valid Markdown: {filepath}")
        return True, ""
    
    
    def _validate_xml_syntax(self, filepath: str, content: str) -> Tuple[bool, str]:
        """
        XMLæ§‹æ–‡ãƒã‚§ãƒƒã‚¯ï¼ˆSVGã‚’å«ã‚€ï¼‰
        
        Args:
            filepath: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            content: ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹
        
        Returns:
            (is_valid, error_message): æ§‹æ–‡ã®å¦¥å½“æ€§ã¨ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        """
        if not filepath.endswith(('.xml', '.svg')):
            return True, ""
        
        try:
            import xml.etree.ElementTree as ET
            ET.fromstring(content)
            self.logger.debug(f"[XML Syntax Check] âœ… Valid XML: {filepath}")
            return True, ""
        except ET.ParseError as e:
            error_msg = f"Line {e.position[0]}, Column {e.position[1]}: {e.msg}"
            self.logger.debug(f"[XML Syntax Check] âŒ Invalid XML: {filepath}")
            self.logger.debug(f"  Error: {error_msg}")
            return False, error_msg
        except Exception as e:
            error_msg = str(e)
            self.logger.debug(f"[XML Syntax Check] âŒ XML parse error: {filepath}")
            self.logger.debug(f"  Error: {error_msg}")
            return False, error_msg
    
    
    def _validate_naming_conventions(self, filepath: str, content: str) -> Tuple[bool, List[str]]:
        """
        å‘½åè¦å‰‡ãƒã‚§ãƒƒã‚¯ï¼ˆåŸºæœ¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿ï¼‰
        
        Args:
            filepath: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            content: ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹
        
        Returns:
            (is_valid, warnings): æ¤œè¨¼çµæœã¨è­¦å‘Šãƒªã‚¹ãƒˆ
        """
        warnings = []
        
        # Python: snake_case ãƒã‚§ãƒƒã‚¯
        if filepath.endswith('.py'):
            import re
            # ã‚¯ãƒ©ã‚¹å®šç¾©: PascalCaseæ¨å¥¨
            class_pattern = r'class\s+([a-z][a-z0-9_]*)\s*[:(]'
            lowercase_classes = re.findall(class_pattern, content)
            for class_name in lowercase_classes:
                warnings.append(f"Class name '{class_name}' should use PascalCase (e.g., {class_name.title()})")
            
            # å®šæ•°: UPPER_CASEæ¨å¥¨
            const_pattern = r'^([a-z][a-z0-9_]*)\s*=\s*["\'].*["\']'
            lowercase_consts = re.findall(const_pattern, content, re.MULTILINE)
            # ç°¡æ˜“ãƒã‚§ãƒƒã‚¯: å…¨ã¦å¤§æ–‡å­—ã®ã‚‚ã®ãŒã‚ã‚Œã°å®šæ•°ã¨åˆ¤æ–­
            if re.search(r'^[A-Z][A-Z0-9_]*\s*=', content, re.MULTILINE):
                for const in lowercase_consts[:3]:  # æœ€åˆã®3ã¤ã®ã¿
                    if len(const) > 2 and const.upper() != const:
                        warnings.append(f"Consider using UPPER_CASE for constant: {const}")
        
        # JavaScript: camelCase ãƒã‚§ãƒƒã‚¯
        elif filepath.endswith(('.js', '.jsx')):
            import re
            # é–¢æ•°å®šç¾©: snake_caseä½¿ç”¨ã®æ¤œå‡º
            func_pattern = r'function\s+([a-z][a-z0-9]*_[a-z0-9_]*)\s*\('
            snake_funcs = re.findall(func_pattern, content)
            for func_name in snake_funcs[:3]:
                camel_name = ''.join(word.capitalize() if i > 0 else word for i, word in enumerate(func_name.split('_')))
                warnings.append(f"Function name '{func_name}' should use camelCase (e.g., {camel_name})")
        
        # CSS: kebab-case ãƒã‚§ãƒƒã‚¯ï¼ˆclassåï¼‰
        elif filepath.endswith('.css'):
            import re
            # classåã«ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚„PascalCaseæ¤œå‡º
            class_pattern = r'\.([A-Z][a-zA-Z0-9]*|[a-z][a-zA-Z0-9]*_[a-zA-Z0-9_]*)\s*\{'
            invalid_classes = re.findall(class_pattern, content)
            for class_name in invalid_classes[:3]:
                warnings.append(f"CSS class '.{class_name}' should use kebab-case")
        
        # è­¦å‘Šã¯å¤±æ•—ã§ã¯ãªã„
        is_valid = True
        
        if warnings:
            self.logger.debug(f"[Naming Convention Check] âš ï¸  Style warnings for: {filepath}")
            for warning in warnings[:5]:  # æœ€åˆã®5ã¤ã®ã¿ãƒ­ã‚°
                self.logger.debug(f"  - {warning}")
        else:
            self.logger.debug(f"[Naming Convention Check] âœ… No style issues: {filepath}")
        
        return is_valid, warnings
    
    
    def _validate_encoding(self, filepath: str, content: str) -> Tuple[bool, str]:
        """
        ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œè¨¼ï¼ˆBOMä»˜ãUTF-8ã®æ¤œå‡ºï¼‰
        
        Args:
            filepath: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            content: ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹
        
        Returns:
            (is_valid, error_message): æ¤œè¨¼çµæœã¨ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        """
        # BOMï¼ˆByte Order Markï¼‰ã®æ¤œå‡º
        if content.startswith('\ufeff'):
            error_msg = "File contains UTF-8 BOM (Byte Order Mark)"
            self.logger.debug(f"[Encoding Check] âš ï¸  BOM detected: {filepath}")
            self.logger.debug(f"  {error_msg}")
            # BOMã¯è­¦å‘Šãƒ¬ãƒ™ãƒ«ï¼ˆå¤±æ•—ã§ã¯ãªã„ï¼‰
            return True, error_msg
        
        self.logger.debug(f"[Encoding Check] âœ… No encoding issues: {filepath}")
        return True, ""
    
    
    def _validate_line_endings(self, filepath: str, content: str) -> Tuple[bool, str]:
        """
        æ”¹è¡Œã‚³ãƒ¼ãƒ‰ã®çµ±ä¸€ãƒã‚§ãƒƒã‚¯ï¼ˆCRLF vs LFï¼‰
        
        Args:
            filepath: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            content: ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹
        
        Returns:
            (is_valid, warning_message): æ¤œè¨¼çµæœã¨è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        """
        has_crlf = '\r\n' in content
        has_lf_only = '\n' in content.replace('\r\n', '')
        
        if has_crlf and has_lf_only:
            warning_msg = "Mixed line endings detected (CRLF and LF)"
            self.logger.debug(f"[Line Ending Check] âš ï¸  Mixed line endings: {filepath}")
            self.logger.debug(f"  {warning_msg}")
            # æ··åœ¨ã¯è­¦å‘Šãƒ¬ãƒ™ãƒ«ï¼ˆå¤±æ•—ã§ã¯ãªã„ï¼‰
            return True, warning_msg
        
        self.logger.debug(f"[Line Ending Check] âœ… Consistent line endings: {filepath}")
        return True, ""
    
    
    def _validate_css_syntax(self, filepath: str, content: str) -> Tuple[bool, str]:
        """
        CSSåŸºæœ¬æ§‹æ–‡ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        
        Args:
            filepath: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            content: ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹
        
        Returns:
            (is_valid, error_message): æ§‹æ–‡ã®å¦¥å½“æ€§ã¨ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        """
        if not filepath.endswith('.css'):
            return True, ""
        
        errors = []
        
        # åŸºæœ¬çš„ãªãƒ–ãƒ¬ãƒ¼ã‚¹ä¸€è‡´ãƒã‚§ãƒƒã‚¯
        open_braces = content.count('{')
        close_braces = content.count('}')
        if open_braces != close_braces:
            errors.append(f"Unmatched braces: {open_braces} open, {close_braces} close")
        
        # ã‚³ãƒ¡ãƒ³ãƒˆé–‰ã˜å¿˜ã‚Œãƒã‚§ãƒƒã‚¯
        import re
        # /* ã§å§‹ã¾ã‚Š */ ã§çµ‚ã‚ã‚‰ãªã„ã‚³ãƒ¡ãƒ³ãƒˆ
        unclosed_comments = re.findall(r'/\*(?:(?!\*/).)*$', content, re.DOTALL | re.MULTILINE)
        if unclosed_comments:
            errors.append("Unclosed CSS comment (/* without */)")
        
        if errors:
            error_msg = "; ".join(errors)
            self.logger.debug(f"[CSS Syntax Check] âŒ Invalid CSS: {filepath}")
            self.logger.debug(f"  Errors: {error_msg}")
            return False, error_msg
        
        self.logger.debug(f"[CSS Syntax Check] âœ… Valid CSS: {filepath}")
        return True, ""
    
    
    def _validate_code_completeness(self, filepath: str, content: str) -> Tuple[bool, List[str]]:
        """
        ã‚³ãƒ¼ãƒ‰ã®å®Œå…¨æ€§ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹
        
        Args:
            filepath: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            content: ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹
        
        Returns:
            (is_complete, errors): å®Œå…¨æ€§ã®åˆ¤å®šçµæœã¨ã‚¨ãƒ©ãƒ¼ãƒªã‚¹ãƒˆ
        """
        # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯ç‰¹åˆ¥å‡¦ç†
        if filepath.endswith('.html'):
            errors = []
            
            # 1. HTMLã‚¿ã‚°ã®é–‹é–‰ãƒã‚§ãƒƒã‚¯
            tags_valid, tag_errors = self._validate_html_tags(content)
            if not tags_valid:
                errors.extend(tag_errors)
            
            # 2. åŸ‹ã‚è¾¼ã¿JavaScriptã®æŠ½å‡ºã¨æ¤œè¨¼
            js_code = self._extract_embedded_javascript(content)
            if js_code.strip():
                # JavaScriptã¨ã—ã¦æ‹¬å¼§ãƒ»ãƒ–ãƒ¬ãƒ¼ã‚¹ã®ãƒã‚§ãƒƒã‚¯
                js_errors = self._check_javascript_brackets(js_code)
                if js_errors:
                    errors.extend([f"Embedded JavaScript: {e}" for e in js_errors])
            
            # 3. åŸ‹ã‚è¾¼ã¿CSSã®æŠ½å‡ºã¨æ¤œè¨¼
            css_code = self._extract_embedded_css(content)
            if css_code.strip():
                # CSSã¨ã—ã¦ãƒ–ãƒ¬ãƒ¼ã‚¹ã®ãƒã‚§ãƒƒã‚¯
                open_braces = css_code.count('{')
                close_braces = css_code.count('}')
                if open_braces != close_braces:
                    errors.append(f"Embedded CSS: Unmatched braces: {open_braces} open, {close_braces} close")
            
            # 4. ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³è¨˜å·ã®æ®‹å­˜ãƒã‚§ãƒƒã‚¯
            if content.strip().startswith('```'):
                errors.append("File starts with markdown code block marker")
            
            is_complete = len(errors) == 0
            
            if not is_complete:
                self.logger.debug(f"[Completeness Check] âŒ Incomplete HTML: {filepath}")
                for error in errors:
                    self.logger.debug(f"  - {error}")
            else:
                self.logger.debug(f"[Completeness Check] âœ… Complete HTML: {filepath}")
            
            return is_complete, errors
        
        # Pythonãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯ç‰¹åˆ¥å‡¦ç†
        if filepath.endswith('.py'):
            errors = []
            
            # æ‹¬å¼§ãƒ»ãƒ–ãƒ¬ãƒ¼ã‚¹ãƒ»ãƒ–ãƒ©ã‚±ãƒƒãƒˆã®æ•°ãƒã‚§ãƒƒã‚¯
            open_braces = content.count('{')
            close_braces = content.count('}')
            if open_braces != close_braces:
                errors.append(f"Unmatched braces: {open_braces} open, {close_braces} close")
            
            open_parens = content.count('(')
            close_parens = content.count(')')
            if open_parens != close_parens:
                errors.append(f"Unmatched parentheses: {open_parens} open, {close_parens} close")
            
            open_brackets = content.count('[')
            close_brackets = content.count(']')
            if open_brackets != close_brackets:
                errors.append(f"Unmatched brackets: {open_brackets} open, {close_brackets} close")
            
            # ä¸å®Œå…¨ãªæœ€çµ‚è¡Œã®ãƒã‚§ãƒƒã‚¯
            lines = content.strip().split('\n')
            if lines:
                last_line = lines[-1].strip()
                incomplete_patterns = [
                    'def ', 'class ', 'import ', 'from ',
                    'if ', 'elif ', 'else:', 'for ', 'while ',
                    '= ', 'return ', 'raise ', 'assert ',
                    'try:', 'except ', 'finally:', 'with ',
                ]
                for pattern in incomplete_patterns:
                    if last_line.endswith(pattern) or last_line.endswith(pattern.rstrip()):
                        errors.append(f"Incomplete statement at end of file: '{last_line[-50:]}'")
                        break
            
            # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³è¨˜å·ã®æ®‹å­˜ãƒã‚§ãƒƒã‚¯
            if content.strip().startswith('```'):
                errors.append("File starts with markdown code block marker")
            
            is_complete = len(errors) == 0
            
            if not is_complete:
                self.logger.debug(f"[Completeness Check] âŒ Incomplete Python: {filepath}")
                for error in errors:
                    self.logger.debug(f"  - {error}")
            else:
                self.logger.debug(f"[Completeness Check] âœ… Complete Python: {filepath}")
            
            return is_complete, errors
        
        # CSSãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯ç‰¹åˆ¥å‡¦ç†
        if filepath.endswith('.css'):
            errors = []
            
            # ãƒ–ãƒ¬ãƒ¼ã‚¹ã®æ•°ãƒã‚§ãƒƒã‚¯
            open_braces = content.count('{')
            close_braces = content.count('}')
            if open_braces != close_braces:
                errors.append(f"Unmatched braces: {open_braces} open, {close_braces} close")
            
            # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³è¨˜å·ã®æ®‹å­˜ãƒã‚§ãƒƒã‚¯
            if content.strip().startswith('```'):
                errors.append("File starts with markdown code block marker")
            
            is_complete = len(errors) == 0
            
            if not is_complete:
                self.logger.debug(f"[Completeness Check] âŒ Incomplete CSS: {filepath}")
                for error in errors:
                    self.logger.debug(f"  - {error}")
            else:
                self.logger.debug(f"[Completeness Check] âœ… Complete CSS: {filepath}")
            
            return is_complete, errors
        
        # JavaScript/TypeScriptãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚§ãƒƒã‚¯
        if not filepath.endswith(('.js', '.jsx', '.ts', '.tsx')):
            return True, []
        
        errors = []
        
        # 1. export default ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯ï¼ˆApp.jsç­‰ã®ä¸»è¦ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
        if 'App.js' in filepath or 'App.jsx' in filepath or 'Main.js' in filepath or 'Main.jsx' in filepath:
            if 'export default' not in content and 'export {' not in content:
                errors.append("Missing 'export default' statement")
        
        # 2. æ‹¬å¼§ã®æ•°ãƒã‚§ãƒƒã‚¯
        open_braces = content.count('{')
        close_braces = content.count('}')
        if open_braces != close_braces:
            errors.append(f"Unmatched braces: {open_braces} open, {close_braces} close")
        
        open_parens = content.count('(')
        close_parens = content.count(')')
        if open_parens != close_parens:
            errors.append(f"Unmatched parentheses: {open_parens} open, {close_parens} close")
        
        open_brackets = content.count('[')
        close_brackets = content.count(']')
        if open_brackets != close_brackets:
            errors.append(f"Unmatched brackets: {open_brackets} open, {close_brackets} close")
        
        # 3. ä¸å®Œå…¨ãªæœ€çµ‚è¡Œã®ãƒã‚§ãƒƒã‚¯
        lines = content.strip().split('\n')
        if lines:
            last_line = lines[-1].strip()
            incomplete_patterns = [
                'alt="', 'src="', 'href="',
                '<img ', '<a ', '<div ',
                'const ', 'let ', 'var ',
                'import ', 'from ',
                'function ', 'class ',
                '= {', '= [', '= (',
            ]
            for pattern in incomplete_patterns:
                if last_line.endswith(pattern) or last_line.endswith(pattern.rstrip()):
                    errors.append(f"Incomplete statement at end of file: '{last_line[-50:]}'")
                    break
        
        # 4. ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³è¨˜å·ã®æ®‹å­˜ãƒã‚§ãƒƒã‚¯
        if content.strip().startswith('```'):
            errors.append("File starts with markdown code block marker")
        
        is_complete = len(errors) == 0
        
        if not is_complete:
            self.logger.debug(f"[Completeness Check] âŒ Incomplete code: {filepath}")
            for error in errors:
                self.logger.debug(f"  - {error}")
        else:
            self.logger.debug(f"[Completeness Check] âœ… Complete code: {filepath}")
        
        return is_complete, errors

    
    def _validate_javascript_syntax(self, filepath: str, content: str) -> Tuple[bool, Optional[str]]:
        """
        JavaScript/JSXã®æ§‹æ–‡ãƒã‚§ãƒƒã‚¯ï¼ˆNode.jsä½¿ç”¨ï¼‰
        
        Args:
            filepath: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            content: ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹
        
        Returns:
            (is_valid, error_message): æ§‹æ–‡ã®å¦¥å½“æ€§ã¨ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        """
        if not filepath.endswith(('.js', '.jsx', '.ts', '.tsx')):
            return True, None
        
        # Node.jsãŒåˆ©ç”¨å¯èƒ½ã‹ç¢ºèª
        try:
            result = subprocess.run(
                ['node', '--version'],
                capture_output=True,
                timeout=2,
                encoding='utf-8',
                errors='replace'
            )
            if result.returncode != 0:
                self.logger.debug(f"[Syntax Check] âš   Node.js not available, skipping syntax check")
                return True, None
        except (FileNotFoundError, subprocess.TimeoutExpired):
            self.logger.debug(f"[Syntax Check] âš   Node.js not available, skipping syntax check")
            return True, None
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã‚“ã§æ§‹æ–‡ãƒã‚§ãƒƒã‚¯
        import tempfile
        with tempfile.NamedTemporaryFile(mode='w', suffix='.js', delete=False, encoding='utf-8') as f:
            f.write(content)
            temp_path = f.name
        
        try:
            result = subprocess.run(
                ['node', '--check', temp_path],
                capture_output=True,
                timeout=5,
                encoding='utf-8',
                errors='replace'
            )
            
            if result.returncode != 0:
                error_message = result.stderr.strip()
                self.logger.debug(f"[Syntax Check] âŒ Syntax error: {filepath}")
                self.logger.debug(f"  Error: {error_message}")
                return False, error_message
            
            self.logger.debug(f"[Syntax Check] âœ… Valid syntax: {filepath}")
            return True, None
        
        except subprocess.TimeoutExpired:
            self.logger.debug(f"[Syntax Check] âš   Timeout during syntax check: {filepath}")
            return True, None
        
        finally:
            try:
                os.unlink(temp_path)
            except:
                pass

    def _generate_missing_files(
        self,
        generated_files: Dict[str, str],
        missing_requirements: List[FileRequirement],
        goal: str
    ) -> Dict[str, str]:
        """ä¸è¶³ã—ã¦ã„ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ"""
        completed_files = generated_files.copy()
        
        if not missing_requirements:
            return completed_files
        
        self.logger.debug(f"[Post-Validation] Generating {len(missing_requirements)} missing files...")
        
        # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚å«ã‚ãŸå®Œå…¨ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰
        existing_files_context = []
        
        # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ï¼ˆå…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
        for requirement in missing_requirements:
            existing_file_path = Path(requirement.filepath)
            if existing_file_path.exists():
                try:
                    with open(existing_file_path, 'r', encoding='utf-8') as f:
                        original_content = f.read()
                    existing_files_context.append(
                        f"## {requirement.filepath} (ORIGINAL VERSION - PRESERVE FUNCTIONALITY)\n```\n{original_content}\n```"
                    )
                except Exception as e:
                    self.logger.debug(f"[Post-Validation] Could not read existing file {requirement.filepath}: {e}")
        
        # ç”Ÿæˆæ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ ï¼ˆå…¨æ–‡ï¼‰
        for filename, code in generated_files.items():
            existing_files_context.append(
                f"## {filename} (NEWLY GENERATED)\n```\n{code}\n```"
            )
        
        existing_context = "\n\n".join(existing_files_context)
        
        for requirement in missing_requirements:
            filepath = requirement.filepath
            file_type = requirement.file_type
            reason = requirement.reason
            max_retries = 2
            
            for attempt in range(max_retries):
                self.logger.debug(f"[Post-Validation] Generating: {filepath} (Attempt {attempt + 1}/{max_retries})")
                
            
                self.logger.debug(f"[Post-Validation] Generating: {filepath} (Reason: {reason})")
                self.logger.debug(f"[Post-Validation] Existing context length: {len(existing_context)} chars")
                self.logger.debug(f"[Post-Validation] Context includes {len(existing_files_context)} files")
            
                # è§£æ±ºç­–2+3: ãƒ•ã‚¡ã‚¤ãƒ«åã¨Reasonã‹ã‚‰ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’åˆ¤å®š
                is_react_file = (
                    filepath.startswith('src/') and 
                    filepath.endswith(('.js', '.jsx', '.tsx'))
                )
                is_react_project = 'React' in reason
                
                # ğŸ†• no_framework_modeã®å ´åˆã¯ReactæŒ‡ç¤ºã‚’ã‚¹ã‚­ãƒƒãƒ—
                if self.no_framework_mode and is_react_project:
                    self.logger.debug(f"[Post-Validation] Skipping React instruction for {filepath} due to no-framework mode")
                    is_react_project = False
                
                # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å›ºæœ‰ã®æŒ‡ç¤ºã‚’æ§‹ç¯‰
                framework_instruction = ""
                if is_react_file and is_react_project:
                    self.logger.debug(f"[Post-Validation] Detected React file: {filepath}")
                    framework_instruction = """
âš ï¸ FRAMEWORK REQUIREMENT: REACT
This file MUST be created as a React component/module.

MANDATORY REACT STRUCTURE:
1. Import React at the top:
   - For components: import React from 'react';
   - For index.js: import React from 'react'; import ReactDOM from 'react-dom/client';

2. Create a React component:
   - Functional component (preferred): function App() { ... } or const App = () => { ... }
   - Class component: class App extends React.Component { ... }

3. Use JSX syntax for UI elements:
   - Return JSX: return <div>...</div>;
   - NOT vanilla DOM manipulation

4. Export the component:
   - export default ComponentName;

5. For index.js specifically:
   - Import the App component: import App from './App';
   - Use ReactDOM: const root = ReactDOM.createRoot(document.getElementById('root'));
   - Render: root.render(<App />);

âš ï¸ CRITICAL: DO NOT create vanilla JavaScript. This MUST be React code with imports and JSX.

"""
            
                completion_prompt = framework_instruction + f"""Generate the missing file for the project.

    âš  CRITICAL INSTRUCTIONS:
    1. An ORIGINAL version of this file may exist - you MUST preserve its functionality
    2. You are MODIFYING an existing file, NOT creating from scratch
    3. PRESERVE the existing code structure, event handlers, and method names
    4. ONLY ADD the new functionality requested in the goal
    5. DO NOT change existing working code unless absolutely necessary
    6. DO NOT introduce new method names that don't exist in JavaScript files
    7. CHECK that all method calls in HTML match method definitions in JavaScript

    Original Goal: {goal}

    Context - Existing and Generated Files:
    {existing_context}

    Missing/Incomplete File to Generate:
    - File: {filepath}
    - Type: {file_type}
    - Reason: {reason}

    REQUIREMENTS:
    1. Generate the COMPLETE file: {filepath}
    2. If an ORIGINAL version exists above, PRESERVE its structure and functionality
    3. Ensure ALL method calls in HTML match method definitions in JavaScript
    4. Use the SAME event handling approach as the original (e.g., data-action vs data-operation)
    5. Use the SAME data attribute style (e.g., data-number="7" not just data-number)
    6. Make it production-ready and complete
    7. DO NOT use method names that don't exist (check JavaScript class methods first)

    Generate the complete code for {filepath}:"""
            
                # æ”¹å–„ç­–B: ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã«å¿œã˜ã¦max_tokensã‚’å‹•çš„è¨­å®š
                # 
                # å•é¡Œ: app.jsï¼ˆå°æ–‡å­—ï¼‰ãŒLARGE_FILE_PATTERNSã«ãƒãƒƒãƒã›ãšã€
                #       max_tokens=4000ã§ã¯è¤‡é›‘ãªJSãƒ•ã‚¡ã‚¤ãƒ«ãŒé€”ä¸­ã§åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã‚‹
                # 
                # è§£æ±ºç­–: 
                # 1. JSç³»ãƒ•ã‚¡ã‚¤ãƒ«ã¯åŸºæœ¬çš„ã«å¤§ããªmax_tokensã‚’ä½¿ç”¨
                # 2. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒå¤§ãã„å ´åˆã¯æ›´ã«å¢—ã‚„ã™
                # 3. ç‰¹ã«é‡è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯æœ€å¤§å€¤ã‚’ä½¿ç”¨
                
                LARGE_FILE_PATTERNS = [
                    'App.js', 'App.jsx', 'App.tsx',
                    'app.js', 'app.jsx', 'app.tsx',  # å°æ–‡å­—ã‚‚è¿½åŠ 
                    'Main.js', 'Main.jsx', 'Main.tsx',
                    'main.js', 'main.jsx', 'main.tsx',  # å°æ–‡å­—ã‚‚è¿½åŠ 
                    'index.js', 'index.jsx', 'index.tsx',  # index.js ã‚‚è¿½åŠ 
                    'script.js', 'scripts.js',  # ä¸€èˆ¬çš„ãªJSãƒ•ã‚¡ã‚¤ãƒ«å
                    'calendar.js', 'todo.js', 'calculator.js',  # ã‚¢ãƒ—ãƒªç³»
                ]
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã«åŸºã¥ãåŸºæœ¬max_tokens
                JS_FILE_EXTENSIONS = ['.js', '.jsx', '.ts', '.tsx']
                is_js_file = any(filepath.endswith(ext) for ext in JS_FILE_EXTENSIONS)
                is_large_file_pattern = any(pattern in filepath for pattern in LARGE_FILE_PATTERNS)
                
                # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®é•·ã•ã«åŸºã¥ãåˆ¤æ–­
                context_length = len(existing_context)
                is_large_context = context_length > 10000  # 10KBä»¥ä¸Š
                
                # max_tokens ã®æ±ºå®šãƒ­ã‚¸ãƒƒã‚¯
                if is_large_file_pattern:
                    # é‡è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ â†’ æœ€å¤§å€¤
                    max_tokens = 16000
                    self.logger.debug(f"[Post-Validation] Large file pattern detected: {filepath}, using max_tokens={max_tokens}")
                elif is_js_file and is_large_context:
                    # JSç³» + å¤§ããªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ â†’ å¤§ããªå‡ºåŠ›ãŒå¿…è¦
                    max_tokens = 16000
                    self.logger.debug(f"[Post-Validation] JS file with large context ({context_length} chars): {filepath}, using max_tokens={max_tokens}")
                elif is_js_file:
                    # JSç³»ãƒ•ã‚¡ã‚¤ãƒ« â†’ åŸºæœ¬çš„ã«å¤§ãã‚
                    max_tokens = 8000
                    self.logger.debug(f"[Post-Validation] JS file detected: {filepath}, using max_tokens={max_tokens}")
                else:
                    # ãã®ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«
                    max_tokens = 4000
            
                try:
                    response = self.llm_manager.generate_response(
                        prompt=completion_prompt,
                        system_prompt="You are a code generation expert. Generate complete, production-ready code files.",
                        max_tokens=max_tokens
                    )
                
                    if hasattr(response, 'content'):
                        response_text = response.content
                    else:
                        response_text = str(response)
                
                    # å¯¾ç­–5-A: æ”¹å–„ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯
                    file_code = self._extract_code_from_response(response_text, filepath)
                    
                    # ä¸å®Œå…¨ãƒ•ãƒ©ã‚°ãƒã‚§ãƒƒã‚¯
                    if file_code.startswith("__INCOMPLETE__"):
                        file_code = file_code.replace("__INCOMPLETE__\n", "", 1)
                        self.logger.debug(f"[Post-Validation] âš ï¸  Incomplete code detected: {filepath}")
                        if attempt < max_retries - 1:
                            self.logger.debug(f"[Post-Validation] ğŸ”„ Retrying due to incomplete code...")
                            continue  # å†è©¦è¡Œ
                        else:
                            self.logger.debug(f"[Post-Validation] âŒ Failed to generate complete code after {max_retries} attempts")
                            self.logger.debug(f"  Saving incomplete code anyway (manual review required)")
                            # å…ƒã®ã‚³ãƒ¼ãƒ‰ã¨åŒã˜å‹•ä½œ: ä¸å®Œå…¨ã§ã‚‚ä¿å­˜ã—ã¦å‡¦ç†ã‚’ç¶™ç¶š
                    
                    completed_files[filepath] = file_code
                    
                    # å¯¾ç­–5-B: æ§‹æ–‡ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥ï¼‰
                    syntax_valid = True
                    syntax_error = None
                    
                    # Pythonæ§‹æ–‡ãƒã‚§ãƒƒã‚¯
                    if filepath.endswith('.py'):
                        syntax_valid, syntax_error = self._validate_python_syntax(file_code, filepath)
                    # JavaScript/TypeScriptæ§‹æ–‡ãƒã‚§ãƒƒã‚¯
                    elif filepath.endswith(('.js', '.jsx', '.ts', '.tsx')):
                        syntax_valid, syntax_error = self._validate_javascript_syntax(filepath, file_code)
                    # JSONæ§‹æ–‡ãƒã‚§ãƒƒã‚¯
                    elif filepath.endswith('.json'):
                        syntax_valid, syntax_error = self._validate_json_syntax(filepath, file_code)
                    # YAMLæ§‹æ–‡ãƒã‚§ãƒƒã‚¯
                    elif filepath.endswith(('.yaml', '.yml')):
                        syntax_valid, syntax_error = self._validate_yaml_syntax(filepath, file_code)
                    # Markdownæ§‹æ–‡ãƒã‚§ãƒƒã‚¯
                    elif filepath.endswith(('.md', '.markdown')):
                        syntax_valid, syntax_error = self._validate_markdown_syntax(filepath, file_code)
                    # XML/SVGæ§‹æ–‡ãƒã‚§ãƒƒã‚¯
                    elif filepath.endswith(('.xml', '.svg')):
                        syntax_valid, syntax_error = self._validate_xml_syntax(filepath, file_code)
                    # CSSæ§‹æ–‡ãƒã‚§ãƒƒã‚¯
                    elif filepath.endswith('.css'):
                        syntax_valid, syntax_error = self._validate_css_syntax(filepath, file_code)
                    
                    # å¯¾ç­–5-C: å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯
                    complete_valid, completeness_errors = self._validate_code_completeness(filepath, file_code)
                    
                    # å¯¾ç­–5-D: è¿½åŠ ãƒã‚§ãƒƒã‚¯ï¼ˆè­¦å‘Šãƒ¬ãƒ™ãƒ«ï¼‰
                    # å‘½åè¦å‰‡ãƒã‚§ãƒƒã‚¯
                    naming_valid, naming_warnings = self._validate_naming_conventions(filepath, file_code)
                    if naming_warnings:
                        self.logger.debug(f"[Post-Validation] âš ï¸  Style warnings: {filepath}")
                        for warning in naming_warnings[:3]:
                            self.logger.debug(f"    {warning}")
                    
                    # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒã‚§ãƒƒã‚¯
                    encoding_valid, encoding_msg = self._validate_encoding(filepath, file_code)
                    if encoding_msg:
                        self.logger.debug(f"[Post-Validation] âš ï¸  Encoding: {encoding_msg}")
                    
                    # æ”¹è¡Œã‚³ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
                    lineend_valid, lineend_msg = self._validate_line_endings(filepath, file_code)
                    if lineend_msg:
                        self.logger.debug(f"[Post-Validation] âš ï¸  Line endings: {lineend_msg}")
                    
                    # æ¤œè¨¼çµæœã«åŸºã¥ã„ã¦å‡¦ç†
                    if syntax_valid and complete_valid:
                        # âœ… æ¤œè¨¼æˆåŠŸ
                        self.logger.debug(f"[Post-Validation] âœ… Validation passed: {filepath}")
                        break  # å†è©¦è¡Œãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
                    else:
                        # âŒ æ¤œè¨¼å¤±æ•—
                        if attempt < max_retries - 1:
                            self.logger.debug(f"[Post-Validation] ğŸ”„ Retrying generation for {filepath}...")
                            if not syntax_valid:
                                self.logger.debug(f"  Reason: Syntax error - {syntax_error}")
                            if not complete_valid:
                                self.logger.debug(f"  Reason: Incomplete code - {completeness_errors}")
                        else:
                            # æœ€çµ‚è©¦è¡Œã§ã‚‚å¤±æ•— â†’ ãã‚Œã§ã‚‚ä¿å­˜
                            self.logger.debug(f"[Post-Validation] âš   Failed validation after {max_retries} attempts: {filepath}")
                            self.logger.debug(f"  Saving anyway (manual review required)")
                
                except Exception as e:
                    self.logger.debug(f"[Post-Validation] âŒ Failed to generate {filepath}: {e}")
                    import traceback
                    self.logger.debug(traceback.format_exc())
        
        self.logger.debug(f"[Post-Validation] Completion finished: {len(completed_files)} total files")
        
        # ç”Ÿæˆå¾Œã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        consistency_issues = self._validate_file_consistency(completed_files)
        if consistency_issues:
            self.logger.debug("[Post-Validation] âš   Consistency issues detected:")
            for issue in consistency_issues:
                self.logger.debug(f"   - {issue}")
        
        return completed_files
    
    def _validate_file_consistency(self, files: Dict[str, str]) -> List[str]:
        """
        ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«é–“ã®æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯
        
        Args:
            files: {filename: code}
        
        Returns:
            æ•´åˆæ€§ã®å•é¡Œãƒªã‚¹ãƒˆ
        """
        issues = []
        
        # â­ ä¿®æ­£B: æœªå®Ÿè£…ãƒ¡ã‚½ãƒƒãƒ‰æ¤œå‡ºï¼ˆpass onlyï¼‰
        issues.extend(self._check_unimplemented_methods(files))
        
        # HTMLã¨JavaScriptã®ãƒšã‚¢ã‚’æ¢ã™
        html_files = {f: c for f, c in files.items() if f.endswith('.html')}
        js_files = {f: c for f, c in files.items() if f.endswith('.js')}
        
        for html_file, html_code in html_files.items():
            for js_file, js_code in js_files.items():
                # HTMLã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã¦ã„ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æŠ½å‡º
                method_calls = re.findall(r'calculator\.(\w+)\(', html_code)
                
                # JavaScriptã§å®šç¾©ã•ã‚Œã¦ã„ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æŠ½å‡º
                method_definitions = re.findall(r'^\s*(\w+)\s*\([^)]*\)\s*\{', js_code, re.MULTILINE)
                
                # å‘¼ã³å‡ºã•ã‚Œã¦ã„ã‚‹ãŒå®šç¾©ã•ã‚Œã¦ã„ãªã„ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æ¤œå‡º
                undefined_methods = set(method_calls) - set(method_definitions)
                for method in undefined_methods:
                    issues.append(
                        f"{html_file}: Method 'calculator.{method}()' is called but not defined in {js_file}"
                    )
                
                # dataå±æ€§ã®å€¤ãƒã‚§ãƒƒã‚¯ï¼ˆdata-number ãªã©ãŒç©ºã§ãªã„ã‹ï¼‰
                empty_data_attrs = re.findall(r'data-(\w+)>(?!\w)', html_code)
                if empty_data_attrs:
                    for attr in set(empty_data_attrs):
                        issues.append(
                            f"{html_file}: data-{attr} attribute has no value (should be data-{attr}=\"value\")"
                        )
        
        return issues
    
    def _check_unimplemented_methods(self, files: Dict[str, str]) -> List[str]:
        """
        â­ ä¿®æ­£B: æœªå®Ÿè£…ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆpass onlyï¼‰ã‚’æ¤œå‡º
        
        def method_name(...):
            pass
        
        ã¾ãŸã¯
        
        def method_name(...):
            '''docstring'''
            pass
        
        ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºã—ã€è­¦å‘Šã‚’è¿”ã™ã€‚
        
        Args:
            files: {filename: code}
        
        Returns:
            æœªå®Ÿè£…ãƒ¡ã‚½ãƒƒãƒ‰ã®è­¦å‘Šãƒªã‚¹ãƒˆ
        """
        issues = []
        
        for filename, code in files.items():
            if not filename.endswith('.py'):
                continue
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³1: def xxx(...): ã®å¾Œã« pass ã®ã¿ï¼ˆdocstringãªã—ï¼‰
            # ãƒ‘ã‚¿ãƒ¼ãƒ³2: def xxx(...): ã®å¾Œã« docstring + pass
            # 
            # æ­£è¦è¡¨ç¾ã§æ¤œå‡º:
            # def ãƒ¡ã‚½ãƒƒãƒ‰å(å¼•æ•°): 
            #     (ã‚ªãƒ—ã‚·ãƒ§ãƒ³: """docstring""" ã¾ãŸã¯ '''docstring''')
            #     pass
            
            pattern = re.compile(
                r'def\s+(\w+)\s*\([^)]*\)\s*(?:->\s*[^:]+)?\s*:\s*\n'  # def xxx(...) -> type:
                r'(?:\s*(?:"""[\s\S]*?"""|\'\'\'[\s\S]*?\'\'\')\s*\n)?'  # optional docstring
                r'\s*pass\s*(?:\n|$)',  # pass
                re.MULTILINE
            )
            
            matches = pattern.findall(code)
            
            for method_name in matches:
                # __init__ ã‚„ __repr__ ãªã©ã®ç‰¹æ®Šãƒ¡ã‚½ãƒƒãƒ‰ã§ pass ã®ã¿ã¯è¨±å®¹
                if method_name.startswith('__') and method_name.endswith('__'):
                    continue
                
                issues.append(
                    f"{filename}: Method '{method_name}()' is not implemented (pass only)"
                )
                logger.debug(f"[Consistency] Unimplemented method detected: {filename}:{method_name}()")
        
        return issues


@dataclass
class FileReferenceRule:
    """ãƒ•ã‚¡ã‚¤ãƒ«å‚ç…§ãƒ«ãƒ¼ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    name: str
    phase: int
    priority: str
    trigger: str
    target: str
    prompt: str


class FileReferenceRuleParser:
    """ãƒ•ã‚¡ã‚¤ãƒ«å‚ç…§ãƒ«ãƒ¼ãƒ«ã®ãƒ‘ãƒ¼ã‚µãƒ¼"""
    
    def __init__(self, logger: Optional[logging.Logger] = None):
        self.logger = logger or logging.getLogger(__name__)
        self.rules: List[FileReferenceRule] = []
    
    def load_rules(self, project_root: Optional[Path] = None) -> List[FileReferenceRule]:
        """
        ãƒ«ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        
        å„ªå…ˆé †ä½:
        1. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå›ºæœ‰ãƒ«ãƒ¼ãƒ« (<project>/.cognix/file_reference_rules.md)
        2. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ«ãƒ¼ãƒ« (~/.cognix/rules/default_file_reference_rules.md)
        
        Args:
            project_root: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆNoneã®å ´åˆã¯ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼‰
        
        Returns:
            èª­ã¿è¾¼ã¾ã‚ŒãŸãƒ«ãƒ¼ãƒ«ã®ãƒªã‚¹ãƒˆ
        """
        rules = []
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ«ãƒ¼ãƒ« - å„ªå…ˆé †ä½ä»˜ãã§è¤‡æ•°ãƒ‘ã‚¹ã‚’è©¦ã™
        # å„ªå…ˆé †ä½1: ~/.cognix/rules/default_file_reference_rules.md
        default_rules_path = Path.home() / '.cognix' / 'rules' / 'default_file_reference_rules.md'
        
        # å„ªå…ˆé †ä½2: ~/.cognix/default_file_reference_rules.md
        if not default_rules_path.exists():
            default_rules_path = Path.home() / '.cognix' / 'default_file_reference_rules.md'
        
        if default_rules_path.exists():
            try:
                default_rules = self._parse_rule_file(default_rules_path)
                rules.extend(default_rules)
                self.logger.debug(f"[Rule Loader] âœ… Loaded {len(default_rules)} default rules from {default_rules_path}")
            except Exception as e:
                self.logger.warning(f"[Rule Loader] âš ï¸  Failed to load default rules: {e}")
        else:
            self.logger.debug(f"[Rule Loader] âš ï¸  Default rules not found in any location")
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå›ºæœ‰ãƒ«ãƒ¼ãƒ« (<project>/.cognix/file_reference_rules.md)
        if project_root:
            project_rules_path = project_root / '.cognix' / 'file_reference_rules.md'
            if project_rules_path.exists():
                try:
                    project_rules = self._parse_rule_file(project_rules_path)
                    
                    # åŒã˜åå‰ã®ãƒ«ãƒ¼ãƒ«ã¯ä¸Šæ›¸ã
                    default_rule_names = {r.name for r in rules}
                    for rule in project_rules:
                        if rule.name in default_rule_names:
                            rules = [r for r in rules if r.name != rule.name]
                            self.logger.debug(f"[Rule Loader] ğŸ”„ Overriding default rule: {rule.name}")
                        rules.append(rule)
                    
                    self.logger.debug(f"[Rule Loader] âœ… Loaded {len(project_rules)} project rules from {project_rules_path}")
                except Exception as e:
                    self.logger.warning(f"[Rule Loader] âš ï¸  Failed to load project rules: {e}")
        
        self.rules = rules
        self.logger.debug(f"[Rule Loader] ğŸ“‹ Total rules loaded: {len(rules)}")
        return rules
    
    def _parse_rule_file(self, filepath: Path) -> List[FileReferenceRule]:
        """
        Markdownãƒ«ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‘ãƒ¼ã‚¹
        
        ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:
        ## Rule: [Rule-Name]
        
        **Phase:** [1|2|3]
        **Priority:** [Critical|High|Medium|Low]
        **Trigger:** [Condition]
        **Target:** [Condition]
        
        **Prompt:**
        ```
        [Prompt text]
        ```
        
        Args:
            filepath: ãƒ«ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        
        Returns:
            ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸãƒ«ãƒ¼ãƒ«ã®ãƒªã‚¹ãƒˆ
        """
        rules = []
        
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # "## Rule:" ã§å§‹ã¾ã‚‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’åˆ†å‰²
        rule_sections = re.split(r'\n## Rule:\s*', content)
        
        for section in rule_sections[1:]:  # æœ€åˆã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯ãƒ˜ãƒƒãƒ€ãƒ¼ãªã®ã§ã‚¹ã‚­ãƒƒãƒ—
            try:
                rule = self._parse_rule_section(section)
                if rule:
                    rules.append(rule)
            except Exception as e:
                self.logger.warning(f"[Rule Parser] Failed to parse rule section: {e}")
                continue
        
        return rules
    
    def _parse_rule_section(self, section: str) -> Optional[FileReferenceRule]:
        """
        ãƒ«ãƒ¼ãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ãƒ‘ãƒ¼ã‚¹
        
        Args:
            section: ãƒ«ãƒ¼ãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚­ã‚¹ãƒˆ
        
        Returns:
            FileReferenceRuleã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€ã¾ãŸã¯None
        """
        # ãƒ«ãƒ¼ãƒ«åã‚’æŠ½å‡ºï¼ˆæœ€åˆã®è¡Œï¼‰
        lines = section.split('\n')
        name = lines[0].strip()
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        phase_match = re.search(r'\*\*Phase:\*\*\s*(\d+)', section)
        priority_match = re.search(r'\*\*Priority:\*\*\s*(Critical|High|Medium|Low)', section)
        trigger_match = re.search(r'\*\*Trigger:\*\*\s*([^\n]+)', section)
        target_match = re.search(r'\*\*Target:\*\*\s*([^\n]+)', section)
        
        if not all([phase_match, priority_match, trigger_match, target_match]):
            self.logger.debug(f"[Rule Parser] Incomplete rule metadata: {name}")
            return None
        
        phase = int(phase_match.group(1))
        priority = priority_match.group(1)
        trigger = trigger_match.group(1).strip()
        target = target_match.group(1).strip()
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æŠ½å‡º
        # **Prompt:** ã®å¾Œã® ```...``` ãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡º
        prompt_match = re.search(r'\*\*Prompt:\*\*\s*```\s*\n(.*?)\n```', section, re.DOTALL)
        if not prompt_match:
            self.logger.warning(f"[Rule Parser] No prompt found for rule: {name}")
            return None
        
        prompt = prompt_match.group(1).strip()
        
        return FileReferenceRule(
            name=name,
            phase=phase,
            priority=priority,
            trigger=trigger,
            target=target,
            prompt=prompt
        )
    
    def evaluate_trigger(self, rule: FileReferenceRule, filepaths: List[str]) -> bool:
        """
        ãƒˆãƒªã‚¬ãƒ¼æ¡ä»¶ã‚’è©•ä¾¡
        
        å¯¾å¿œã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³:
        - "HTML files exist (*.html)" â†’ any(f.endswith('.html') for f in filepaths)
        - "React component files exist (*.jsx, *.tsx)" â†’ è¤‡æ•°ã®æ‹¡å¼µå­
        - "Progressive Web App keywords detected (PWA, service worker, offline)" â†’ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢
        - "Flask project detected" â†’ .pyãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨
        - "Django project detected" â†’ .pyãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨
        - "FastAPI detected" â†’ .pyãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨
        
        Args:
            rule: è©•ä¾¡ã™ã‚‹ãƒ«ãƒ¼ãƒ«
            filepaths: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        
        Returns:
            æ¡ä»¶ã‚’æº€ãŸã™å ´åˆTrue
        """
        trigger = rule.trigger.lower()
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: "files exist (*.ext)" å½¢å¼
        exist_match = re.search(r'files? exist.*?\(([\*\.\w\s,]+)\)', trigger)
        if exist_match:
            extensions = exist_match.group(1)
            # æ‹¡å¼µå­ã‚’æŠ½å‡ºï¼ˆä¾‹: "*.html, *.htm" â†’ [".html", ".htm"]ï¼‰
            ext_list = [ext.strip().replace('*', '') for ext in extensions.split(',')]
            return any(
                any(filepath.endswith(ext) for ext in ext_list)
                for filepath in filepaths
            )
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: "keywords detected (...)" å½¢å¼
        keyword_match = re.search(r'keywords? detected.*?\(([^\)]+)\)', trigger)
        if keyword_match:
            keywords = [kw.strip().lower() for kw in keyword_match.group(1).split(',')]
            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã¾ãŸã¯ãƒ•ã‚¡ã‚¤ãƒ«åã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹ã‹ç¢ºèª
            return any(
                any(keyword in filepath.lower() for keyword in keywords)
                for filepath in filepaths
            )
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: "directives detected in CSS" å½¢å¼ï¼ˆTailwindç­‰ï¼‰
        if 'directives detected' in trigger or 'directive detected' in trigger:
            # CSSãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèªï¼ˆå®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®ãƒã‚§ãƒƒã‚¯ã¯çœç•¥ï¼‰
            return any(filepath.endswith('.css') for filepath in filepaths)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³4: ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆFlask, Django, FastAPIç­‰ï¼‰
        # "Flask project detected", "Django project detected", "FastAPI detected" ãªã©
        framework_patterns = {
            'flask': ['.py', '.html'],  # Flaskã¯Pythonã¨HTMLãŒå¿…è¦
            'django': ['.py', '.html'],
            'fastapi': ['.py'],
            'express': ['.js'],
        }
        for framework, required_exts in framework_patterns.items():
            if f'{framework} project detected' in trigger or f'{framework} detected' in trigger:
                # å¿…è¦ãªæ‹¡å¼µå­ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
                has_required = all(
                    any(filepath.endswith(ext) for filepath in filepaths)
                    for ext in required_exts
                )
                if has_required:
                    self.logger.debug(f"[Rule Evaluator] âœ… {framework.capitalize()} framework detected")
                    return True
                return False
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³5: "Blueprints detected" ãªã©ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        if 'blueprint' in trigger:
            return any(filepath.endswith('.py') for filepath in filepaths)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³6: ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯ï¼ˆrequirements.txt existsç­‰ï¼‰
        file_exists_patterns = [
            ('requirements.txt exists', 'requirements.txt'),
            ('package.json exists', 'package.json'),
            ('dockerfile exists', 'dockerfile'),
            ('next.config' in trigger, 'next.config'),
            ('nuxt.config' in trigger, 'nuxt.config'),
            ('remix.config' in trigger, 'remix.config'),
        ]
        for pattern, filename in file_exists_patterns:
            if pattern in trigger if isinstance(pattern, str) else pattern:
                return any(filename.lower() in filepath.lower() for filepath in filepaths)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³7: "HTML files referencing" ãªã©å‚ç…§ãƒã‚§ãƒƒã‚¯
        if 'html files referencing' in trigger or 'referencing css' in trigger or 'referencing js' in trigger:
            return any(filepath.endswith('.html') for filepath in filepaths)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³8: "any ui-related files" â†’ HTML/CSS/JSã®ã„ãšã‚Œã‹ãŒå­˜åœ¨
        if 'ui-related' in trigger or 'ui related' in trigger:
            ui_extensions = ['.html', '.css', '.js', '.jsx', '.tsx', '.vue', '.svelte']
            return any(
                any(filepath.endswith(ext) for ext in ui_extensions)
                for filepath in filepaths
            )
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³9: "calculator pattern detected" â†’ ç‰¹å®šã®ãƒ‘ã‚¿ãƒ¼ãƒ³å
        if 'pattern detected' in trigger:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³åã‚’æŠ½å‡ºï¼ˆä¾‹: "calculator pattern detected" â†’ "calculator"ï¼‰
            pattern_match = re.search(r'(\w+)\s+pattern\s+detected', trigger)
            if pattern_match:
                # ãƒ‘ã‚¿ãƒ¼ãƒ³åãŒãƒ•ã‚¡ã‚¤ãƒ«åã«å«ã¾ã‚Œã‚‹ã‹ç¢ºèª
                pattern_name = pattern_match.group(1)
                return any(pattern_name in filepath.lower() for filepath in filepaths)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³10: "package.json with scripts exists" â†’ package.jsonãŒå­˜åœ¨
        if 'package.json with' in trigger:
            return any('package.json' in filepath.lower() for filepath in filepaths)
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: æ¡ä»¶ã‚’æº€ãŸã•ãªã„
        self.logger.debug(f"[Rule Evaluator] Unknown trigger pattern: {trigger}")
        return False
    
    def evaluate_target(self, rule: FileReferenceRule, filepaths: List[str]) -> bool:
        """
        ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ¡ä»¶ã‚’è©•ä¾¡
        
        Args:
            rule: è©•ä¾¡ã™ã‚‹ãƒ«ãƒ¼ãƒ«
            filepaths: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        
        Returns:
            æ¡ä»¶ã‚’æº€ãŸã™å ´åˆTrue
        """
        target = rule.target.lower()
        
        # "Always" ã®å ´åˆã¯å¸¸ã«True
        if target == 'always':
            return True
        
        # ãã‚Œä»¥å¤–ã¯evaluate_triggerã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯
        return self.evaluate_trigger(
            FileReferenceRule(
                name=rule.name,
                phase=rule.phase,
                priority=rule.priority,
                trigger=rule.target,  # targetã‚’triggerã¨ã—ã¦è©•ä¾¡
                target='Always',
                prompt=rule.prompt
            ),
            filepaths
        )
    
    def get_applicable_rules(
        self, 
        filepaths: List[str], 
        max_phase: int = 3,
        priority_filter: Optional[List[str]] = None
    ) -> List[FileReferenceRule]:
        """
        é©ç”¨å¯èƒ½ãªãƒ«ãƒ¼ãƒ«ã‚’å–å¾—
        
        Args:
            filepaths: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
            max_phase: æœ€å¤§ãƒ•ã‚§ãƒ¼ã‚ºï¼ˆ1-3ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3 = ã™ã¹ã¦ï¼‰
            priority_filter: å„ªå…ˆé †ä½ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆä¾‹: ['High', 'Medium']ï¼‰
        
        Returns:
            é©ç”¨å¯èƒ½ãªãƒ«ãƒ¼ãƒ«ã®ãƒªã‚¹ãƒˆï¼ˆå„ªå…ˆé †ä½é †ï¼‰
        """
        applicable = []
        
        for rule in self.rules:
            # ãƒ•ã‚§ãƒ¼ã‚ºãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
            if rule.phase > max_phase:
                continue
            
            # å„ªå…ˆé †ä½ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
            if priority_filter and rule.priority not in priority_filter:
                continue
            
            # ãƒˆãƒªã‚¬ãƒ¼ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ¡ä»¶ã‚’è©•ä¾¡
            if self.evaluate_trigger(rule, filepaths) and self.evaluate_target(rule, filepaths):
                applicable.append(rule)
                self.logger.debug(f"[Rule Evaluator] âœ… Rule applicable: {rule.name} (Phase {rule.phase}, {rule.priority})")
        
        # å„ªå…ˆé †ä½ã§ã‚½ãƒ¼ãƒˆï¼ˆHigh â†’ Medium â†’ Lowï¼‰
        priority_order = {'High': 0, 'Medium': 1, 'Low': 2}
        applicable.sort(key=lambda r: (r.phase, priority_order.get(r.priority, 3)))
        
        return applicable


class AppPatternDetector:
    """
    app_patterns.jsonã‚’æ´»ç”¨ã—ã¦ã‚¢ãƒ—ãƒªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºã—ã€
    ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¼·åŒ–ã‚’æä¾›ã™ã‚‹ã‚¯ãƒ©ã‚¹
    """
    
    def __init__(self, logger=None):
        self.logger = logger
        self.patterns = {}
        self._load_patterns()
    
    def _load_patterns(self):
        """app_patterns.jsonã‚’èª­ã¿è¾¼ã‚€"""
        try:
            # å„ªå…ˆé †ä½1: ~/.cognix/app_patterns.json
            patterns_path = Path.home() / '.cognix' / 'app_patterns.json'
            
            # å„ªå…ˆé †ä½2: ~/.cognix/patterns/app_patterns.json
            if not patterns_path.exists():
                patterns_path = Path.home() / '.cognix' / 'patterns' / 'app_patterns.json'
            
            if patterns_path.exists():
                with open(patterns_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.patterns = data.get('web_app_patterns', {})
                    if self.logger:
                        self.logger.debug(f"[AppPattern] Loaded {len(self.patterns)} patterns from {patterns_path}")
            else:
                if self.logger:
                    self.logger.debug(f"[AppPattern] No app_patterns.json found")
        except Exception as e:
            if self.logger:
                self.logger.warning(f"[AppPattern] Failed to load patterns: {e}")
    
    def detect_pattern(self, goal: str) -> Optional[str]:
        """
        goalã‹ã‚‰ã‚¢ãƒ—ãƒªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
        
        Args:
            goal: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å®Ÿè£…ç›®æ¨™
        
        Returns:
            æ¤œå‡ºã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³åã€ã¾ãŸã¯ None
        """
        if not self.patterns:
            return None
        
        goal_lower = goal.lower()
        
        # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒãƒƒãƒãƒ³ã‚°
        for pattern_name, config in self.patterns.items():
            trigger_config = config.get('trigger_conditions', {})
            triggers = trigger_config.get('triggers', {})
            
            # explicit_keywordsã§ãƒã‚§ãƒƒã‚¯ï¼ˆé«˜ä¿¡é ¼åº¦ï¼‰
            explicit = triggers.get('explicit_keywords', {})
            if explicit.get('enabled', False):
                for keyword in explicit.get('keywords', []):
                    if keyword.lower() in goal_lower:
                        if self.logger:
                            self.logger.debug(f"[AppPattern] Detected '{pattern_name}' via explicit keyword: {keyword}")
                        return pattern_name
            
            # implicit_keywordsã§ãƒã‚§ãƒƒã‚¯ï¼ˆä¸­ä¿¡é ¼åº¦ï¼‰
            implicit = triggers.get('implicit_keywords', {})
            if implicit.get('enabled', False):
                for keyword in implicit.get('keywords', []):
                    if keyword.lower() in goal_lower:
                        # ç«¶åˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
                        context_reqs = implicit.get('context_requirements', [])
                        if 'no_conflicting_keywords_like_cli_or_terminal' in context_reqs:
                            if 'cli' in goal_lower or 'terminal' in goal_lower or 'command line' in goal_lower:
                                continue
                        if self.logger:
                            self.logger.debug(f"[AppPattern] Detected '{pattern_name}' via implicit keyword: {keyword}")
                        return pattern_name
        
        return None
    
    def get_prompt_enhancement(self, pattern_name: str) -> str:
        """
        ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ããƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¼·åŒ–ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
        
        Args:
            pattern_name: ãƒ‘ã‚¿ãƒ¼ãƒ³å
        
        Returns:
            ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¼·åŒ–ãƒ†ã‚­ã‚¹ãƒˆ
        """
        if pattern_name not in self.patterns:
            return ""
        
        config = self.patterns[pattern_name]
        pe = config.get('prompt_enhancement', {})
        
        if not pe:
            return ""
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        prompt_parts = []
        
        # ãƒ™ãƒ¼ã‚¹èª¬æ˜
        base = pe.get('base', '')
        if base:
            prompt_parts.append(f"âš ï¸ APP PATTERN DETECTED: {pattern_name.upper()}")
            prompt_parts.append(base)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«è¦ä»¶
        files = pe.get('files', {})
        if files:
            prompt_parts.append("\nREQUIRED FILES:")
            for ftype, fconfig in files.items():
                filename = fconfig.get('filename', f'{pattern_name}.{ftype}')
                purpose = fconfig.get('purpose', '')
                requirements = fconfig.get('requirements', [])
                
                prompt_parts.append(f"\n{ftype.upper()} - {filename}:")
                prompt_parts.append(f"  Purpose: {purpose}")
                if requirements:
                    prompt_parts.append("  Requirements:")
                    for req in requirements[:5]:  # æœ€å¤§5ä»¶
                        prompt_parts.append(f"    - {req}")
        
        # å“è³ªè¦ä»¶
        qr = pe.get('quality_requirements', {})
        if qr:
            prompt_parts.append("\nQUALITY REQUIREMENTS:")
            for key, value in list(qr.items())[:3]:  # æœ€å¤§3ä»¶
                prompt_parts.append(f"  - {key}: {value[:100]}...")
        
        return "\n".join(prompt_parts)
    
    def get_required_files(self, pattern_name: str) -> List[str]:
        """
        ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—
        
        Args:
            pattern_name: ãƒ‘ã‚¿ãƒ¼ãƒ³å
        
        Returns:
            å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã®ãƒªã‚¹ãƒˆ ['html', 'css', 'js']
        """
        if pattern_name not in self.patterns:
            return []
        
        return self.patterns[pattern_name].get('files', [])


class SemiAutoEngine:
    """Semi-automatic code generation and modification engine"""
    
    def __init__(self, llm_manager=None, workspace_path: Optional[str] = None, 
                context=None, impact_analyzer=None, related_finder=None,
                diff_engine=None, repository_analyzer=None, config=None, **kwargs):  # â­ è¿½åŠ 
        self.llm_manager = llm_manager
        self.workspace_path = Path(workspace_path) if workspace_path else Path.cwd()
        self.context = context
        self.impact_analyzer = impact_analyzer
        self.related_finder = related_finder
        self.diff_engine = diff_engine
        self.repository_analyzer = repository_analyzer  # â­ è¿½åŠ 
        self.config = config
        self.extractor = StyleCodeExtractor()
        self.session_log = []
        self.temp_dir = None
        self.backup_dir = None
        self._complexity_assessor = ComplexityAssessor()
        # Note: PromptTemplateManager was removed as it was not being used
        # Prompts are handled directly in _get_code_generation_system_prompt()

        # PostGenerationValidator ã®åˆæœŸåŒ– (Layer 3)
        self.post_validator = PostGenerationValidator(
            llm_manager=self.llm_manager,
            repository_analyzer=self.repository_analyzer,
            logger=logger
        )

        # RequirementValidator ã®åˆæœŸåŒ–
        try:
            from cognix.requirement_validator import RequirementValidator
            self.requirement_validator = RequirementValidator()
        except ImportError:
            self.requirement_validator = None
            logger.debug(StatusIndicator.warning_rich("RequirementValidator not available (optional feature)"))
        except Exception as e:
            self.requirement_validator = None
            logger.debug(StatusIndicator.warning_rich(f"RequirementValidator initialization failed: {e}"))

        # DependencyChecker ã®åˆæœŸåŒ–
        try:
            from cognix.dependency_checker import DependencyChecker
            self.dependency_checker = DependencyChecker()
        except ImportError:
            self.dependency_checker = None
            logger.debug(StatusIndicator.warning_rich("DependencyChecker not available (optional feature)"))
        except Exception as e:
            self.dependency_checker = None
            logger.debug(StatusIndicator.warning_rich(f"DependencyChecker initialization failed: {e}"))

        # LinterIntegration ã®åˆæœŸåŒ–
        try:
            if LinterIntegration is not None:
                self.linter_integration = LinterIntegration()
            else:
                self.linter_integration = None
        except ImportError:
            self.linter_integration = None
            logger.debug(StatusIndicator.warning_rich("LinterIntegration not available (optional feature)"))
        except Exception as e:
            self.linter_integration = None
            logger.debug(StatusIndicator.warning_rich(f"LinterIntegration initialization failed: {e}"))

        self._init_workspace()
        
        # ============================================
        # å·®åˆ†ã‚¹ã‚­ãƒ£ãƒ³ç”¨Issueã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®åˆæœŸåŒ–
        # ============================================
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ§‹é€ : Dict[str, List[CachedIssue]]
        # ã‚­ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã€å€¤: ãã®ãƒ•ã‚¡ã‚¤ãƒ«ã®CachedIssueãƒªã‚¹ãƒˆ
        self._issue_cache: Optional[Dict[str, List[CachedIssue]]] = None
        
        # å·®åˆ†ã‚¹ã‚­ãƒ£ãƒ³æœ‰åŠ¹ãƒ•ãƒ©ã‚°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæœ‰åŠ¹ï¼‰
        self._differential_scan_enabled: bool = True
        
    def _init_workspace(self):
        """Initialize workspace and backup directories"""
        self.workspace_path.mkdir(parents=True, exist_ok=True)
        
        # Create backup directory
        self.backup_dir = self.workspace_path / ".cognix_backups"
        self.backup_dir.mkdir(exist_ok=True)
        
        # Create temp directory for staging
        self.temp_dir = self.workspace_path / ".cognix_temp"
        self.temp_dir.mkdir(exist_ok=True)

    def _assess_goal_complexity(self, goal: str) -> str:
        """
        è¤‡é›‘åº¦åˆ¤å®šãƒ¡ã‚½ãƒƒãƒ‰
        
        Args:
            goal: å®Ÿè£…ç›®æ¨™ã®èª¬æ˜
            
        Returns:
            "simple" | "medium" | "complex"
        """
        # è¤‡é›‘åº¦åˆ¤å®šå®Ÿè¡Œ
        complexity, details = self._complexity_assessor.assess_complexity(goal)
        
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±å‡ºåŠ›(é–‹ç™ºæ™‚ã®ã¿)
        if self.config and self.config.get('debug_complexity', False):
            logger.debug(f"\n[Complexity Assessment Debug]")
            logger.debug(f"  Goal: {goal}")
            logger.debug(f"  Complexity: {complexity}")
            logger.debug(f"  Score: {details['score']:.2f}")
            logger.debug(f"  Simple matches: {len(details['simple_matches'])}")
            logger.debug(f"  Medium matches: {len(details['medium_matches'])}")
            logger.debug(f"  Complex matches: {len(details['complex_matches_filtered'])}")
            if details['complex_matches_original'] != details['complex_matches_filtered']:
                excluded = set(details['complex_matches_original']) - set(details['complex_matches_filtered'])
                logger.debug(f"  Excluded (false positives): {excluded}")
        
        return complexity

    def _detect_target_file(self, goal: str) -> Optional[str]:
            """
            goalã‹ã‚‰å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¨æ¸¬
            
            Args:
                goal: å®Ÿè£…ç›®æ¨™ã®èª¬æ˜
                
            Returns:
                å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«å or None
            """
            import re
            
            # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³
            js_files = list(self.workspace_path.glob('*.js'))
            html_files = list(self.workspace_path.glob('*.html'))
            css_files = list(self.workspace_path.glob('*.css'))
            py_files = list(self.workspace_path.glob('*.py'))
            
            existing_files = js_files + html_files + css_files + py_files
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ãŒ1ã¤ã ã‘ãªã‚‰ã€ãã‚Œã‚’å¯¾è±¡ã¨ã™ã‚‹
            if len(existing_files) == 1:
                logger.debug(f"Single file detected: {existing_files[0].name}")
                return existing_files[0].name
            
            # goalã‹ã‚‰ç›´æ¥ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ¤œå‡º
            # âœ… è¤‡æ•°ã®ãƒ•ã‚¡ã‚¤ãƒ«åãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ None ã‚’è¿”ã™
            mentioned_files = []
            for file in existing_files:
                if file.name.lower() in goal.lower():
                    mentioned_files.append(file.name)
                    logger.debug(f"File mentioned in goal: {file.name}")

            # è¤‡æ•°ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ˜ç¤ºçš„ã«æŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ None ã‚’è¿”ã™
            # (LLM ã«åˆ¤æ–­ã‚’ä»»ã›ã‚‹)
            if len(mentioned_files) > 1:
                logger.debug(f"Multiple files mentioned in goal: {mentioned_files}. Letting LLM decide.")
                return None

            # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨
            if len(mentioned_files) == 1:
                logger.debug(f"Single file mentioned: {mentioned_files[0]}")
                return mentioned_files[0]
            
            # "Add to X" ã‚„ "Modify X" ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
            file_pattern = r'(?:add|modify|update|edit|change).*?(?:to|in)\s+([a-zA-Z0-9_\-\.]+\.[a-z]+)'
            match = re.search(file_pattern, goal, re.IGNORECASE)
            if match:
                filename = match.group(1)
                # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã«å­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
                for file in existing_files:
                    if file.name == filename:
                        logger.debug(f"File pattern matched: {filename}")
                        return filename
            
            # ä¸»è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’å„ªå…ˆ(script.js > index.html > styles.css)
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥ã®å„ªå…ˆé †ä½
            # JSãƒ•ã‚¡ã‚¤ãƒ«ã‚’æœ€å„ªå…ˆã€æ¬¡ã«HTMLã€æœ€å¾Œã«CSS
            js_priority = ['script.js', 'main.js', 'app.js', 'index.js']

            # å„ªå…ˆJSãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
            for priority in js_priority:
                for file in js_files:
                    if file.name == priority:
                        logger.debug(f"Using priority JS file: {priority}")
                        return priority

            # ä»»æ„ã®JSãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ
            if js_files:
                logger.debug(f"Defaulting to first JS file: {js_files[0].name}")
                return js_files[0].name

            # JSãŒãªã‘ã‚Œã°HTMLã‚’é¸æŠ
            if html_files:
                logger.debug(f"Defaulting to first HTML file: {html_files[0].name}")
                return html_files[0].name

            # HTMLãŒãªã‘ã‚Œã°CSSã‚’é¸æŠ
            if css_files:
                logger.debug(f"Defaulting to first CSS file: {css_files[0].name}")
                return css_files[0].name
            
            logger.debug("No target file detected")
            return None

    # ============================================
    # ğŸ†• LLM-based Project Structure Detection
    # ============================================
    
    def _determine_project_structure(self, goal: str) -> ProjectStructure:
        """
        LLMã§ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ã‚’åˆ¤å®š
        
        ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡ºã®é™ç•Œã‚’è¶…ãˆã€æ–‡è„ˆã‚’ç†è§£ã—ãŸåˆ¤å®šã‚’è¡Œã†ã€‚
        ä¾‹: "Web Audio API" ã‚’ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã§ã¯ãªããƒ–ãƒ©ã‚¦ã‚¶å†…è”µAPIã¨èªè­˜
        
        Args:
            goal: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å®Ÿè£…ç›®æ¨™
            
        Returns:
            ProjectStructure(stage, file_types, reason)
        """
        prompt = f'''# Project Structure Analysis

Analyze the requirement and determine the project structure.

## Response Format

Respond with ONLY valid JSON (no markdown, no explanation):
{{"stage": "single" or "multi", "file_types": [...], "reason": "..."}}

## Stage Definitions

### single
- Frontend-only applications (HTML/CSS/JS, React, Vue)
- Simple scripts or utilities
- No backend server process required
- Client-side storage only (localStorage, IndexedDB)

### multi
- Backend server required (Flask, Express, FastAPI, etc.)
- Server-side database (PostgreSQL, MySQL, MongoDB, etc.)
- REST API / GraphQL server
- Server-side authentication/sessions

## file_types Reference

| Type | Description |
|------|-------------|
| html | HTML files |
| css | CSS stylesheets |
| js | JavaScript files |
| jsx | React JSX files |
| ts | TypeScript files |
| tsx | React TypeScript files |
| vue | Vue.js files |
| python | Python files |
| node | Node.js backend |

## Critical Clarifications

- Web Audio API, Canvas API, Fetch API = Browser built-in (NOT backend)
- localStorage, IndexedDB = Client-side storage (NOT database)
- "Single HTML file" or "single file" = stage: single, file_types: ["html"]

## Examples

Pomodoro Timer (single HTML, uses Web Audio API):
{{"stage": "single", "file_types": ["html"], "reason": "Frontend-only. Web Audio API is browser built-in. Single file requested."}}

React Dashboard:
{{"stage": "single", "file_types": ["html", "jsx", "css"], "reason": "Frontend-only React app."}}

Flask REST API:
{{"stage": "multi", "file_types": ["python"], "reason": "Backend server with Flask."}}

Flask + React Fullstack:
{{"stage": "multi", "file_types": ["python", "html", "jsx", "css"], "reason": "Fullstack with Flask backend and React frontend."}}

## Requirement

{goal}
'''
        
        try:
            logger.debug("[Project Structure] Starting LLM-based detection...")
            
            response = self.llm_manager.generate_response(
                prompt=prompt,
                system_prompt="You are a project structure analyzer. Respond with ONLY valid JSON, no markdown code blocks.",
                max_tokens=300
            )
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
            if hasattr(response, 'content'):
                response_text = response.content
            else:
                response_text = str(response)
            
            logger.debug(f"[Project Structure] LLM response: {response_text[:200]}...")
            
            # JSONæŠ½å‡ºï¼ˆãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯é™¤å»ï¼‰
            cleaned = re.sub(r'```json\s*|\s*```', '', response_text.strip())
            # å…ˆé ­/æœ«å°¾ã®ç©ºç™½ã‚„æ”¹è¡Œã‚’é™¤å»
            cleaned = cleaned.strip()
            
            result = json.loads(cleaned)
            
            logger.debug(f"[Project Structure] Parsed result: {result}")
            
            # çµæœã‚’æ§‹ç¯‰
            structure = ProjectStructure(
                stage=result.get('stage', 'single'),
                file_types=result.get('file_types', ['html']),
                reason=result.get('reason', '')
            )
            
            logger.debug(f"[Project Structure] âœ… LLM detection successful: stage={structure.stage}, file_types={structure.file_types}")
            
            return structure
            
        except json.JSONDecodeError as e:
            logger.warning(f"[Project Structure] JSON parse error: {e}")
            logger.warning(f"[Project Structure] Raw response: {response_text if 'response_text' in locals() else 'N/A'}")
            logger.warning("[Project Structure] Falling back to keyword detection")
            return self._fallback_project_structure(goal)
            
        except Exception as e:
            logger.warning(f"[Project Structure] LLM detection failed: {e}")
            logger.warning("[Project Structure] Falling back to keyword detection")
            return self._fallback_project_structure(goal)
    
    def _fallback_project_structure(self, goal: str) -> ProjectStructure:
        """
        ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ—¢å­˜ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡ºã‹ã‚‰ProjectStructureã‚’ç”Ÿæˆ
        
        LLMåˆ¤å®šãŒå¤±æ•—ã—ãŸå ´åˆã«ä½¿ç”¨
        
        Args:
            goal: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å®Ÿè£…ç›®æ¨™
            
        Returns:
            ProjectStructure(stage, file_types, reason)
        """
        logger.debug("[Project Structure] Using fallback keyword detection...")
        
        detection = self._detect_multi_file_requirement(goal)
        
        file_types = detection.get('file_types', [])
        
        # fullstackåˆ¤å®šï¼ˆæ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
        file_types_set = set(file_types)
        backend_types = {'python', 'node'}
        frontend_types = {'html', 'jsx', 'vue', 'css', 'js', 'ts', 'tsx'}
        has_backend = bool(backend_types & file_types_set)
        has_frontend = bool(frontend_types & file_types_set)
        is_fullstack = has_backend and has_frontend
        
        stage = "multi" if is_fullstack else "single"
        
        structure = ProjectStructure(
            stage=stage,
            file_types=file_types,
            reason=f"Fallback: {detection.get('reason', 'keyword detection')}"
        )
        
        logger.debug(f"[Project Structure] Fallback result: stage={structure.stage}, file_types={structure.file_types}")
        
        return structure

    def execute_semi_auto_implementation(self, goal: str) -> SemiAutoResult:
        """
        Execute semi-automated implementation based on goal
        
        Args:
            goal: Implementation goal description
            
        Returns:
            SemiAutoResult with analysis, code, and recommendations
        """
        hud = None  # HUDåˆæœŸåŒ–ï¼ˆexceptãƒ–ãƒ­ãƒƒã‚¯ã§ä½¿ç”¨ï¼‰
        
        # â­ Zen HUDç”¨ã‚µãƒãƒªãƒ¼åˆæœŸåŒ–ï¼ˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¤‰æ•°ã¨ã—ã¦ä¿æŒï¼‰
        self._zen_summary = {
            "lint": {"initial": 0, "final": 0, "fixed": False},
            "review": {"initial": 0, "final": 0, "fixed": False, "issues": []}
        }
        
        try:
            # ==========================================
            # Phase 0: Repositoryåˆ†æï¼ˆHUDé–‹å§‹å‰ã«å®Ÿè¡Œï¼‰
            # ==========================================
            if self.repository_analyzer:
                repo_data_count = len(self.repository_analyzer.memory.repository_data)
                
                # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®äº‹å‰ãƒã‚§ãƒƒã‚¯
                from pathlib import Path
                project_root = Path.cwd()
                has_files = any(
                    file.is_file() and file.suffix in ['.py', '.js', '.html', '.css', '.ts', '.jsx', '.tsx', '.json', '.md']
                    for file in project_root.glob('*')
                )
                
                # ğŸ”§ ä¿®æ­£: å¸¸ã«å†åˆ†æï¼ˆãƒ•ã‚¡ã‚¤ãƒ«æ•°ãƒ»å†…å®¹å¤‰æ›´ã‚’ç¢ºå®Ÿã«æ¤œå‡ºï¼‰
                # æ—§ãƒ­ã‚¸ãƒƒã‚¯: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨ï¼ˆrepo_data_count > 0ã®å ´åˆï¼‰
                # æ–°ãƒ­ã‚¸ãƒƒã‚¯: æ¯å›åˆ†æå®Ÿè¡Œï¼ˆãƒ•ã‚¡ã‚¤ãƒ«å¢—åŠ ãƒ»é–¢æ•°è¿½åŠ ã‚’æ¤œå‡ºï¼‰
                if has_files:
                    logger.debug(f"[Repo] Starting analysis (previous cache: {repo_data_count} files)")
                    logger.debug(f"{Icon.GEAR.value} Analyzing project structure...")
                    try:
                        for progress_update in self.repository_analyzer.analyze_project_incrementally(max_files=100):
                            if progress_update.get('status'):
                                status = progress_update['status']
                                # æœ€å¾Œã®é€²æ—ã®ã¿è¡¨ç¤ºï¼ˆå†—é•·ã‚’é¿ã‘ã‚‹ï¼‰
                                if progress_update.get('progress', 0) == 1.0 or \
                                    progress_update.get('current_file') is None:
                                    logger.debug(f"   {status}")
                        
                        # ğŸ” ãƒ‡ãƒãƒƒã‚°: åˆ†æå®Œäº†å¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’ç¢ºèª
                        new_repo_data_count = len(self.repository_analyzer.memory.repository_data)
                        logger.debug(f"[Repo] Analysis completed: {new_repo_data_count} files (was {repo_data_count})")
                    except Exception as analysis_error:
                        logger.debug(f"âš   Repository analysis failed: {analysis_error}")
                        logger.warning(f"Repository analysis failed: {analysis_error}")
                else:
                    logger.debug("[Repo] No files to analyze")
            
            # Phase 1.1: Complexityè©•ä¾¡ã‚’æœ€åˆã«å®Ÿè¡Œï¼ˆå¤šæ®µéšç”Ÿæˆåˆ¤å®šã®ãŸã‚ï¼‰
            complexity = self._assess_goal_complexity(goal)
            
            # Phase 1.2: Goalåˆ¶ç´„æŠ½å‡ºï¼ˆğŸ†• Phase 1å®Ÿè£…ï¼‰
            goal_constraints = None
            if self.requirement_validator:
                try:
                    goal_constraints = self.requirement_validator.extract_goal_constraints(goal)
                    if goal_constraints.get("constraint_strength") in ["strict", "moderate"]:
                        logger.debug(f"[Goal Constraints] Detected: strength={goal_constraints.get('constraint_strength')}")
                        logger.debug(f"[Goal Constraints]   keep_keywords={goal_constraints.get('keep_keywords')}")
                        logger.debug(f"[Goal Constraints]   only_keywords={goal_constraints.get('only_keywords')}")
                        logger.debug(f"[Goal Constraints]   forbidden_actions={goal_constraints.get('forbidden_actions')}")
                except Exception as e:
                    logger.debug(f"[Goal Constraints] Extraction failed: {e}")
                    goal_constraints = None
            
            # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¤‰æ•°ã¨ã—ã¦ä¿å­˜ï¼ˆ_generate_code_implementationã§ä½¿ç”¨ï¼‰
            self._current_goal_constraints = goal_constraints
            
            # ğŸ†• Phase 1.3: ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯åˆ¶ç´„æ¤œå‡º
            self._no_framework_mode = False
            self._required_framework = None
            goal_lower = goal.lower()
            
            # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ç¦æ­¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            no_framework_keywords = [
                'pure html', 'vanilla javascript', 'vanilla js', 
                'no framework', 'no react', 'no vue', 'plain javascript',
                'html + css + js', 'html, css, js', 'html css js',
                'html/css/js', 'without framework', 'without react',
                'html + css + javascript', 'single html', 'no frameworks'
            ]
            for keyword in no_framework_keywords:
                if keyword in goal_lower:
                    self._no_framework_mode = True
                    logger.debug(f"[Framework Restriction] No-framework mode ENABLED (keyword: '{keyword}')")
                    # ğŸ†• PostGenerationValidatorã«ã‚‚ä¼æ’­
                    if hasattr(self, 'post_validator') and self.post_validator:
                        self.post_validator.no_framework_mode = True
                    break
            
            # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯æŒ‡å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆå„ªå…ˆåº¦ä½ï¼šç¦æ­¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒãªã„å ´åˆã®ã¿ï¼‰
            if not self._no_framework_mode:
                if 'react' in goal_lower and 'no react' not in goal_lower:
                    self._required_framework = 'react'
                elif 'vue' in goal_lower and 'no vue' not in goal_lower:
                    self._required_framework = 'vue'
                elif 'angular' in goal_lower:
                    self._required_framework = 'angular'
                
                if self._required_framework:
                    logger.debug(f"[Framework Restriction] Required framework: {self._required_framework}")
            
            # Phase 1.1: ğŸ†• LLMã§ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ã‚’åˆ¤å®šï¼ˆ1å›ã®ã¿ï¼‰
            self._project_structure = self._determine_project_structure(goal)
            logger.debug(f"[Project Structure] stage={self._project_structure.stage}, "
                         f"file_types={self._project_structure.file_types}, "
                         f"reason={self._project_structure.reason}")
            
            # ğŸ†• ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ç¦æ­¢ãƒ¢ãƒ¼ãƒ‰æ™‚ã®file_typesãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if self._no_framework_mode:
                # æ˜ç¤ºçš„ã«React/Vueã‚‚è¦æ±‚ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ§‹æˆã¨åˆ¤æ–­
                explicit_framework_request = (
                    ('react' in goal_lower and ('and react' in goal_lower or 'with react' in goal_lower or 'react for' in goal_lower)) or
                    ('vue' in goal_lower and ('and vue' in goal_lower or 'with vue' in goal_lower or 'vue for' in goal_lower))
                )
                
                if not explicit_framework_request:
                    original_types = self._project_structure.file_types
                    framework_types = ('jsx', 'tsx', 'vue')
                    filtered_types = [ft for ft in original_types if ft not in framework_types]
                    
                    if filtered_types != original_types:
                        removed = [ft for ft in original_types if ft in framework_types]
                        logger.debug(f"[Framework Restriction] Removed framework file types: {removed}")
                        logger.debug(f"[Framework Restriction] file_types: {original_types} -> {filtered_types}")
                        # ProjectStructureã‚’æ›´æ–°
                        self._project_structure = ProjectStructure(
                            stage=self._project_structure.stage,
                            file_types=filtered_types,
                            reason=self._project_structure.reason
                        )
                else:
                    logger.debug(f"[Framework Restriction] Hybrid mode detected - keeping framework file types")
            
            # ğŸ†• stageåˆ¤å®šï¼ˆLLMåˆ¤å®šçµæœã‚’ä½¿ç”¨ï¼‰
            needs_multi_stage = (self._project_structure.stage == "multi")
            
            if needs_multi_stage:
                logger.debug(f"[Multi-Stage] LLM determined multi-stage generation: {self._project_structure.reason}")
            
            if needs_multi_stage:
                # ==========================================
                # å¤šæ®µéšç”Ÿæˆãƒ•ãƒ­ãƒ¼ï¼ˆStepProgressãªã—ï¼‰
                # ==========================================
                logger.normal(f"{Icon.GEAR.value} Using multi-stage generation approach...")
                
                # å¤šæ®µéšç”Ÿæˆã‚’å®Ÿè¡Œ
                result = self._execute_multi_stage_generation(goal, complexity)
                
                if not result or not result[0]:
                    return SemiAutoResult(
                        success=False,
                        error="Failed to generate code with multi-stage approach"
                    )
                
                
                # è¿”ã‚Šå€¤ã‚’ã‚¢ãƒ³ãƒ‘ãƒƒã‚¯
                generated_code, lint_result, quality_scores = result
                
                # analysisã¨planã¯å¤šæ®µéšç”Ÿæˆã§ã¯ä½¿ç”¨ã•ã‚Œãªã„ãŸã‚ç©ºæ–‡å­—åˆ—
                analysis = "Multi-stage generation (analysis performed per phase)"
                plan = "Multi-stage generation (plan performed per phase)"
                
            else:
                # ==========================================
                # é€šå¸¸ç”Ÿæˆãƒ•ãƒ­ãƒ¼ï¼ˆStepProgressã‚ã‚Šï¼‰
                # ==========================================
                
                # ==========================================
                # é€šå¸¸ç”Ÿæˆãƒ•ãƒ­ãƒ¼ï¼ˆStepHUDä½¿ç”¨ - Zen HUDè¨­è¨ˆæº–æ‹ ï¼‰
                # ==========================================
                
                # Zen HUD: å›ºå®šè¡Œã‚¹ãƒ†ãƒƒãƒ—è¡¨ç¤ºï¼ˆè¨­è¨ˆæ›¸æº–æ‹ ï¼‰
                steps = [
                    "Assess goal complexity",
                    "Analyze implementation goal",
                    "Create implementation plan",
                    "Generate code implementation",
                    "Auto-completion",
                    "Lint check & Auto-fix",
                    "Quality assessment & Comprehensive Review",
                ]
                hud = StepHUD(section_title="Code Generation", steps=steps)
                hud.start()
                
                # Step 0: Complexity assessmentï¼ˆæ—¢ã«å®Œäº†ï¼‰
                hud.complete("Assess goal complexity")
                
                # Step 1: Analyze the goal
                hud.mark("Analyze implementation goal")
                analysis = self._analyze_implementation_goal(goal, complexity)
                
                if not analysis:
                    hud.finish(success=False)
                    return SemiAutoResult(
                        success=False,
                        error="Failed to analyze implementation goal"
                    )
                
                hud.complete("Analyze implementation goal")
                
                # Step 2: Generate implementation plan
                hud.mark("Create implementation plan")
                plan = self._create_implementation_plan(goal, analysis, complexity)
                hud.complete("Create implementation plan")

                # Step 3: Generate code
                hud.mark("Generate code implementation")
                
                # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡ºã—ã¦force_filenameã¨ã—ã¦æ¸¡ã™
                # âš  force_filenameã‚’æŒ‡å®šã™ã‚‹ã¨1ãƒ•ã‚¡ã‚¤ãƒ«ã—ã‹ç”Ÿæˆã§ããªã„ãŸã‚ã€ç„¡åŠ¹åŒ–
                # LLMãŒRepository Mapã‚’ä½¿ã£ã¦å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ¤æ–­ã™ã‚‹
                target_filename = None  # self._detect_target_file(goal)
                
                generated_code = self._generate_code_implementation(
                    goal, 
                    analysis, 
                    plan, 
                    complexity,
                    force_filename=None,  # å¸¸ã«Noneã‚’æ¸¡ã™
                    skip_multi_stage=True  # ğŸ†• ã‚·ãƒ³ã‚°ãƒ«ã‚¹ãƒ†ãƒ¼ã‚¸ã‹ã‚‰ã®å‘¼ã³å‡ºã—ã‚’æ˜ç¤º
                ) 
                
                if not generated_code:
                    hud.finish(success=False)
                    return SemiAutoResult(
                        success=False,
                        error="Failed to generate code implementation"
                    )
                
                hud.complete("Generate code implementation")
                # Step 4.1: Auto-completion (HUDçµ±åˆç‰ˆ)
                # â­ æ”¹å–„6: _auto_complete_with_import_resolutionã‚’å¸¸ã«å‘¼ã¶
                # ç†ç”±: requirement_validatorã¯goalãƒ™ãƒ¼ã‚¹ã®äºˆæ¸¬ã®ã¿ã§ã€
                # HTMLå†…ã®CSS/JSå‚ç…§ã‚’æ¤œè¨¼ã—ãªã„ã€‚_validate_import_dependencies
                # ï¼ˆæ”¹å–„2: _validate_html_resourcesã‚’å«ã‚€ï¼‰ã¯
                # _auto_complete_with_import_resolutionå†…ã§ã®ã¿å‘¼ã°ã‚Œã‚‹ãŸã‚ã€
                # missing_filesã®æœ‰ç„¡ã«é–¢ä¿‚ãªãå¸¸ã«å‘¼ã¶å¿…è¦ãŒã‚ã‚‹ã€‚
                
                validation_result = {}
                if self.requirement_validator:
                    validation_result = self.requirement_validator.validate_requirements(goal, generated_code)
                
                    if validation_result.get('missing_files'):
                        missing_count = len(validation_result['missing_files'])
                        logger.debug(f"[Auto-completion] Missing {missing_count} files detected (from requirement_validator)")
                
                # HUD: Auto-completioné–‹å§‹
                hud.mark("Auto-completion")
                
                # â­ æ”¹å–„6: å¸¸ã«_auto_complete_with_import_resolutionã‚’å‘¼ã¶
                # ã“ã‚Œã«ã‚ˆã‚Šã€_validate_import_dependenciesï¼ˆHTML/CSSæ¤œè¨¼å«ã‚€ï¼‰ãŒå®Ÿè¡Œã•ã‚Œã‚‹
                logger.debug(f"[Auto-completion] Starting _auto_complete_with_import_resolution...")
                generated_code = self._auto_complete_with_import_resolution(
                    goal=goal,
                    existing_files=generated_code,
                    validation_result=validation_result,
                    complexity=complexity
                )
                
                # è‡ªå‹•è£œå®Œå¾Œã®å†æ¤œè¨¼
                if self.requirement_validator:
                    revalidation_result = self.requirement_validator.validate_requirements(
                        goal,
                        generated_code
                    )
                
                    logger.debug(f"[Auto-completion] After auto-completion:")
                    logger.debug(f"[Auto-completion]   Fulfillment Score: {revalidation_result['fulfillment_score']:.2f}")
                    logger.debug(f"[Auto-completion]   Total files: {len(generated_code)}")
                
                    if revalidation_result['is_complete']:
                        logger.debug(f"[Auto-completion]   All requirements satisfied")
                    else:
                        remaining_missing = revalidation_result.get('missing_files', [])
                        if remaining_missing:
                            logger.debug(f"[Auto-completion]   Still missing: {remaining_missing}")
                
                # ==========================================
                # Step 4.3: Consistency Checks (G-1ã€œG-31)
                # ==========================================
                # Note: Auto-completion [DONE]ã¯G-ãƒã‚§ãƒƒã‚¯å®Œäº†å¾Œã«è¡¨ç¤º
                # G-ãƒã‚§ãƒƒã‚¯ã¯Auto-completionã®ä¸€éƒ¨ã¨ã—ã¦æ‰±ã„ã€HUDã«ã¯å†…éƒ¨å‡¦ç†ã¨ã—ã¦éè¡¨ç¤º
                
                # G-1: HTML-CSS ã‚¯ãƒ©ã‚¹æ•´åˆæ€§
                missing_css_classes = self._validate_html_css_class_consistency(generated_code)
                if missing_css_classes:
                    logger.debug(f"[G-1] Detected {len(missing_css_classes)} missing CSS classes")
                    logger.debug(f"\nâ— CSS consistency check: {len(missing_css_classes)} missing class(es)")
                    generated_code = self._auto_complete_missing_css_classes(missing_css_classes, generated_code)
                
                # G-2: HTML-JS IDæ•´åˆæ€§
                missing_html_ids = self._validate_html_js_id_consistency(generated_code)
                if missing_html_ids:
                    logger.debug(f"[G-2] Detected {len(missing_html_ids)} missing HTML IDs")
                    logger.debug(f"â— HTML-JS consistency check: {len(missing_html_ids)} missing ID(s)")
                    generated_code = self._auto_complete_missing_html_ids(missing_html_ids, generated_code)
                
                # G-3: Pythonç’°å¢ƒ-ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸äº’æ›æ€§
                compatibility_issues = self._validate_python_package_compatibility(generated_code)
                if compatibility_issues:
                    logger.debug(f"[G-3] Detected {len(compatibility_issues)} compatibility issues")
                    logger.debug(f"â— Package compatibility check: {len(compatibility_issues)} issue(s)")
                    for issue in compatibility_issues:
                        logger.debug(f"  - {issue['package']}=={issue['current_version']} â†’ {issue['fix']}")
                    generated_code = self._auto_fix_package_compatibility(compatibility_issues, generated_code)
                
                # G-4: Pythonã‚·ãƒ³ãƒœãƒ«ãƒ¬ãƒ™ãƒ«importæ•´åˆæ€§
                symbol_issues = self._validate_python_symbol_imports(generated_code)
                if symbol_issues:
                    logger.debug(f"[G-4] Detected {len(symbol_issues)} symbol import issues")
                    logger.debug(f"â— Symbol import check: {len(symbol_issues)} issue(s)")
                    for issue in symbol_issues[:5]:
                        logger.debug(f"  - {issue['importer']}: '{issue['symbol']}' not in {issue['module']}")
                    generated_code = self._auto_fix_symbol_imports(symbol_issues, generated_code)
                
                # G-5: Import vs Requirements.txt æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
                requirements_issues = self._validate_import_requirements_consistency(generated_code)
                if requirements_issues:
                    logger.debug(f"[G-5] Detected {len(requirements_issues)} missing packages in requirements.txt")
                    logger.debug(f"â— Requirements.txt check: {len(requirements_issues)} missing package(s)")
                    for pkg in requirements_issues[:5]:
                        logger.debug(f"  - {pkg['import_name']} â†’ {pkg['package_name']}")
                    generated_code = self._auto_fix_requirements_txt(requirements_issues, generated_code)
                
                # G-6: ãƒ©ã‚¤ãƒ–ãƒ©ãƒªè¦æ±‚ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
                library_issues = self._validate_library_requirements(generated_code)
                if library_issues:
                    logger.debug(f"[G-6] Detected {len(library_issues)} library requirement issues")
                    logger.debug(f"â— Library requirements check: {len(library_issues)} issue(s)")
                    for issue in library_issues[:5]:
                        logger.debug(f"  - [{issue['library']}] {issue['requirement']}")
                    generated_code = self._auto_fix_library_requirements(library_issues, generated_code)
                
                # G-7: ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ä¾å­˜æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
                frontend_issues = self._validate_frontend_dependencies(generated_code)
                if frontend_issues:
                    logger.debug(f"[G-7] Detected {len(frontend_issues)} frontend dependency issues")
                    logger.debug(f"â— Frontend dependencies check: {len(frontend_issues)} issue(s)")
                    for issue in frontend_issues[:5]:
                        logger.debug(f"  - [{issue['type']}] {issue['dependency']}")
                    generated_code = self._auto_fix_frontend_dependencies(frontend_issues, generated_code)
                
                # G-8: Modelâ†”Schemaæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
                schema_issues = self._validate_model_schema_consistency(generated_code)
                if schema_issues:
                    logger.debug(f"[G-8] Detected {len(schema_issues)} model-schema consistency issues")
                    logger.debug(f"â— Model-Schema consistency check: {len(schema_issues)} issue(s)")
                    for issue in schema_issues[:5]:
                        logger.debug(f"  - {issue['schema']}.{issue['field']} not in {issue['model']}")
                    generated_code = self._auto_fix_model_schema_consistency(schema_issues, generated_code)
                
                # G-9: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç’°å¢ƒä¾å­˜ãƒã‚§ãƒƒã‚¯
                doc_issues = self._validate_document_environment_commands(generated_code)
                if doc_issues:
                    logger.debug(f"[G-9] Detected {len(doc_issues)} environment-dependent commands")
                    logger.debug(f"â— Document environment check: {len(doc_issues)} issue(s)")
                    for issue in doc_issues[:5]:
                        logger.debug(f"  - {issue['file']}:{issue['line']} - {issue['type']}")
                    generated_code = self._auto_fix_document_environment_commands(doc_issues, generated_code)
                
                # G-10: ã‚¯ãƒ©ã‚¹å®šç¾©å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
                MAX_G10_FIX_ATTEMPTS = 2
                class_safety_issues = []
                for g10_attempt in range(MAX_G10_FIX_ATTEMPTS):
                    class_safety_issues = self._validate_class_definition_safety(generated_code)
                    if not class_safety_issues:
                        if g10_attempt > 0:
                            logger.debug(f"[G-10] âœ… All issues fixed after {g10_attempt} attempt(s)")
                        break
                    
                    logger.debug(f"[G-10] Detected {len(class_safety_issues)} issues (attempt {g10_attempt + 1}/{MAX_G10_FIX_ATTEMPTS})")
                    if g10_attempt == 0:
                        logger.debug(f"â— Class definition safety check: {len(class_safety_issues)} issue(s)")
                        for issue in class_safety_issues[:5]:
                            logger.debug(f"  - {issue['file']}:{issue['line']} - {issue['type']}")
                    
                    generated_code = self._auto_fix_class_definition_safety(class_safety_issues, generated_code)
                
                if class_safety_issues:
                    logger.warning(f"[G-10] âš  {len(class_safety_issues)} issues remain after {MAX_G10_FIX_ATTEMPTS} fix attempts")
                
                # G-11: ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯äºˆç´„èªãƒã‚§ãƒƒã‚¯ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
                MAX_G11_FIX_ATTEMPTS = 2
                framework_reserved_issues = []
                for g11_attempt in range(MAX_G11_FIX_ATTEMPTS):
                    framework_reserved_issues = self._validate_framework_reserved_words(generated_code)
                    if not framework_reserved_issues:
                        if g11_attempt > 0:
                            logger.debug(f"[G-11] âœ… All issues fixed after {g11_attempt} attempt(s)")
                        break
                    
                    logger.debug(f"[G-11] Detected {len(framework_reserved_issues)} issues (attempt {g11_attempt + 1}/{MAX_G11_FIX_ATTEMPTS})")
                    if g11_attempt == 0:
                        logger.debug(f"â— Framework reserved word check: {len(framework_reserved_issues)} issue(s)")
                        for issue in framework_reserved_issues[:5]:
                            logger.debug(f"  - {issue['file']}:{issue['line']} - {issue['reserved_word']}")
                    
                    generated_code = self._auto_fix_framework_reserved_words(framework_reserved_issues, generated_code)
                
                if framework_reserved_issues:
                    logger.warning(f"[G-11] âš  {len(framework_reserved_issues)} issues remain after {MAX_G11_FIX_ATTEMPTS} fix attempts")
                
                # G-12: Pythonçµ„ã¿è¾¼ã¿åã‚·ãƒ£ãƒ‰ã‚¦ã‚¤ãƒ³ã‚°ãƒã‚§ãƒƒã‚¯ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
                MAX_G12_FIX_ATTEMPTS = 2
                builtin_shadowing_issues = []
                for g12_attempt in range(MAX_G12_FIX_ATTEMPTS):
                    builtin_shadowing_issues = self._validate_python_builtin_shadowing(generated_code)
                    if not builtin_shadowing_issues:
                        if g12_attempt > 0:
                            logger.debug(f"[G-12] âœ… All issues fixed after {g12_attempt} attempt(s)")
                        break
                    
                    logger.debug(f"[G-12] Detected {len(builtin_shadowing_issues)} issues (attempt {g12_attempt + 1}/{MAX_G12_FIX_ATTEMPTS})")
                    if g12_attempt == 0:
                        logger.debug(f"â— Python builtin shadowing check: {len(builtin_shadowing_issues)} issue(s)")
                        for issue in builtin_shadowing_issues[:5]:
                            logger.debug(f"  - {issue['file']}:{issue['line']} - {issue['builtin_name']}")
                    
                    generated_code = self._auto_fix_python_builtin_shadowing(builtin_shadowing_issues, generated_code)
                
                if builtin_shadowing_issues:
                    logger.warning(f"[G-12] âš  {len(builtin_shadowing_issues)} issues remain after {MAX_G12_FIX_ATTEMPTS} fix attempts")
                
                # G-13: Flask Blueprintãƒ«ãƒ¼ãƒˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
                MAX_G13_FIX_ATTEMPTS = 2
                route_duplicate_issues = []
                for g13_attempt in range(MAX_G13_FIX_ATTEMPTS):
                    route_duplicate_issues = self._validate_flask_route_duplicates(generated_code)
                    if not route_duplicate_issues:
                        if g13_attempt > 0:
                            logger.debug(f"[G-13] âœ… All issues fixed after {g13_attempt} attempt(s)")
                        break
                    
                    logger.debug(f"[G-13] Detected {len(route_duplicate_issues)} issues (attempt {g13_attempt + 1}/{MAX_G13_FIX_ATTEMPTS})")
                    if g13_attempt == 0:
                        logger.debug(f"â— Flask route duplicate check: {len(route_duplicate_issues)} issue(s)")
                        for issue in route_duplicate_issues[:5]:
                            logger.debug(f"  - {issue['file']}:{issue['line']} - {issue['method']} {issue['route']}")
                    
                    generated_code = self._auto_fix_flask_route_duplicates(route_duplicate_issues, generated_code)
                
                if route_duplicate_issues:
                    logger.warning(f"[G-13] âš  {len(route_duplicate_issues)} issues remain after {MAX_G13_FIX_ATTEMPTS} fix attempts")
                
                # G-14: SQLAlchemyãƒ†ãƒ¼ãƒ–ãƒ«åé‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
                MAX_G14_FIX_ATTEMPTS = 2
                table_duplicate_issues = []
                for g14_attempt in range(MAX_G14_FIX_ATTEMPTS):
                    table_duplicate_issues = self._validate_sqlalchemy_table_duplicates(generated_code)
                    if not table_duplicate_issues:
                        if g14_attempt > 0:
                            logger.debug(f"[G-14] âœ… All issues fixed after {g14_attempt} attempt(s)")
                        break
                    
                    logger.debug(f"[G-14] Detected {len(table_duplicate_issues)} issues (attempt {g14_attempt + 1}/{MAX_G14_FIX_ATTEMPTS})")
                    if g14_attempt == 0:
                        logger.debug(f"â— SQLAlchemy table duplicate check: {len(table_duplicate_issues)} issue(s)")
                        for issue in table_duplicate_issues[:5]:
                            logger.debug(f"  - {issue['file']}:{issue['line']} - table '{issue['table']}'")
                    
                    generated_code = self._auto_fix_sqlalchemy_table_duplicates(table_duplicate_issues, generated_code)
                
                if table_duplicate_issues:
                    logger.warning(f"[G-14] âš  {len(table_duplicate_issues)} issues remain after {MAX_G14_FIX_ATTEMPTS} fix attempts")
                
                # G-15: JavaScriptäºˆç´„èªãƒã‚§ãƒƒã‚¯ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
                MAX_G15_FIX_ATTEMPTS = 2
                js_reserved_issues = []
                for g15_attempt in range(MAX_G15_FIX_ATTEMPTS):
                    js_reserved_issues = self._validate_javascript_reserved_words(generated_code)
                    if not js_reserved_issues:
                        if g15_attempt > 0:
                            logger.debug(f"[G-15] âœ… All issues fixed after {g15_attempt} attempt(s)")
                        break
                    
                    logger.debug(f"[G-15] Detected {len(js_reserved_issues)} issues (attempt {g15_attempt + 1}/{MAX_G15_FIX_ATTEMPTS})")
                    if g15_attempt == 0:
                        logger.debug(f"â— JavaScript reserved word check: {len(js_reserved_issues)} issue(s)")
                        for issue in js_reserved_issues[:5]:
                            logger.debug(f"  - {issue['file']}:{issue['line']} - {issue['reserved_word']}")
                    
                    generated_code = self._auto_fix_javascript_reserved_words(js_reserved_issues, generated_code)
                
                if js_reserved_issues:
                    logger.warning(f"[G-15] âš  {len(js_reserved_issues)} issues remain after {MAX_G15_FIX_ATTEMPTS} fix attempts")
                
                # G-16: SQLAlchemy relationshipå‚ç…§å…ˆæ¤œè¨¼ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
                MAX_G16_FIX_ATTEMPTS = 2
                relationship_issues = []
                for g16_attempt in range(MAX_G16_FIX_ATTEMPTS):
                    relationship_issues = self._validate_relationship_references(generated_code)
                    if not relationship_issues:
                        if g16_attempt > 0:
                            logger.debug(f"[G-16] âœ… All issues fixed after {g16_attempt} attempt(s)")
                        break
                    
                    logger.debug(f"[G-16] Detected {len(relationship_issues)} issues (attempt {g16_attempt + 1}/{MAX_G16_FIX_ATTEMPTS})")
                    if g16_attempt == 0:
                        logger.debug(f"â— Relationship reference check: {len(relationship_issues)} issue(s)")
                        for issue in relationship_issues[:5]:
                            logger.debug(f"  - {issue['file']}:{issue['line']} - {issue['type']}")
                    
                    generated_code = self._auto_fix_relationship_references(relationship_issues, generated_code)
                
                if relationship_issues:
                    logger.warning(f"[G-16] âš  {len(relationship_issues)} issues remain after {MAX_G16_FIX_ATTEMPTS} fix attempts")
                
                # G-17: ForeignKeyå‚ç…§å…ˆæ¤œè¨¼ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
                MAX_G17_FIX_ATTEMPTS = 2
                foreignkey_issues = []
                for g17_attempt in range(MAX_G17_FIX_ATTEMPTS):
                    foreignkey_issues = self._validate_foreignkey_references(generated_code)
                    if not foreignkey_issues:
                        if g17_attempt > 0:
                            logger.debug(f"[G-17] âœ… All issues fixed after {g17_attempt} attempt(s)")
                        break
                    
                    logger.debug(f"[G-17] Detected {len(foreignkey_issues)} issues (attempt {g17_attempt + 1}/{MAX_G17_FIX_ATTEMPTS})")
                    if g17_attempt == 0:
                        logger.debug(f"â— ForeignKey reference check: {len(foreignkey_issues)} issue(s)")
                        for issue in foreignkey_issues[:5]:
                            logger.debug(f"  - {issue['file']}:{issue['line']} - {issue['referenced_table']}")
                    
                    generated_code = self._auto_fix_foreignkey_references(foreignkey_issues, generated_code)
                
                if foreignkey_issues:
                    logger.warning(f"[G-17] âš  {len(foreignkey_issues)} issues remain after {MAX_G17_FIX_ATTEMPTS} fix attempts")
                
                # G-18: SQLAlchemy Enumå®šç¾©æ¤œè¨¼ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
                MAX_G18_FIX_ATTEMPTS = 2
                enum_usage_issues = []
                for g18_attempt in range(MAX_G18_FIX_ATTEMPTS):
                    enum_usage_issues = self._validate_sqlalchemy_enum_usage(generated_code)
                    if not enum_usage_issues:
                        if g18_attempt > 0:
                            logger.debug(f"[G-18] âœ… All issues fixed after {g18_attempt} attempt(s)")
                        break
                    
                    logger.debug(f"[G-18] Detected {len(enum_usage_issues)} issues (attempt {g18_attempt + 1}/{MAX_G18_FIX_ATTEMPTS})")
                    if g18_attempt == 0:
                        logger.debug(f"â— Enum usage check: {len(enum_usage_issues)} issue(s)")
                        for issue in enum_usage_issues[:5]:
                            logger.debug(f"  - {issue['file']}:{issue['line']} - {issue['referenced_class']}")
                    
                    generated_code = self._auto_fix_sqlalchemy_enum_usage(enum_usage_issues, generated_code)
                
                if enum_usage_issues:
                    logger.warning(f"[G-18] âš  {len(enum_usage_issues)} issues remain after {MAX_G18_FIX_ATTEMPTS} fix attempts")
                
                # G-19: Mappedå‹ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ•´åˆæ€§æ¤œè¨¼ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
                MAX_G19_FIX_ATTEMPTS = 2
                mapped_type_issues = []
                for g19_attempt in range(MAX_G19_FIX_ATTEMPTS):
                    mapped_type_issues = self._validate_mapped_type_consistency(generated_code)
                    if not mapped_type_issues:
                        if g19_attempt > 0:
                            logger.debug(f"[G-19] âœ… All issues fixed after {g19_attempt} attempt(s)")
                        break
                    
                    logger.debug(f"[G-19] Detected {len(mapped_type_issues)} issues (attempt {g19_attempt + 1}/{MAX_G19_FIX_ATTEMPTS})")
                    if g19_attempt == 0:
                        logger.debug(f"â— Mapped type consistency check: {len(mapped_type_issues)} issue(s)")
                        for issue in mapped_type_issues[:5]:
                            logger.debug(f"  - {issue['file']}:{issue['line']} - {issue['attribute']}")
                    
                    generated_code = self._auto_fix_mapped_type_consistency(mapped_type_issues, generated_code)
                
                if mapped_type_issues:
                    logger.warning(f"[G-19] âš  {len(mapped_type_issues)} issues remain after {MAX_G19_FIX_ATTEMPTS} fix attempts")
                
                # G-20: Flaské™çš„ãƒ‘ã‚¹æ¤œè¨¼ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
                MAX_G20_FIX_ATTEMPTS = 2
                flask_static_issues = []
                for g20_attempt in range(MAX_G20_FIX_ATTEMPTS):
                    flask_static_issues = self._validate_flask_static_paths(generated_code)
                    if not flask_static_issues:
                        if g20_attempt > 0:
                            logger.debug(f"[G-20] âœ… All issues fixed after {g20_attempt} attempt(s)")
                        break
                    
                    logger.debug(f"[G-20] Detected {len(flask_static_issues)} issues (attempt {g20_attempt + 1}/{MAX_G20_FIX_ATTEMPTS})")
                    if g20_attempt == 0:
                        logger.debug(f"â— Flask static path check: {len(flask_static_issues)} issue(s)")
                        for issue in flask_static_issues[:5]:
                            logger.debug(f"  - {issue['file']}:{issue['line']} - {issue['current_path']} â†’ {issue['suggested_path']}")
                    
                    generated_code = self._auto_fix_flask_static_paths(flask_static_issues, generated_code)
                
                if flask_static_issues:
                    logger.warning(f"[G-20] âš  {len(flask_static_issues)} issues remain after {MAX_G20_FIX_ATTEMPTS} fix attempts")
                
                # G-21: HTMLå‚ç…§æ•´åˆæ€§æ¤œè¨¼ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
                MAX_G21_FIX_ATTEMPTS = 2
                html_asset_issues = []
                for g21_attempt in range(MAX_G21_FIX_ATTEMPTS):
                    html_asset_issues = self._validate_html_asset_references(generated_code)
                    if not html_asset_issues:
                        if g21_attempt > 0:
                            logger.debug(f"[G-21] âœ… All issues fixed after {g21_attempt} attempt(s)")
                        break
                    
                    logger.debug(f"[G-21] Detected {len(html_asset_issues)} issues (attempt {g21_attempt + 1}/{MAX_G21_FIX_ATTEMPTS})")
                    if g21_attempt == 0:
                        logger.debug(f"â— HTML asset reference check: {len(html_asset_issues)} issue(s)")
                        for issue in html_asset_issues[:5]:
                            logger.debug(f"  - {issue['html_file']}:{issue['line']} - {issue['ref_type']}: {issue['ref_path']}")
                    
                    generated_code = self._auto_fix_html_asset_references(html_asset_issues, generated_code)
                
                if html_asset_issues:
                    logger.warning(f"[G-21] âš  {len(html_asset_issues)} issues remain after {MAX_G21_FIX_ATTEMPTS} fix attempts")
                
                # G-22: JavaScriptã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ—ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œè¨¼ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
                MAX_G22_FIX_ATTEMPTS = 2
                js_animation_issues = []
                for g22_attempt in range(MAX_G22_FIX_ATTEMPTS):
                    js_animation_issues = self._validate_js_animation_loop(generated_code)
                    if not js_animation_issues:
                        if g22_attempt > 0:
                            logger.debug(f"[G-22] âœ… All issues fixed after {g22_attempt} attempt(s)")
                        break
                    
                    logger.debug(f"[G-22] Detected {len(js_animation_issues)} issues (attempt {g22_attempt + 1}/{MAX_G22_FIX_ATTEMPTS})")
                    if g22_attempt == 0:
                        logger.debug(f"â— JavaScript animation loop check: {len(js_animation_issues)} issue(s)")
                        for issue in js_animation_issues[:5]:
                            logger.debug(f"  - {issue['file']}:{issue['line']} - {issue['method_name']}() called without timestamp")
                    
                    generated_code = self._auto_fix_js_animation_loop(js_animation_issues, generated_code)
                
                if js_animation_issues:
                    logger.warning(f"[G-22] âš  {len(js_animation_issues)} issues remain after {MAX_G22_FIX_ATTEMPTS} fix attempts")
                
                # G-23: APIå¥‘ç´„æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ï¼ˆè­¦å‘Šã®ã¿ã€è‡ªå‹•ä¿®æ­£ãªã—ï¼‰
                api_contract_issues = self._validate_api_contract_consistency(generated_code)
                if api_contract_issues:
                    logger.debug(f"â— API contract consistency: {len(api_contract_issues)} warning(s)")
                    for issue in api_contract_issues[:3]:
                        logger.debug(f"  - {issue['details']}")
                    # è­¦å‘Šã®ã¿ãªã®ã§ã€è‡ªå‹•ä¿®æ­£ã¯å‘¼ã³å‡ºã•ãªã„
                
                # G-24: back_populatesç›¸äº’å‚ç…§ãƒã‚§ãƒƒã‚¯
                MAX_G24_FIX_ATTEMPTS = 2
                back_populates_issues = []
                for g24_attempt in range(MAX_G24_FIX_ATTEMPTS):
                    back_populates_issues = self._validate_back_populates_consistency(generated_code)
                    if not back_populates_issues:
                        if g24_attempt > 0:
                            logger.debug(f"[G-24] âœ… All issues fixed after {g24_attempt} attempt(s)")
                        break
                    logger.debug(f"[G-24] Detected {len(back_populates_issues)} issues (attempt {g24_attempt + 1}/{MAX_G24_FIX_ATTEMPTS})")
                    generated_code = self._auto_fix_back_populates_consistency(back_populates_issues, generated_code)
                
                if back_populates_issues:
                    logger.debug(f"â— back_populates consistency: {len(back_populates_issues)} error(s)")
                    for issue in back_populates_issues[:3]:
                        logger.debug(f"  - {issue['details']}")
                
                # G-25: ForeignKeyå‹æ³¨é‡ˆæ¬ å¦‚ãƒã‚§ãƒƒã‚¯
                foreignkey_annotation_issues = self._validate_foreignkey_type_annotation(generated_code)
                if foreignkey_annotation_issues:
                    logger.debug(f"[G-25] Detected {len(foreignkey_annotation_issues)} issues, auto-fixing...")
                    generated_code = self._auto_fix_foreignkey_type_annotation(foreignkey_annotation_issues, generated_code)
                    foreignkey_annotation_issues = self._validate_foreignkey_type_annotation(generated_code)
                
                if foreignkey_annotation_issues:
                    logger.debug(f"â— ForeignKey type annotations: {len(foreignkey_annotation_issues)} warning(s)")
                
                # G-26: Mappedå‹ã¨SQLAlchemyå‹ã®ä¸ä¸€è‡´ãƒã‚§ãƒƒã‚¯
                mapped_mismatch_issues = self._validate_mapped_sqlalchemy_type_mismatch(generated_code)
                if mapped_mismatch_issues:
                    logger.debug(f"[G-26] Detected {len(mapped_mismatch_issues)} issues, auto-fixing...")
                    generated_code = self._auto_fix_mapped_sqlalchemy_type_mismatch(mapped_mismatch_issues, generated_code)
                    mapped_mismatch_issues = self._validate_mapped_sqlalchemy_type_mismatch(generated_code)
                
                if mapped_mismatch_issues:
                    logger.debug(f"â— Mapped/SQLAlchemy type mismatch: {len(mapped_mismatch_issues)} warning(s)")
                
                # G-27: relationship overlapsæ¤œå‡º
                relationship_overlap_issues = self._validate_relationship_overlaps(generated_code)
                if relationship_overlap_issues:
                    generated_code = self._auto_fix_relationship_overlaps(relationship_overlap_issues, generated_code)
                    logger.debug(f"â— Relationship overlaps: {len(relationship_overlap_issues)} warning(s)")
                
                # G-28: å¾ªç’°ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ¤œå‡º
                circular_import_issues = self._validate_circular_imports(generated_code)
                if circular_import_issues:
                    generated_code = self._auto_fix_circular_imports(circular_import_issues, generated_code)
                    logger.debug(f"â— Circular imports: {len(circular_import_issues)} error(s)")
                    for issue in circular_import_issues[:2]:
                        logger.debug(f"  - {issue['details']}")
                
                # G-29: Marshmallow Nestedå¾ªç’°å‚ç…§æ¤œå‡º
                marshmallow_circular_issues = self._validate_marshmallow_nested_circular(generated_code)
                if marshmallow_circular_issues:
                    generated_code = self._auto_fix_marshmallow_nested_circular(marshmallow_circular_issues, generated_code)
                    logger.debug(f"â— Marshmallow Nested circular: {len(marshmallow_circular_issues)} warning(s)")
                
                # G-30: JavaScript thiså‚ç…§å–ªå¤±ãƒã‚§ãƒƒã‚¯
                js_this_loss_issues = self._validate_js_this_context_loss(generated_code)
                if js_this_loss_issues:
                    logger.debug(f"[G-30] Detected {len(js_this_loss_issues)} issues, auto-fixing...")
                    generated_code = self._auto_fix_js_this_context_loss(js_this_loss_issues, generated_code)
                    js_this_loss_issues = self._validate_js_this_context_loss(generated_code)
                
                if js_this_loss_issues:
                    logger.debug(f"â— JavaScript this context loss: {len(js_this_loss_issues)} error(s)")
                    for issue in js_this_loss_issues[:3]:
                        logger.debug(f"  - {issue['details']}")
                
                # G-31: Marshmallow APIèª¤ç”¨æ¤œå‡ºï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
                MAX_G31_FIX_ATTEMPTS = 2
                marshmallow_api_issues = []
                for g31_attempt in range(MAX_G31_FIX_ATTEMPTS):
                    marshmallow_api_issues = self._validate_marshmallow_api_usage(generated_code)
                    if not marshmallow_api_issues:
                        if g31_attempt > 0:
                            logger.debug(f"[G-31] âœ… All issues fixed after {g31_attempt} attempt(s)")
                        break
                    
                    logger.debug(f"[G-31] Detected {len(marshmallow_api_issues)} issues (attempt {g31_attempt + 1}/{MAX_G31_FIX_ATTEMPTS})")
                    if g31_attempt == 0:
                        logger.debug(f"â— Marshmallow API usage check: {len(marshmallow_api_issues)} issue(s)")
                        for issue in marshmallow_api_issues[:5]:
                            logger.debug(f"  - {issue['file']}:{issue['line']} - {issue['field']}")
                    
                    generated_code = self._auto_fix_marshmallow_api_usage(marshmallow_api_issues, generated_code)
                
                if marshmallow_api_issues:
                    logger.warning(f"[G-31] âš  {len(marshmallow_api_issues)} issues remain after {MAX_G31_FIX_ATTEMPTS} fix attempts")
                
                if not missing_css_classes and not missing_html_ids and not compatibility_issues and not symbol_issues and not requirements_issues and not library_issues and not frontend_issues and not schema_issues and not doc_issues and not class_safety_issues and not framework_reserved_issues and not builtin_shadowing_issues and not route_duplicate_issues and not table_duplicate_issues and not js_reserved_issues and not relationship_issues and not foreignkey_issues and not enum_usage_issues and not mapped_type_issues and not flask_static_issues and not html_asset_issues and not js_animation_issues and not back_populates_issues and not circular_import_issues and not js_this_loss_issues and not marshmallow_api_issues:
                    logger.debug("[G-1/.../G-31] All consistency checks passed")
                
                # HUD: Auto-completionå®Œäº†ï¼ˆG-1ã€œG-31å®Œäº†å¾Œï¼‰
                hud.complete("Auto-completion")

                # ==========================================
                # Step 4.5: HTMLå‚ç…§ä¿®æ­£ï¼ˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ç¦æ­¢ãƒ¢ãƒ¼ãƒ‰ï¼‰
                # ==========================================
                if getattr(self, '_no_framework_mode', False):
                    logger.debug("[G-32] Checking HTML script references for vanilla JS...")
                    generated_code = self._fix_html_script_reference_for_vanilla_js(generated_code)

                # ==========================================
                # Step 4.6: Runtime Import Testï¼ˆå¯¾ç­–Cï¼‰
                # ==========================================
                # ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã‚’å®Ÿéš›ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦å®Ÿè¡Œæ™‚ã‚¨ãƒ©ãƒ¼ã‚’æ¤œå‡º
                # é™çš„è§£æã§ã¯æ¤œå‡ºã§ããªã„ã‚¯ãƒ©ã‚¹å®šç¾©æ™‚ã®ã‚¨ãƒ©ãƒ¼ãªã©ã‚’äº‹å‰ã«ç™ºè¦‹
                logger.debug("[Runtime Import] Starting import test...")
                
                import_success, import_error, failing_file = self._validate_runtime_import(generated_code)
                
                # â­ å¯¾ç­–A: å¤–éƒ¨ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ä¸è¶³ã®å ´åˆã¯æƒ…å ±è¡¨ç¤ºã®ã¿ï¼ˆLLMä¿®æ­£ã¯ä¸è¦ï¼‰
                if import_success and import_error and import_error.startswith("EXTERNAL_PACKAGE:"):
                    package_name = import_error.replace("EXTERNAL_PACKAGE:", "")
                    logger.debug(f"\nâ— Runtime import test skipped:")
                    logger.debug(f"   â„¹ Requires external package: {package_name}")
                    logger.debug(f"   â†’ Run: pip install -r requirements.txt")
                    logger.debug(f"[Runtime Import] Skipped due to external package: {package_name}")
                elif not import_success:
                    logger.debug(f"\nâš  Runtime import error detected:")
                    logger.debug(f"   File: {failing_file or 'Unknown'}")
                    logger.debug(f"   Error: {import_error}")
                    
                    # LLMã§è‡ªå‹•ä¿®æ­£ã‚’è©¦è¡Œ
                    MAX_IMPORT_FIX_ATTEMPTS = 2
                    for attempt in range(MAX_IMPORT_FIX_ATTEMPTS):
                        logger.debug(f"[Runtime Import] Fix attempt {attempt + 1}/{MAX_IMPORT_FIX_ATTEMPTS}")
                        
                        fixed_code = self._fix_import_error_with_llm(
                            generated_code,
                            import_error,
                            failing_file
                        )
                        
                        if fixed_code:
                            # ä¿®æ­£å¾Œã«å†ãƒ†ã‚¹ãƒˆ
                            retry_success, retry_error, retry_file = self._validate_runtime_import(fixed_code)
                            
                            # å¤–éƒ¨ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ä¸è¶³ã«ãªã£ãŸå ´åˆã‚‚OK
                            if retry_success:
                                if retry_error and retry_error.startswith("EXTERNAL_PACKAGE:"):
                                    package_name = retry_error.replace("EXTERNAL_PACKAGE:", "")
                                    logger.debug(f"   âœ… Code error fixed. External package required: {package_name}")
                                else:
                                    logger.debug(f"   âœ… Import error fixed (attempt {attempt + 1})")
                                generated_code = fixed_code
                                logger.debug(f"[Runtime Import] âœ… Fixed on attempt {attempt + 1}")
                                break
                            else:
                                # ä¿®æ­£ã—ãŸãŒåˆ¥ã®ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ
                                import_error = retry_error
                                failing_file = retry_file
                                generated_code = fixed_code  # éƒ¨åˆ†çš„ãªä¿®æ­£ã‚’ä¿æŒ
                                logger.debug(f"[Runtime Import] Partial fix, new error: {retry_error}")
                        else:
                            logger.debug(f"[Runtime Import] Fix attempt {attempt + 1} failed")
                    else:
                        # å…¨ã¦ã®è©¦è¡ŒãŒå¤±æ•—
                        logger.debug(f"   âš  Could not auto-fix. Manual review required.")
                        logger.debug("[Runtime Import] All fix attempts failed")

                # Step 4: Lint check & Auto-fix (æ–°è¦)
                lint_result = None  # Qualityè©•ä¾¡ã§ä½¿ç”¨ã™ã‚‹ãŸã‚åˆæœŸåŒ–
                hud.mark("Lint check & Auto-fix")
                
                # Linterçµ±åˆã‚’è©¦è¡Œ (å¤–éƒ¨linterãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿)
                try:
                    from cognix.linter_integration import LinterIntegration
                    from cognix import hud_components as hud_comp
                    
                    linter = LinterIntegration()
                    
                    # linterãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã®ã¿å®Ÿè¡Œ
                    if linter.available_linters:
                        logger.debug("[Linter] Running external linters...")
                        lint_result = linter.lint_generated_code(generated_code)
                        
                        if lint_result['has_errors']:
                            error_count = len(lint_result['errors'])
                            warning_count = len(lint_result['warnings'])
                            linters_used = ', '.join(lint_result['linters_used'])
                            
                            logger.debug(f"[Linter] Detected {error_count} error(s), {warning_count} warning(s)")
                            logger.debug(f"[Linter] Used: {linters_used}")
                            
                            # Zen HUD: Linterè¦ç´„ã®ã¿
                            error_count = len(lint_result.get('errors', []))
                            warning_count = len(lint_result.get('warnings', []))
                            ok_count = 0  # Linterã¯ã‚¨ãƒ©ãƒ¼/è­¦å‘Šã®ã¿æ¤œå‡º
                            
                            # â­ Zen HUD: åˆæœŸLintã‚¨ãƒ©ãƒ¼æ•°ã‚’è¨˜éŒ²
                            self._zen_summary["lint"]["initial"] = error_count
                            
                            logger.debug("")  # ç©ºè¡Œ
                            logger.debug(hud_comp.lint_summary_line(ok_count, warning_count, error_count))
                            
                            # è‡ªå‹•ä¿®æ­£å¯èƒ½ãªå ´åˆã¯LLMã«ä¿®æ­£ä¾é ¼
                            if lint_result['can_auto_fix']:
                                logger.debug("[Linter] Attempting auto-fix with LLM...")
                                logger.debug("[Linter] Auto-fix attempt started")
                                
                                fixed_files = self._fix_linter_errors_with_llm(
                                    generated_code,
                                    lint_result['errors'],
                                    lint_result.get('file_languages', {})
                                )
                                
                                if fixed_files:
                                    logger.debug("[Linter] Auto-fix successful")
                                    logger.debug(f"{Icon.GEAR.value} Linter errors have been auto-fixed")
                                    generated_code = fixed_files
                                    
                                    # ==========================================
                                    # Auto-fixå¾Œã«å†åº¦Lintã‚’å®Ÿè¡Œ
                                    # ==========================================
                                    try:
                                        logger.debug("[Linter] Re-running linter after auto-fix...")
                                        recheck_result = linter.lint_generated_code(generated_code)
                                        
                                        # ä¿®æ­£å‰å¾Œã®ã‚¨ãƒ©ãƒ¼æ•°ã‚’æ¯”è¼ƒ
                                        error_count_before = len(lint_result.get('errors', []))
                                        warning_count_before = len(lint_result.get('warnings', []))
                                        error_count_after = len(recheck_result.get('errors', []))
                                        warning_count_after = len(recheck_result.get('warnings', []))
                                        
                                        # ä¿®æ­£åŠ¹æœã‚’è¡¨ç¤º
                                        logger.debug("")  # ç©ºè¡Œ
                                        logger.debug("  After auto-fix:")
                                        logger.debug(hud_comp.lint_summary_line(0, warning_count_after, error_count_after))
                                        
                                        # ã‚¨ãƒ©ãƒ¼å†…è¨³ã‚’è¡¨ç¤ºï¼ˆOption A: ç°¡æ½”ç‰ˆï¼‰
                                        if error_count_after > 0 or warning_count_after > 0:
                                            breakdown = self._get_error_breakdown(
                                                recheck_result.get('errors', []),
                                                recheck_result.get('warnings', [])
                                            )
                                            # 0ã§ãªã„ã‚«ãƒ†ã‚´ãƒªã®ã¿è¡¨ç¤ºï¼ˆç°¡æ½”åŒ–ï¼‰
                                            error_parts = []
                                            if breakdown['critical'] > 0:
                                                error_parts.append(f"Critical={breakdown['critical']}")
                                            if breakdown['high'] > 0:
                                                error_parts.append(f"High={breakdown['high']}")
                                            if breakdown['medium'] > 0:
                                                error_parts.append(f"Medium={breakdown['medium']}")
                                            if breakdown['low'] > 0:
                                                error_parts.append(f"Low={breakdown['low']} (style issues)")
                                            if breakdown['env'] > 0:
                                                error_parts.append(f"Env={breakdown['env']}")
                                            if breakdown['warnings'] > 0:
                                                error_parts.append(f"Warnings={breakdown['warnings']}")
                                            
                                            if error_parts:
                                                logger.debug(f"  Error Types: {', '.join(error_parts)}")

                                        
                                        # Qualityè©•ä¾¡ç”¨ã«lint_resultã‚’æ›´æ–°ï¼ˆé‡è¦ï¼ï¼‰
                                        lint_result = recheck_result
                                        
                                        # â­ Zen HUD: Auto-fixå¾Œã®Lintã‚¨ãƒ©ãƒ¼æ•°ã‚’è¨˜éŒ²
                                        self._zen_summary["lint"]["final"] = error_count_after
                                        self._zen_summary["lint"]["fixed"] = (error_count_after < error_count_before)
                                        
                                        # ä¿®æ­£åŠ¹æœã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                                        errors_fixed = error_count_before - error_count_after
                                        if error_count_after == 0 and warning_count_after == 0:
                                            logger.debug(f"  âœ“ All linter errors resolved!")
                                            # 0ã§ãªã„ã‚«ãƒ†ã‚´ãƒªã®ã¿è¡¨ç¤ºï¼ˆç°¡æ½”åŒ–ï¼‰
                                            error_parts = []
                                            if breakdown['critical'] > 0:
                                                error_parts.append(f"Critical={breakdown['critical']}")
                                            if breakdown['high'] > 0:
                                                error_parts.append(f"High={breakdown['high']}")
                                            if breakdown['medium'] > 0:
                                                error_parts.append(f"Medium={breakdown['medium']}")
                                            if breakdown['low'] > 0:
                                                error_parts.append(f"Low={breakdown['low']} (style issues)")
                                            if breakdown['env'] > 0:
                                                error_parts.append(f"Env={breakdown['env']}")
                                            if breakdown['warnings'] > 0:
                                                error_parts.append(f"Warnings={breakdown['warnings']}")
                                            
                                            if error_parts:
                                                logger.debug(f"  Error Types: {', '.join(error_parts)}")

                                        
                                    except Exception as e:
                                        logger.debug(f"[Linter] Re-check failed: {e}")
                                        # lint_resultã¯Auto-fixå‰ã®ã¾ã¾ã«ã™ã‚‹ï¼ˆfallbackï¼‰

                                else:
                                    logger.debug("[Linter] Auto-fix failed")
                                    logger.debug("  âš   Auto-fix failed. Manual review required.")
                            else:
                                logger.debug("  â†’ run with --verbose for details")
                                logger.debug("  âš   Manual review required (syntax errors detected)")
                        
                        elif lint_result['has_warnings']:
                            warning_count = len(lint_result['warnings'])
                            logger.debug(f"[Linter] Detected {warning_count} warning(s)")
                            logger.debug(f"\nâ— Code passed linter checks ({warning_count} warnings)")
                        else:
                            logger.debug("[Linter] No issues detected")
                            logger.debug("\nâ— Code passed all linter checks")
                    else:
                        logger.debug("[Linter] No external linters available (optional feature)")
                
                except ImportError:
                    logger.debug("[Linter] LinterIntegration not available (optional feature)")
                except Exception as e:
                    logger.debug(f"[Linter] Error during linting: {e}")
                
                # ==========================================
                # Cross-File Consistency Check (æ–°è¦è¿½åŠ )
                # ãƒ¢ãƒ‡ãƒ«å®šç¾©ã¨åˆæœŸãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ã‚’æ¤œè¨¼
                # ==========================================
                try:
                    logger.debug("[Cross-File] Starting cross-file consistency check...")
                    cross_file_result = self._check_cross_file_consistency(generated_code)
                    
                    if cross_file_result['has_issues']:
                        issue_count = len(cross_file_result['issues'])
                        logger.debug(f"[Cross-File] Detected {issue_count} consistency issue(s)")
                        
                        # å•é¡Œã®è©³ç´°ã‚’ãƒ­ã‚°ã«å‡ºåŠ›
                        for issue in cross_file_result['issues']:
                            logger.debug(
                                f"[Cross-File]   {issue['model']}.{issue['field']}: "
                                f"{issue['message']}"
                            )
                        
                        # Zen HUD: å•é¡Œæ¤œå‡ºã‚’è¡¨ç¤º
                        logger.debug("")
                        logger.debug(f"  {Icon.WARNING.value} Cross-file consistency: {issue_count} issue(s) detected")
                        
                        # LLMã«ã‚ˆã‚‹è‡ªå‹•ä¿®æ­£ã‚’è©¦è¡Œ
                        logger.debug("[Cross-File] Attempting auto-fix with LLM...")
                        fixed_files = self._fix_cross_file_issues_with_llm(
                            generated_code,
                            cross_file_result['issues'],
                            cross_file_result.get('model_definitions', {})
                        )
                        
                        if fixed_files:
                            # ä¿®æ­£ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’åæ˜ 
                            for filename, code in fixed_files.items():
                                generated_code[filename] = code
                            
                            logger.debug(f"[Cross-File] Auto-fix successful: {list(fixed_files.keys())}")
                            logger.debug(f"  {Icon.GEAR.value} Cross-file issues have been auto-fixed")
                            
                            # ä¿®æ­£å¾Œã«å†æ¤œè¨¼
                            recheck_result = self._check_cross_file_consistency(generated_code)
                            if recheck_result['has_issues']:
                                remaining = len(recheck_result['issues'])
                                logger.debug(f"  {Icon.WARNING.value} {remaining} issue(s) remain after auto-fix")
                            else:
                                logger.debug(f"  {Icon.CHECK.value} All cross-file issues resolved")
                        else:
                            logger.debug("[Cross-File] Auto-fix failed")
                            logger.debug(f"  {Icon.WARNING.value} Auto-fix failed. Manual review required.")
                    else:
                        logger.debug("[Cross-File] No consistency issues detected")
                
                except Exception as e:
                    logger.debug(f"[Cross-File] Error during consistency check: {e}")
                    # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚å‡¦ç†ã‚’ç¶™ç¶š
                
                # ==========================================
                # ESLintçµ±åˆ: JavaScriptå“è³ªãƒã‚§ãƒƒã‚¯
                # ==========================================
                eslint_result = None
                try:
                    if self._check_eslint_available():
                        logger.debug("[ESLint] Running JavaScript linter...")
                        eslint_result = self._run_eslint_on_files(generated_code)
                        
                        if eslint_result['files_checked'] > 0:
                            error_count = len(eslint_result['errors'])
                            warning_count = len(eslint_result['warnings'])
                            
                            if error_count > 0 or warning_count > 0:
                                logger.debug("")
                                logger.debug(f"â— ESLint: {error_count} error(s), {warning_count} warning(s)")
                                
                                # ã‚¨ãƒ©ãƒ¼è©³ç´°ã‚’è¡¨ç¤ºï¼ˆæœ€å¤§5ä»¶ï¼‰
                                for issue in eslint_result['errors'][:5]:
                                    logger.debug(f"  - {issue['file']}:{issue['line']} [{issue['rule']}] {issue['message']}")
                                
                                if error_count > 5:
                                    logger.debug(f"  ... and {error_count - 5} more error(s)")
                                
                                # LLMã§è‡ªå‹•ä¿®æ­£
                                if error_count > 0:
                                    logger.debug("[ESLint] Attempting auto-fix with LLM...")
                                    generated_code = self._fix_eslint_errors_with_llm(
                                        generated_code,
                                        eslint_result['errors']
                                    )
                                    
                                    # å†æ¤œè¨¼
                                    recheck_result = self._run_eslint_on_files(generated_code)
                                    error_count_after = len(recheck_result['errors'])
                                    warning_count_after = len(recheck_result['warnings'])
                                    
                                    logger.debug("")
                                    logger.debug(f"  After auto-fix: {error_count_after} error(s), {warning_count_after} warning(s)")
                                    
                                    if error_count_after == 0:
                                        logger.debug(f"  âœ“ All ESLint errors resolved!")
                                    
                                    eslint_result = recheck_result
                            else:
                                logger.debug("[ESLint] No issues found")
                    else:
                        logger.debug("[ESLint] Not available, skipping JavaScript lint check")
                        
                except Exception as e:
                    logger.debug(f"[ESLint] Error during lint check: {e}")
                
                hud.complete("Lint check & Auto-fix")

                # Step 4: Quality assessment & Comprehensive Review
                hud.mark("Quality assessment & Comprehensive Review")
                quality_scores = self._assess_code_quality(
                    generated_code, 
                    complexity, 
                    goal,
                    lint_result=lint_result
                )

                # Quality issues ã®æ¤œå‡ºã¨è¡¨ç¤ºï¼ˆåˆå›ç”Ÿæˆæ™‚ï¼‰
                quality_issues = []
                quality_issue_count = 0
                
                for filename, score in quality_scores.items():
                    if score < 0.70:  # 70%æœªæº€ã¯å•é¡Œã‚ã‚Š
                        quality_issue_count += 1
                        # è©³ç´°ã‚’å–å¾—
                        code = generated_code.get(filename, '')
                        ext = filename.split('.')[-1]
                        placeholder_count, detected_lines = self._detect_placeholders_in_comments(code, ext)
                        
                        if placeholder_count > 0:
                            quality_issues.append({
                                'filename': filename,
                                'score': score,
                                'issue_type': 'placeholder',
                                'count': placeholder_count,
                                'details': detected_lines[:3]  # æœ€å¤§3ä»¶
                            })
                
                # Quality issues è¡¨ç¤ºï¼ˆåˆå›ç”Ÿæˆæ™‚ï¼‰
                if quality_issue_count > 0:
                    logger.debug(f"\n[Quality] Code quality check: {quality_issue_count} quality issue(s) detected")
                    
                    for issue in quality_issues[:5]:  # æœ€å¤§5ä»¶è¡¨ç¤º
                        logger.debug(f"  - {issue['filename']}: {issue['count']} placeholder(s) in comments")
                        for detail in issue['details']:
                            logger.debug(f"    â€¢ {detail}")
                    
                    # Quality issues ã®è‡ªå‹•ä¿®æ­£
                    if quality_issues:
                        logger.debug("[Quality] Attempting to fix quality issues...")
                        fixed_files = self._fix_quality_issues_with_llm(generated_code, quality_issues)
                        
                        if fixed_files:
                            generated_code = fixed_files
                            logger.debug("[Quality] Quality issues fixed")
                            logger.debug(f"{Icon.GEAR.value} Quality issues have been auto-fixed")
                            
                            # Quality å†è©•ä¾¡
                            quality_scores = self._assess_code_quality(
                                generated_code,
                                complexity,
                                goal,
                                lint_result=lint_result
                            )

                # Step 4ã®ç¶šã: ç·åˆãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆComprehensive Reviewï¼‰
                # ==========================================
                # ã€Œä¸€ç™ºã§å‹•ãã€ã‚’ä¿è¨¼ã™ã‚‹ãŸã‚ã®æœ€çµ‚ãƒã‚§ãƒƒã‚¯
                logger.debug("[Comprehensive Review] Starting final review...")
                generated_code = self._final_comprehensive_review(generated_code, goal)
                
                # ğŸ†• Comprehensive Reviewå¾Œã®HTMLå‚ç…§ä¿®æ­£ï¼ˆã‚¹ã‚¿ãƒ–å†ç”Ÿæˆå¾Œã«å¿…è¦ï¼‰
                if getattr(self, '_no_framework_mode', False):
                    logger.debug("[Post-Review] Checking HTML script references after comprehensive review...")
                    generated_code = self._fix_html_script_reference_for_vanilla_js(generated_code)
                
                # ç·åˆãƒ¬ãƒ“ãƒ¥ãƒ¼å¾Œã®Qualityå†è©•ä¾¡
                quality_scores = self._assess_code_quality(
                    generated_code,
                    complexity,
                    goal,
                    lint_result=lint_result
                )

                # Quality assessment & Comprehensive Review å®Œäº†
                hud.complete("Quality assessment & Comprehensive Review")

                # HUDçµ‚äº†ï¼ˆAuto-completionå®Œäº†å¾Œï¼‰
                hud.finish(success=True)

            
            # ã“ã“ã‹ã‚‰å…±é€šå‡¦ç†ï¼ˆboth pathsï¼‰
            

            # Step 4.7: Impact Analysis (æ—¢å­˜ã‚³ãƒ¼ãƒ‰ - å¤‰æ›´ãªã—)
            impact_results = {}
            if self.impact_analyzer:
                logger.verbose("Analyzing impact on existing files...")
                
                for filename, code in generated_code.items():
                    file_path = self.workspace_path / filename
                    
                    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã®ã¿å½±éŸ¿åˆ†æ
                    if file_path.exists():
                        try:
                            impact = self.impact_analyzer.analyze_change_impact(
                                str(file_path),
                                change_type="modify"
                            )
                            impact_results[filename] = impact
                        except Exception as e:
                            logger.debug(f"Impact analysis error for {filename}: {e}")
            
            # Step 5: Generate recommendations (æ—¢å­˜ã‚³ãƒ¼ãƒ‰ - å¤‰æ›´ãªã—)
            recommendations = self._generate_recommendations(
                goal,
                analysis,
                generated_code,
                quality_scores
            )
            
            # Phase 2: Zen HUDè¦ç´„è¡¨ç¤ºã®ã¿ï¼ˆãƒãƒŠãƒ¼å‰Šé™¤ï¼‰
            # è¨­è¨ˆæ›¸: å®Œäº†å¾Œã¯è¦ç´„ã ã‘
            
            
            # æˆåŠŸçµæœã‚’è¿”ã™
            return SemiAutoResult(
                success=True,
                analysis=analysis,
                generated_code=generated_code,
                quality_scores=quality_scores,
                recommendations=recommendations,
                impact_analysis=impact_results,
                lint_result=lint_result,
                zen_summary=self._zen_summary  # â­ Zen HUDç”¨ã‚µãƒãƒªãƒ¼
            )
            
        except Exception as e:
            logger.error(f"Semi-auto implementation error: {e}")
            
            # HUDãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯å¤±æ•—ã¨ã—ã¦çµ‚äº†
            try:
                if hud is not None:
                    hud.finish(success=False)
            except:
                pass  # HUDçµ‚äº†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ç„¡è¦–
            
            # ã‚¨ãƒ©ãƒ¼ãƒãƒŠãƒ¼ã‚’è¡¨ç¤ºï¼ˆPhase 1: UIæ”¹å–„ï¼‰
            logger.debug("")  # ç©ºè¡Œ
            print_error_banner(
                "Code generation failed",
                details=str(e)
            )
            return SemiAutoResult(
                success=False,
                error=str(e)
            )

    def retry_lint_check(self, generated_code: Dict[str, str], prev_lint_result: Optional[Dict[str, Any]] = None) -> Tuple[Dict[str, str], Optional[Dict[str, Any]], Dict[str, float]]:
        """
        Lintå†å®Ÿè¡Œã¨Auto-fixï¼ˆ[t] Try againç”¨ï¼‰
        
        Args:
            generated_code: ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ {filename: code}
            prev_lint_result: å‰å›ã®Lintçµæœï¼ˆæ”¹å–„æ¤œå‡ºç”¨ï¼‰
            
        Returns:
            Tuple[updated_code, lint_result, quality_scores]
        """
        # Note: err_console and Icon are already imported at file top (lines 13, 19)
        # Import only console and hud_comp
        try:
            from rich.console import Console
            console = Console()
        except ImportError:
            # Fallback: use err_console
            console = err_console
        
        try:
            from cognix import hud_components as hud_comp
        except ImportError:
            # Fallback: create minimal hud_comp wrapper
            class HudCompFallback:
                @staticmethod
                def lint_summary_line(ok, warn, fail):
                    from rich.text import Text
                    return Text.from_ansi(f"â“˜ Lint Summary   OK {ok} / WARN {warn} / FAIL {fail}")
            hud_comp = HudCompFallback()
        
        logger.debug("[Linter Retry] Starting Lint check & Auto-fix retry...")
        
        try:
            from cognix.linter_integration import LinterIntegration
            
            linter = LinterIntegration()
            
            # LinterãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
            if not linter.available_linters:
                logger.debug("[Linter Retry] No external linters available")
                logger.debug(f"{Icon.WARNING.value} No external linters available. Cannot retry.")
                return (generated_code, prev_lint_result, {})
            
            # Lintå®Ÿè¡Œ
            logger.debug("[Linter Retry] Running external linters...")
            lint_result = linter.lint_generated_code(generated_code)
            
            # å‰å›ã®çµæœã¨æ¯”è¼ƒï¼ˆæ”¹å–„æ¤œå‡ºï¼‰
            if prev_lint_result:
                prev_fail_count = len(prev_lint_result.get('errors', []))
                current_fail_count = len(lint_result.get('errors', []))
            else:
                prev_fail_count = None
                current_fail_count = len(lint_result.get('errors', []))
            
            # Lintçµæœè¡¨ç¤º
            if lint_result['has_errors']:
                error_count = len(lint_result['errors'])
                warning_count = len(lint_result['warnings'])
                ok_count = 0
                
                logger.debug("")  # ç©ºè¡Œ
                logger.debug(hud_comp.lint_summary_line(ok_count, warning_count, error_count))
                
                # è‡ªå‹•ä¿®æ­£ã‚’è©¦è¡Œ
                if lint_result['can_auto_fix']:
                    logger.debug("[Linter Retry] Attempting auto-fix with LLM...")
                    
                    fixed_files = self._fix_linter_errors_with_llm(
                        generated_code,
                        lint_result['errors'],
                        lint_result.get('file_languages', {})
                    )
                    
                    if fixed_files:
                        logger.debug("[Linter Retry] Auto-fix successful")
                        logger.debug(f"{Icon.GEAR.value} Linter errors have been auto-fixed")
                        generated_code = fixed_files
                        
                        # Auto-fixå¾Œã«å†åº¦Lintå®Ÿè¡Œ
                        try:
                            logger.debug("[Linter Retry] Re-running linter after auto-fix...")
                            recheck_result = linter.lint_generated_code(generated_code)
                            
                            error_count_after = len(recheck_result.get('errors', []))
                            warning_count_after = len(recheck_result.get('warnings', []))
                            
                            # ä¿®æ­£åŠ¹æœã‚’è¡¨ç¤º
                            logger.debug("")  # ç©ºè¡Œ
                            logger.debug("  After auto-fix:")
                            logger.debug(hud_comp.lint_summary_line(0, warning_count_after, error_count_after))
                            
                            # ã‚¨ãƒ©ãƒ¼å†…è¨³ã‚’è¡¨ç¤º
                            if error_count_after > 0 or warning_count_after > 0:
                                breakdown = self._get_error_breakdown(
                                    recheck_result.get('errors', []),
                                    recheck_result.get('warnings', [])
                                )
                                error_parts = []
                                if breakdown['critical'] > 0:
                                    error_parts.append(f"Critical={breakdown['critical']}")
                                if breakdown['high'] > 0:
                                    error_parts.append(f"High={breakdown['high']}")
                                if breakdown['medium'] > 0:
                                    error_parts.append(f"Medium={breakdown['medium']}")
                                if breakdown['low'] > 0:
                                    error_parts.append(f"Low={breakdown['low']} (style issues)")
                                if breakdown['env'] > 0:
                                    error_parts.append(f"Env={breakdown['env']}")
                                if breakdown['warnings'] > 0:
                                    error_parts.append(f"Warnings={breakdown['warnings']}")
                                
                                if error_parts:
                                    logger.debug(f"  Error Types: {', '.join(error_parts)}")
                            
                            # lint_resultã‚’æ›´æ–°
                            lint_result = recheck_result
                            
                            # æ”¹å–„æ¤œå‡º
                            if prev_fail_count is not None:
                                current_fail_count_after = len(recheck_result.get('errors', []))
                                if current_fail_count_after < prev_fail_count:
                                    fixed_count = prev_fail_count - current_fail_count_after
                                    logger.debug("")  # ç©ºè¡Œ
                                    logger.debug(f"\033[92m{Icon.SUCCESS.value} Improved!\033[0m Fixed {fixed_count} more error(s).")
                                elif current_fail_count_after == prev_fail_count:
                                    logger.debug("")  # ç©ºè¡Œ
                                    logger.debug(f"{Icon.WARNING.value} No improvement detected. Remaining errors cannot be auto-fixed.")
                                    logger.debug("  These are minor style issues that can be safely ignored.")
                        
                        except Exception as e:
                            logger.debug(f"[Linter Retry] Re-check failed: {e}")
                    
                    else:
                        logger.debug("[Linter Retry] Auto-fix failed")
                        logger.debug("  âš   Auto-fix failed. Manual review required.")
                else:
                    logger.debug("  âš   Manual review required (syntax errors detected)")
            
            elif lint_result['has_warnings']:
                warning_count = len(lint_result['warnings'])
                logger.debug(f"[Linter Retry] Detected {warning_count} warning(s)")
                logger.debug(f"\n{Icon.INFO.value} Code passed linter checks ({warning_count} warnings)")
            else:
                logger.debug("[Linter Retry] No syntax errors detected")
                logger.debug(f"\n{Icon.SUCCESS.value} Syntax check passed (0 syntax errors, 0 warnings)")
            
            # Qualityå†è¨ˆç®—
            logger.debug("[Linter Retry] Re-assessing code quality...")
            quality_scores = self._assess_code_quality(
                generated_code,
                "medium",  # è¤‡é›‘åº¦ã¯å…ƒã®ã¾ã¾
                "",  # goalã¯ä¸è¦
                lint_result=lint_result
            )
            
            # Quality issues ã®æ¤œå‡ºã¨ä¿®æ­£
            quality_issues = []
            quality_issue_count = 0
            
            for filename, score in quality_scores.items():
                if score < 0.70:  # 70%æœªæº€ã¯å•é¡Œã‚ã‚Š
                    quality_issue_count += 1
                    # è©³ç´°ã‚’å–å¾—
                    code = generated_code.get(filename, '')
                    ext = filename.split('.')[-1]
                    placeholder_count, detected_lines = self._detect_placeholders_in_comments(code, ext)
                    
                    if placeholder_count > 0:
                        quality_issues.append({
                            'filename': filename,
                            'score': score,
                            'issue_type': 'placeholder',
                            'count': placeholder_count,
                            'details': detected_lines[:3]  # æœ€å¤§3ä»¶
                        })
            
            # Quality issues è¡¨ç¤ºã¨ä¿®æ­£
            if quality_issue_count > 0:
                logger.debug(f"\n[Quality] Code quality check: {quality_issue_count} quality issue(s) detected")
                
                for issue in quality_issues[:5]:  # æœ€å¤§5ä»¶è¡¨ç¤º
                    logger.debug(f"  - {issue['filename']}: {issue['count']} placeholder(s) in comments")
                    for detail in issue['details']:
                        logger.debug(f"    â€¢ {detail}")
                
                # LLM ã« Quality issues ã®ä¿®æ­£ã‚’ä¾é ¼
                if quality_issues:
                    logger.debug("[Quality] Attempting to fix quality issues with LLM...")
                    logger.debug(f"{Icon.GEAR.value} Attempting to fix quality issues...")
                    
                    fixed_files = self._fix_quality_issues_with_llm(
                        generated_code,
                        quality_issues
                    )
                    
                    if fixed_files:
                        logger.debug("[Quality] Quality fix successful")
                        logger.debug(f"{Icon.SUCCESS.value} Quality issues have been fixed")
                        generated_code = fixed_files
                        
                        # Quality å†è©•ä¾¡
                        quality_scores = self._assess_code_quality(
                            generated_code,
                            "medium",
                            "",
                            lint_result=lint_result
                        )
                        
                        # æ”¹å–„ã‚’è¡¨ç¤º
                        improved_count = 0
                        for filename, new_score in quality_scores.items():
                            old_issue = next((i for i in quality_issues if i['filename'] == filename), None)
                            if old_issue and new_score >= 0.70:
                                improved_count += 1
                        
                        if improved_count > 0:
                            logger.debug(f"{Icon.SUCCESS.value} Improved {improved_count} file(s)")
                    else:
                        logger.debug("[Quality] Quality fix failed or not attempted")
                        logger.debug(f"{Icon.WARNING.value} Could not auto-fix all quality issues")
            
            # Qualityè¡¨ç¤ºï¼ˆLint/Reviewæƒ…å ±ã‚’åæ˜ ã—ãŸç·åˆè©•ä¾¡ï¼‰
            if quality_scores:
                logger.debug("")  # ç©ºè¡Œ
                
                # Lintæƒ…å ±ã‚’å–å¾—
                lint_errors = len(lint_result.get('errors', [])) if lint_result else 0
                lint_warnings = len(lint_result.get('warnings', [])) if lint_result else 0
                
                # Reviewæƒ…å ±ã‚’å–å¾—ï¼ˆ_zen_summaryã‹ã‚‰ï¼‰
                review_issues = 0
                if hasattr(self, '_zen_summary') and self._zen_summary:
                    review_issues = self._zen_summary.get("review", {}).get("final", 0)
                
                # ç·åˆè©•ä¾¡ã‚’è¨ˆç®—ãƒ»è¡¨ç¤º
                overall_score, quality_display = self._format_quality_display(
                    base_quality_scores=quality_scores,
                    lint_errors=lint_errors,
                    lint_warnings=lint_warnings,
                    review_issues=review_issues
                )
                
                # ç·åˆè©•ä¾¡ã‚’è¡¨ç¤º
                console.print(quality_display)
                console.print()  # ç©ºè¡Œ
                
                # å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¹ã‚³ã‚¢ã‚‚è¡¨ç¤ºï¼ˆè¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã®ã¿ï¼‰
                if len(quality_scores) > 1:
                    from rich.text import Text
                    console.print("Quality Scores:")
                    for filename, score in quality_scores.items():
                        score_percent = int(score * 100)
                        # è‰²ã¨ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ¤å®šï¼ˆcli_chat_workflow.pyã¨çµ±ä¸€ï¼‰
                        if score >= 0.9:
                            color = "\033[92m"  # GREEN
                            grade = "Excellent"
                        elif score >= 0.75:
                            color = "\033[92m"  # GREEN
                            grade = "Good"
                        elif score >= 0.6:
                            color = "\033[93m"  # YELLOW
                            grade = "Fair"
                        else:
                            color = "\033[93m"  # YELLOW
                            grade = "Needs Review"
                        reset = "\033[0m"
                        console.print(Text.from_ansi(f"  {color}{filename}{reset}: {score_percent}% {color}{grade}{reset}"))
                    console.print()  # ç©ºè¡Œ
            
            return (generated_code, lint_result, quality_scores)
        
        except ImportError as ie:
            # è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’å‡ºåŠ›ã—ã¦åŸå› ã‚’ç‰¹å®š
            import sys
            import traceback
            
            logger.error(f"[Linter Retry] ImportError details:")
            logger.error(f"  Error message: {str(ie)}")
            logger.error(f"  Module name: {getattr(ie, 'name', 'unknown')}")
            logger.error(f"  Module path: {getattr(ie, 'path', 'unknown')}")
            logger.error(f"  Traceback:")
            for line in traceback.format_tb(sys.exc_info()[2]):
                logger.error(f"    {line.strip()}")
            
            logger.debug("[Linter Retry] LinterIntegration not available")
            logger.debug(f"{Icon.WARNING.value} Linter not available. Cannot retry.")
            logger.debug(f"[DEBUG] Import failed: {ie}")
            return (generated_code, prev_lint_result, {})
        except Exception as e:
            logger.debug(f"[Linter Retry] Error during retry: {e}")
            logger.debug(f"{Icon.ERROR.value} Error during Lint retry: {e}")
            return (generated_code, prev_lint_result, {})
    
    def _fix_quality_issues_with_llm(
        self,
        generated_code: Dict[str, str],
        quality_issues: List[Dict[str, Any]]
    ) -> Optional[Dict[str, str]]:
        """
        LLMã«Quality issuesã®ä¿®æ­£ã‚’ä¾é ¼
        
        Args:
            generated_code: ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰
            quality_issues: Qualityå•é¡Œã®ãƒªã‚¹ãƒˆ
        
        Returns:
            ä¿®æ­£ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ï¼ˆå¤±æ•—æ™‚ã¯Noneï¼‰
        """
        try:
            # Placeholder issues ã®ã¿å¯¾å¿œï¼ˆç¾æ™‚ç‚¹ï¼‰
            placeholder_issues = [i for i in quality_issues if i['issue_type'] == 'placeholder']
            
            if not placeholder_issues:
                logger.debug("[Quality Fix] No placeholder issues to fix")
                return None
            
            # LLMã«ä¿®æ­£ä¾é ¼
            prompt = self._build_quality_fix_prompt(generated_code, placeholder_issues)
            
            # llm_manager ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
            if not self.llm_manager:
                logger.debug("[Quality Fix] LLM manager not available")
                return None
            
            logger.debug("[Quality Fix] Sending fix request to LLM...")
            response = self.llm_manager.generate_response(
                prompt=prompt,
                system_prompt="You are a code quality improvement expert. Fix placeholder code with proper implementations.",
                max_tokens=8000
            )
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹è§£æ
            logger.debug("[Quality Fix] Parsing LLM response...")
            if hasattr(response, 'content'):
                response_text = response.content
            else:
                response_text = str(response)
            fixed_files = self._parse_llm_response(response_text)
            
            if fixed_files:
                # ä¿®æ­£ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿æ›´æ–°
                updated_code = generated_code.copy()
                actually_updated = 0
                for filename, code in fixed_files.items():
                    if filename in generated_code:
                        updated_code[filename] = code
                        actually_updated += 1
                        logger.debug(f"[Quality Fix] Updated {filename}")
                
                if actually_updated > 0:
                    return updated_code
                else:
                    logger.debug("[Quality Fix] All fixes were skipped (no matching files)")
                    return None
            
            logger.debug("[Quality Fix] No files returned from LLM")
            return None
            
        except Exception as e:
            logger.error(f"[Quality Fix] Error: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return None
    
    def _build_quality_fix_prompt(
        self,
        generated_code: Dict[str, str],
        placeholder_issues: List[Dict[str, Any]]
    ) -> str:
        """Quality fixç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰"""
        
        files_with_issues = [i['filename'] for i in placeholder_issues]
        
        prompt = f"""You are tasked with fixing code quality issues.

**Files with placeholder comments (need implementation):**

"""
        
        for issue in placeholder_issues:
            prompt += f"{issue['filename']}:\n"
            prompt += f"  - {issue['count']} placeholder comment(s) detected:\n"
            for detail in issue['details']:
                prompt += f"    â€¢ {detail}\n"
            prompt += "\n"
        
        prompt += f"""**Current code:**

"""
        
        for filename in files_with_issues:
            code = generated_code.get(filename, '')
            prompt += f"<file path='{filename}'>\n{code}\n</file>\n\n"
        
        prompt += """**Your task:**
1. Replace ALL placeholder comments with actual implementations
2. Implement proper functionality for each placeholder
3. For authentication placeholders:
   - Implement proper password hashing (use werkzeug.security)
   - Add proper validation
   - Add error handling
4. Maintain the existing code structure and style
5. Ensure all functions/methods are fully implemented
6. Do NOT add new placeholders or TODO comments

**Output format:**
Return ONLY the fixed files in XML format:
<file path='filename'>
complete fixed code here
</file>

Generate complete, production-ready implementations. No placeholders or TODO comments allowed.
"""
        
        return prompt
    
    def _parse_llm_response(self, response_content) -> Optional[Dict[str, str]]:
        """
        LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰<file>ã‚¿ã‚°ã‚’æŠ½å‡º
        
        Args:
            response_content: LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆæ–‡å­—åˆ—ã¾ãŸã¯contenté…åˆ—ï¼‰
        
        Returns:
            {filename: code} ã®è¾æ›¸ï¼ˆå¤±æ•—æ™‚ã¯Noneï¼‰
        """
        import re
        
        try:
            # response_content ã‹ã‚‰ text ã‚’æŠ½å‡º
            full_text = ""
            
            # æ–‡å­—åˆ—ã®å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
            if isinstance(response_content, str):
                full_text = response_content
            # ãƒªã‚¹ãƒˆã®å ´åˆã¯å„è¦ç´ ã‹ã‚‰ text ã‚’æŠ½å‡º
            elif isinstance(response_content, list):
                for block in response_content:
                    if hasattr(block, 'text'):
                        full_text += block.text
                    elif isinstance(block, dict) and 'text' in block:
                        full_text += block['text']
                    elif isinstance(block, str):
                        full_text += block
            else:
                # ãã®ä»–ã®å‹ã¯æ–‡å­—åˆ—ã«å¤‰æ›
                full_text = str(response_content)
            
            if not full_text:
                logger.debug("[Parse Response] No text in response")
                return None
            
            # <file path='...'>...</file> ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º
            file_pattern = r"<file\s+path=['\"]([^'\"]+)['\"]>(.*?)</file>"
            matches = re.findall(file_pattern, full_text, re.DOTALL)
            
            if not matches:
                logger.debug("[Parse Response] No <file> tags found")
                return None
            
            files = {}
            for filename, code in matches:
                # ã‚³ãƒ¼ãƒ‰ã®å‰å¾Œã®ç©ºç™½ã‚’å‰Šé™¤
                code = code.strip()
                files[filename] = code
                logger.debug(f"[Parse Response] Extracted {filename} ({len(code)} chars)")
            
            return files if files else None
            
        except Exception as e:
            logger.error(f"[Parse Response] Error: {e}")
            return None
    
    def _analyze_implementation_goal(self, goal: str, complexity: str = "medium") -> str:
        """
        Analyze the implementation goal using complexity-aware templates
        
        Args:
            goal: Implementation goal description
            complexity: "simple" | "medium" | "complex"
            
        Returns:
            Analysis result string
        """
        # Simple complexity: Skip detailed analysis
        if complexity == "simple":
            return f"Simple implementation: {goal}\nDirect code generation without detailed analysis."
        
        # LLM not available: Return basic analysis
        if not self.llm_manager:
            return f"Goal analysis: {goal}\nImplementation approach: Create modular solution with proper error handling."
        
        try:
            # Get project context
            project_context = self._get_project_context()
            
            # For medium/complex: Use standard analysis
            # Note: Complex tasks will get more detailed prompts in code generation phase
            analysis_prompt = f"""
    Analyze this implementation goal concisely for user decision-making:

    Goal: {goal}

    Provide EXACTLY 3 sections with 3-5 bullet points each (max 30 words per bullet):

    ## 1. Key Requirements
    - List 3-5 essential features/functions (one line each)

    ## 2. Technical Approach
    - List 3-5 main implementation steps (one line each)

    ## 3. Potential Issues
    - List 3-5 key challenges or risks (one line each)

    Keep total response under 400 words. Be direct and actionable.
    Use markdown format with ## for sections and - for bullets.
    """
            
            response = self.llm_manager.generate_response(
                prompt=analysis_prompt,
                system_prompt="You are a senior software architect providing implementation analysis."
            )
            
            return response.content
            
        except Exception as e:
            return f"Goal analysis: {goal}\nBasic implementation approach with error handling. Analysis error: {str(e)}"

    def _create_implementation_plan(self, goal: str, analysis: str, complexity: str = "medium") -> str:
        """
        Create detailed implementation plan
        
        Args:
            goal: Implementation goal
            analysis: Analysis result
            complexity: Complexity level
            
        Returns:
            Implementation plan string
        """
        # Simple complexity: Skip detailed planning
        if complexity == "simple":
            return f"Simple implementation plan: Direct code generation for {goal}"
        
        if not self.llm_manager:
            return f"Implementation plan for: {goal}\n1. Create main module\n2. Implement core functionality\n3. Add error handling\n4. Create tests"
        
        try:
            plan_prompt = f"""
Based on this analysis, create a detailed implementation plan:

Goal: {goal}
Analysis: {analysis}

Create a step-by-step implementation plan with:
1. File structure and organization
2. Implementation order (which files to create first)
3. Key functions/classes to implement
4. Dependencies and imports needed
5. Testing strategy

Keep it practical and focused on deliverable code.
"""
            
            response = self.llm_manager.generate_response(
                prompt=plan_prompt,
                system_prompt="You are a technical lead creating practical implementation plans."
            )
            
            return response.content
            
        except Exception:
            return f"Basic implementation plan for: {goal}\n1. Main module\n2. Core functions\n3. Error handling\n4. Documentation"

    def _build_fallback_prompt(self, goal: str, analysis: str, plan: str, language: str) -> str:
            """
            Fallback prompt when PromptTemplateManager fails
            â­ Aideræº–æ‹ : JSONæ’é™¤ã€å…·ä½“çš„ãƒ•ã‚¡ã‚¤ãƒ«åä¾‹
            
            Args:
                goal: Implementation goal
                analysis: Analysis result
                plan: Implementation plan
                language: Programming language
                
            Returns:
                Fallback prompt string
            """
            # â­ è¨€èªåˆ¥ã®å…·ä½“çš„ãƒ•ã‚¡ã‚¤ãƒ«åä¾‹
            filename_examples = {
                'python': 'main.py',
                'javascript': 'script.js',
                'html': 'index.html',
                'css': 'styles.css',
                'rust': 'main.rs',
                'java': 'Main.java',
                'cpp': 'main.cpp',
                'typescript': 'main.ts',
                'go': 'main.go',
                'ruby': 'main.rb',
                'php': 'index.php'
            }
            
            example_filename = filename_examples.get(language, f'main.{language}')
            
            return f"""Act as an expert {language} developer.

    CRITICAL: Respond in PLAIN MARKDOWN format ONLY. NEVER use JSON.

    Code format example:
    ```{language} {example_filename}
    def main():
        # Actual working code here
        pass
    ```

    Goal: {goal}

    Analysis:
    {analysis}

    Plan:
    {plan}

    Generate clean, working code now."""

    def _detect_project_language(self, existing_files: Dict[str, str]) -> str:
        """
        æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä¸»è¦è¨€èªã‚’æ¤œå‡º
        
        Args:
            existing_files: æ—¢å­˜ã®ãƒ•ã‚¡ã‚¤ãƒ« {filename: code}
        
        Returns:
            æ¤œå‡ºã•ã‚ŒãŸè¨€èªå(ä¾‹: "javascript", "python", "typescript")
            æ¤œå‡ºã§ããªã„å ´åˆã¯ç©ºæ–‡å­—åˆ—
        """
        if not existing_files:
            return ""
        
        extension_count = {}
        
        # ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®æ‹¡å¼µå­ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        for filename in existing_files.keys():
            if '.' in filename:
                # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ‹¡å¼µå­ã‚’å–å¾—
                ext = filename.split('.')[-1].lower()
                extension_count[ext] = extension_count.get(ext, 0) + 1
        
        if not extension_count:
            return ""
        
        # æœ€ã‚‚å¤šã„æ‹¡å¼µå­ã‚’å–å¾—
        most_common_ext = max(extension_count.items(), key=lambda x: x[1])[0]
        
        # æ‹¡å¼µå­ã‹ã‚‰è¨€èªåã‚’æ¨æ¸¬
        ext_to_lang = {
            'py': 'python',
            'js': 'javascript',
            'jsx': 'react',
            'ts': 'typescript',
            'tsx': 'react-typescript',
            'html': 'html',
            'css': 'css',
            'scss': 'scss',
            'sass': 'sass',
            'less': 'less',
            'java': 'java',
            'cpp': 'c++',
            'cc': 'c++',
            'cxx': 'c++',
            'c': 'c',
            'h': 'c',
            'hpp': 'c++',
            'go': 'go',
            'rs': 'rust',
            'rb': 'ruby',
            'php': 'php',
            'swift': 'swift',
            'kt': 'kotlin',
            'scala': 'scala',
            'cs': 'csharp',
            'vb': 'visual-basic',
            'sql': 'sql',
            'sh': 'shell',
            'bash': 'bash',
            'ps1': 'powershell',
            'r': 'r',
            'lua': 'lua',
            'perl': 'perl',
            'pl': 'perl',
            'dart': 'dart',
            'elm': 'elm',
            'ex': 'elixir',
            'exs': 'elixir',
            'erl': 'erlang',
            'hrl': 'erlang',
            'clj': 'clojure',
            'cljs': 'clojurescript',
            'ml': 'ocaml',
            'fs': 'fsharp',
            'hs': 'haskell',
            'vue': 'vue',
            'svelte': 'svelte'
        }
        
        detected_lang = ext_to_lang.get(most_common_ext, most_common_ext)
        
        return detected_lang

    def _is_directory_path(self, path: str) -> bool:
        """
        ãƒ‘ã‚¹ãŒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ãƒ•ã‚¡ã‚¤ãƒ«ã‹åˆ¤å®š
        
        Args:
            path: åˆ¤å®šã™ã‚‹ãƒ‘ã‚¹æ–‡å­—åˆ—
            
        Returns:
            True: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª, False: ãƒ•ã‚¡ã‚¤ãƒ«
        """
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: æœ«å°¾ãŒ / ã§çµ‚ã‚ã‚‹
        if path.endswith('/') or path.endswith('\\'):
            return True
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: æœ€å¾Œã®è¦ç´ ã«æ‹¡å¼µå­ãŒãªã„
        last_part = path.split('/')[-1].split('\\')[-1]
        
        # æ‹¡å¼µå­ã®åˆ¤å®š (. ãŒå«ã¾ã‚Œã¦ã„ã‚Œã°ãƒ•ã‚¡ã‚¤ãƒ«)
        if '.' not in last_part:
            return True
        
        return False

    def _resolve_directory_to_filename(self, directory: str, goal: str, project_language: str) -> str:
        """
        ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ã‚’å…·ä½“çš„ãªãƒ•ã‚¡ã‚¤ãƒ«åã«è§£æ±º
        
        Args:
            directory: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ (ä¾‹: "routes/", "utils/")
            goal: å®Ÿè£…ç›®æ¨™
            project_language: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨€èª
            
        Returns:
            å…·ä½“çš„ãªãƒ•ã‚¡ã‚¤ãƒ«å (ä¾‹: "routes/auth.js", "utils/logger.js")
        """
        # æ‹¡å¼µå­ã®æ±ºå®š
        ext_map = {
            'javascript': 'js',
            'typescript': 'ts',
            'python': 'py',
            'java': 'java',
            'cpp': 'cpp',
            'rust': 'rs',
            'go': 'go',
            'ruby': 'rb',
            'php': 'php'
        }
        
        extension = ext_map.get(project_language, 'py')
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‹ã‚‰æ¨æ¸¬ã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«å
        dir_name = directory.rstrip('/').rstrip('\\').split('/')[-1].split('\\')[-1]
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ¨æ¸¬
        # ä¾‹: "routes/" â†’ "routes/auth.js", "utils/" â†’ "utils/logger.js"
        filename_map = {
            'routes': 'auth',
            'routers': 'auth',
            'utils': 'logger',
            'helpers': 'helper',
            'services': 'service',
            'middleware': 'auth',
            'middlewares': 'auth',
            'controllers': 'controller',
            'models': 'model',
            'views': 'view'
        }
        
        # goalã‹ã‚‰æ¨æ¸¬ã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«å
        goal_lower = goal.lower()
        if 'auth' in goal_lower or 'login' in goal_lower:
            base_filename = 'auth'
        elif 'user' in goal_lower:
            base_filename = 'user'
        else:
            base_filename = filename_map.get(dir_name, dir_name)
        
        # å®Œå…¨ãªãƒ‘ã‚¹ã‚’æ§‹ç¯‰
        if directory.endswith('/') or directory.endswith('\\'):
            return f"{directory}{base_filename}.{extension}"
        else:
            return f"{directory}/{base_filename}.{extension}"

    def _format_generated_files_context(self, files: Dict[str, str]) -> str:
        """
        ç”Ÿæˆæ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        â­ ä¿®æ­£: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨€èªæƒ…å ±ã‚’è¿½åŠ 
        
        æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’LLMãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å«ã‚ã‚‹ãŸã‚ã€
        ãƒˆãƒ¼ã‚¯ãƒ³ç¯€ç´„ã—ãªãŒã‚‰ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ã‚’ä¼ãˆã‚‹å½¢å¼ã«å¤‰æ›ã—ã¾ã™ã€‚
        
        Args:
            files: ç”Ÿæˆæ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®è¾æ›¸ {filename: code}
            
        Returns:
            ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—åˆ—
        """
        if not files:
            return "No files generated yet."
        
        # â­ æ–°è¦è¿½åŠ : ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨€èªã®æ¤œå‡º
        project_language = self._detect_project_language(files)
        
        context_parts = []
        
        # â­ æ–°è¦è¿½åŠ : è¨€èªæƒ…å ±ã‚’æœ€åˆã«æ˜è¨˜
        context_parts.append(f"Project Language: {project_language}")
        context_parts.append(f"Total Files: {len(files)}")
        context_parts.append("")
        
        for filename, code in files.items():
            # â­ ä¿®æ­£A: ãƒ¢ãƒ‡ãƒ«ãƒ»ã‚¹ã‚­ãƒ¼ãƒãƒ»è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã¯å…¨æ–‡æ¸¡ã™ï¼ˆæ•´åˆæ€§ã®ãŸã‚ï¼‰
            # ã“ã‚Œã‚‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯Phase 2ã§ã‚µãƒ¼ãƒ“ã‚¹å±¤ãŒå‚ç…§ã™ã‚‹ãŸã‚ã€
            # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å®šç¾©ãŒæ¬ è½ã™ã‚‹ã¨æ•´åˆæ€§ã®å•é¡ŒãŒç™ºç”Ÿã™ã‚‹
            is_critical_file = (
                'models/' in filename or filename.startswith('models/') or
                'schemas/' in filename or filename.startswith('schemas/') or
                'config/' in filename or filename.startswith('config/') or
                filename == 'database.py'
            )
            
            if is_critical_file:
                # ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã¯å…¨æ–‡æ¸¡ã™
                preview = code
            else:
                # ãã®ä»–ã¯50è¡Œã«åˆ¶é™ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ç¯€ç´„ï¼‰
                lines = code.split('\n')[:50]
                preview = '\n'.join(lines)
                
                # 50è¡Œã‚’è¶…ãˆã‚‹å ´åˆã¯çœç•¥è¨˜å·ã‚’è¿½åŠ 
                if len(code.split('\n')) > 50:
                    preview += '\n... (truncated)'
            
            context_parts.append(f"""File: {filename}
    ---
    {preview}
    """)
        
        return '\n'.join(context_parts)

    # ============================================================
    # ã‚¹ãƒ†ãƒƒãƒ—2-3ã§è¿½åŠ : Importè€ƒæ…®å‹è‡ªå‹•è£œå®Œ
    # ============================================================
    
    def _auto_complete_with_import_resolution(
        self,
        goal: str,
        existing_files: Dict[str, str],
        validation_result: Dict[str, any],
        complexity: str
    ) -> Dict[str, str]:
        """
        Importä¾å­˜é–¢ä¿‚ã‚’è€ƒæ…®ã—ãŸè‡ªå‹•è£œå®Œ(å¼·åŒ–ç‰ˆ)
        
        å¾“æ¥ã®ä¸è¶³ãƒ•ã‚¡ã‚¤ãƒ«è£œå®Œã«åŠ ãˆã¦ã€æœªè§£æ±ºimportã‚’æ¤œå‡ºã—ã€
        å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’è‡ªå‹•ç”Ÿæˆã—ã¾ã™ã€‚
        
        Args:
            goal: å®Ÿè£…ç›®æ¨™
            existing_files: æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ« {filename: code}
            validation_result: validate_requirements_enhanced()ã®çµæœ
                {
                    'missing_files': List[str],
                    'unresolved_imports': List[Dict],
                    ...
                }
            complexity: è¤‡é›‘åº¦
        
        Returns:
            Dict[str, str]: è£œå®Œã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ç¾¤
        
        Example:
            >>> engine = SemiAutoEngine(...)
            >>> result = engine._auto_complete_with_import_resolution(
            ...     goal="create a REST API",
            ...     existing_files={'main.py': '...'},
            ...     validation_result={
            ...         'missing_files': ['config.py'],
            ...         'unresolved_imports': [
            ...             {'file': 'main.py', 'missing_file': 'auth.py', ...}
            ...         ]
            ...     },
            ...     complexity="medium"
            ... )
        """
        completed_files = existing_files.copy()
        
        missing_files = validation_result.get('missing_files', [])
        unresolved_imports = validation_result.get('unresolved_imports', [])
        
        total_to_complete = len(missing_files) + len(unresolved_imports)
        
        if total_to_complete == 0:
            return completed_files
        
        logger.debug(f"[Auto-completion] Auto-completing {total_to_complete} missing components...")
        
        # ==========================================
        # Part 1: å¾“æ¥ã®æ¬ è½ãƒ•ã‚¡ã‚¤ãƒ«è£œå®Œ
        # ==========================================
        if missing_files:
            logger.debug(f"[Auto-completion]   Missing files: {len(missing_files)}")
            
            for missing_file in missing_files:
                # æ—¢ã«è£œå®Œæ¸ˆã¿ã¾ãŸã¯å…ƒã€…å­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                if missing_file in completed_files:
                    continue
                
                logger.debug(f"[Auto-completion]   Generating missing file: {missing_file}")
                
                # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨€èªã®æ¤œå‡º
                detected_language = self._detect_project_language(completed_files)
                
                # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
                existing_context = self._format_generated_files_context(completed_files)
                
                # ğŸ†• ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯åˆ¶ç´„ã‚’æ§‹ç¯‰
                framework_constraint = ""
                if getattr(self, '_no_framework_mode', False):
                    framework_constraint = """
âš ï¸ CRITICAL FRAMEWORK RESTRICTION:
- DO NOT use React, Vue, Angular, or any framework
- DO NOT generate JSX syntax
- DO NOT generate "this file is not used" or stub comments
- YOU MUST generate REAL, FUNCTIONAL vanilla JavaScript code
- The file must work standalone without any framework
"""
                
                # è£œå®Œç”¨ã®goalæ§‹ç¯‰
                completion_goal = f"""Generate the missing file: {missing_file}

Project Language: {detected_language}
{framework_constraint}
Previously generated files:
{existing_context}

Now create ONLY this missing file:
- {missing_file}

REQUIREMENTS:
- Ensure compatibility with existing files
- Follow the same coding style and conventions
- Use appropriate imports from existing modules
- Make the file production-ready
- Generate REAL, FUNCTIONAL code (not stubs or comments)

Generate complete, production-ready code for {missing_file}.
"""
                
                try:
                    # LLMç”Ÿæˆ(force_filenameã‚’ä½¿ç”¨)
                    completion_files = self._generate_code_implementation(
                        goal=completion_goal,
                        analysis="",
                        plan="",
                        complexity="simple",
                        skip_validation=True,
                        skip_multi_stage=True,
                        force_filename=missing_file
                    )
                    
                    if completion_files and missing_file in completion_files:
                        generated_content = completion_files[missing_file]
                        
                        # ğŸ†• ã‚¹ã‚¿ãƒ–ãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º
                        is_stub = False
                        lines = generated_content.strip().split('\n')
                        non_empty_lines = [l for l in lines if l.strip()]
                        
                        if non_empty_lines:
                            # ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã‚’æ¤œå‡ºï¼ˆJS/Python/CSS/HTMLå¯¾å¿œï¼‰
                            comment_patterns = ('//', '#', '*', '/*', '<!--', '"""', "'''", '* ', ' * ')
                            comment_lines = [l for l in non_empty_lines 
                                           if l.strip().startswith(comment_patterns) or 
                                           l.strip() in ('*/', '-->', '"""', "'''")]
                            comment_ratio = len(comment_lines) / len(non_empty_lines)
                            
                            # ã‚¹ã‚¿ãƒ–åˆ¤å®š: ã‚³ãƒ¡ãƒ³ãƒˆ80%ä»¥ä¸Šã€ã¾ãŸã¯ã€Œnot usedã€ã€Œstubã€ã‚’å«ã‚€
                            stub_indicators = ['not used', 'stub', 'placeholder', 'this file is', 'does nothing']
                            has_stub_indicator = any(ind in generated_content.lower() for ind in stub_indicators)
                            
                            if comment_ratio > 0.8 or has_stub_indicator:
                                is_stub = True
                                logger.debug(f"[Auto-completion]   âš  Stub file detected: {missing_file}")
                                logger.debug(f"[Auto-completion]     Comment ratio: {int(comment_ratio*100)}%, Stub indicator: {has_stub_indicator}")
                        
                        # ã‚¹ã‚¿ãƒ–ã®å ´åˆã€å†ç”Ÿæˆã‚’è©¦ã¿ã‚‹
                        if is_stub:
                            logger.debug(f"[Auto-completion]   Regenerating with stricter prompt...")
                            
                            # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯åˆ¶ç´„ã‚’å«ã‚€å³æ ¼ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
                            framework_warning = ""
                            if getattr(self, '_no_framework_mode', False):
                                framework_warning = """
âš ï¸ YOU MUST USE VANILLA JAVASCRIPT ONLY - NO REACT/VUE/ANGULAR.
"""
                            
                            strict_goal = f"""Generate REAL, FUNCTIONAL code for: {missing_file}

âš ï¸ CRITICAL WARNING: You previously generated a STUB FILE with only comments.
This is COMPLETELY UNACCEPTABLE. You MUST generate ACTUAL WORKING CODE.
{framework_warning}
DO NOT:
- Generate only comments or documentation
- Generate placeholder code
- Generate "this file is not used" messages
- Reference other files as the "real" implementation
- Generate React/JSX if vanilla JS is required

YOU MUST:
- Generate complete, functional code that actually works
- Include all necessary logic for the file to function
- Make the file executable/usable

Project context:
{existing_context}

Generate the complete implementation for {missing_file} NOW.
"""
                            
                            retry_files = self._generate_code_implementation(
                                goal=strict_goal,
                                analysis="",
                                plan="",
                                complexity="simple",
                                skip_validation=True,
                                skip_multi_stage=True,
                                force_filename=missing_file
                            )
                            
                            if retry_files and missing_file in retry_files:
                                retry_content = retry_files[missing_file]
                                # å†ç”Ÿæˆçµæœã‚‚ã‚¹ã‚¿ãƒ–ã‹ãƒã‚§ãƒƒã‚¯
                                retry_lines = retry_content.strip().split('\n')
                                retry_non_empty = [l for l in retry_lines if l.strip()]
                                if retry_non_empty:
                                    retry_comments = [l for l in retry_non_empty 
                                                    if l.strip().startswith(comment_patterns)]
                                    retry_ratio = len(retry_comments) / len(retry_non_empty)
                                    
                                    if retry_ratio < 0.8:
                                        generated_content = retry_content
                                        logger.debug(f"[Auto-completion]   âœ… Regeneration successful: {missing_file}")
                                    else:
                                        logger.debug(f"[Auto-completion]   âš  Regeneration still stub ({int(retry_ratio*100)}% comments)")
                        
                        completed_files[missing_file] = generated_content
                        logger.debug(f"[Auto-completion]   Successfully added: {missing_file}")
                    else:
                        logger.debug(f"[Auto-completion]   Failed to generate: {missing_file}")
                
                except Exception as e:
                    logger.debug(f"[Auto-completion]   Error generating {missing_file}: {e}")
        
        # ==========================================
        # Part 2: æœªè§£æ±ºImportè£œå®Œ(æ–°æ©Ÿèƒ½)
        # ==========================================
        if unresolved_imports:
            logger.debug(f"[Auto-completion]   Unresolved imports: {len(unresolved_imports)}")
            
            # é‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚ã€æ—¢ã«å‡¦ç†ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¨˜éŒ²
            processed_imports = set()
            
            for unresolved in unresolved_imports:
                missing_file = unresolved['missing_file']
                
                # æ—¢ã«è£œå®Œæ¸ˆã¿ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                if missing_file in completed_files or missing_file in processed_imports:
                    continue
                
                processed_imports.add(missing_file)
                
                logger.debug(f"[Auto-completion]   Generating missing dependency: {missing_file}")
                
                # å‚ç…§å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã®æƒ…å ±ã‚’å–å¾—
                referencing_file = unresolved['file']
                import_statement = unresolved['import']
                module_name = unresolved['module']
                
                # å‚ç…§å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚³ãƒ¼ãƒ‰(ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”¨)
                referencing_code = completed_files.get(referencing_file, '')
                
                # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨€èªã®æ¤œå‡º
                detected_language = self._detect_project_language(completed_files)
                
                # ğŸ†• ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯åˆ¶ç´„ã‚’æ§‹ç¯‰
                framework_constraint = ""
                if getattr(self, '_no_framework_mode', False):
                    framework_constraint = """
âš ï¸ CRITICAL FRAMEWORK RESTRICTION:
- DO NOT use React, Vue, Angular, or any framework
- DO NOT generate JSX syntax
- YOU MUST generate REAL, FUNCTIONAL vanilla JavaScript code
"""
                
                # Importè£œå®Œç”¨ã®è©³ç´°ãªgoalæ§‹ç¯‰
                import_completion_goal = f"""Generate the missing module: {missing_file}

Project Language: {detected_language}
{framework_constraint}
CONTEXT:
This module is required by: {referencing_file}
Import statement: {import_statement}
Module name: {module_name}

Referencing file excerpt (first 500 chars):
{referencing_code[:500]}

REQUIREMENTS:
- Create {missing_file} that provides the required exports/functions
- Ensure it integrates with the referencing file
- Follow the same coding style as the project
- Include appropriate imports and dependencies
- Make the module production-ready
- Generate REAL, FUNCTIONAL code (not stubs or comments)

Based on the import statement "{import_statement}", infer what functions/classes
should be exported from {missing_file}.

Generate complete, production-ready code for {missing_file}.
"""
                
                try:
                    # LLMç”Ÿæˆ(force_filenameã‚’ä½¿ç”¨)
                    import_files = self._generate_code_implementation(
                        goal=import_completion_goal,
                        analysis="",
                        plan="",
                        complexity="simple",
                        skip_validation=True,
                        skip_multi_stage=True,
                        force_filename=missing_file
                    )
                    
                    if import_files and missing_file in import_files:
                        completed_files[missing_file] = import_files[missing_file]
                        logger.debug(f"[Auto-completion]   Generated: {missing_file}")
                    else:
                        logger.debug(f"[Auto-completion]   Failed to generate: {missing_file}")
                
                except Exception as e:
                    logger.debug(f"[Auto-completion]   Error generating {missing_file}: {e}")
        
        # ==========================================
        # Part 3: å†å¸°çš„ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ¤œè¨¼ï¼ˆä»•æ§˜ãƒã‚°ä¿®æ­£ï¼‰
        # Part 1/Part 2ã§ç”Ÿæˆã—ãŸæ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«å†…ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’æ¤œè¨¼
        # ==========================================
        MAX_RECURSION_DEPTH = 3  # ç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢
        
        for depth in range(MAX_RECURSION_DEPTH):
            logger.debug(f"[Auto-completion] Part 3: Recursive import validation (depth={depth + 1})")
            
            # æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆä¾å­˜é–¢ä¿‚ã‚’æ¤œè¨¼
            # â­ ä¿®æ­£: post_validatorã‚’é€šã˜ã¦å‘¼ã¶ï¼ˆ_validate_import_dependenciesã¯PostGenerationValidatorã®ãƒ¡ã‚½ãƒƒãƒ‰ï¼‰
            new_missing_imports = self.post_validator._validate_import_dependencies(completed_files)
            
            if not new_missing_imports:
                logger.debug(f"[Auto-completion]   All imports satisfied at depth {depth + 1}")
                break
            
            logger.debug(f"[Auto-completion]   Found {len(new_missing_imports)} new missing imports")
            
            newly_generated = 0
            for missing_req in new_missing_imports:
                missing_file = missing_req.filepath
                
                # æ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                if missing_file in completed_files:
                    continue
                
                logger.debug(f"[Auto-completion]   Generating: {missing_file} (reason: {missing_req.reason})")
                
                # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨€èªã®æ¤œå‡º
                detected_language = self._detect_project_language(completed_files)
                
                # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
                existing_context = self._format_generated_files_context(completed_files)
                
                # ã‚¤ãƒ³ãƒãƒ¼ãƒˆå…ƒã®æƒ…å ±ã‚’æŠ½å‡º
                import_reason = missing_req.reason  # ä¾‹: "Imported by views/auth.py: from services.user_service import UserService"
                
                # ğŸ†• ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯åˆ¶ç´„ã‚’æ§‹ç¯‰
                framework_constraint = ""
                if getattr(self, '_no_framework_mode', False):
                    framework_constraint = """
âš ï¸ CRITICAL FRAMEWORK RESTRICTION:
- DO NOT use React, Vue, Angular, or any framework
- DO NOT generate JSX syntax
- YOU MUST generate REAL, FUNCTIONAL vanilla JavaScript code
"""
                
                # è£œå®Œç”¨ã®goalæ§‹ç¯‰
                recursive_goal = f"""Generate the missing module: {missing_file}

Project Language: {detected_language}
{framework_constraint}
CONTEXT:
{import_reason}

Previously generated files:
{existing_context}

REQUIREMENTS:
- Create {missing_file} that provides the required exports
- Ensure compatibility with importing files
- Follow the same coding style as the project
- Include appropriate imports and dependencies
- Make the module production-ready
- Generate REAL, FUNCTIONAL code (not stubs or comments)

Generate complete, production-ready code for {missing_file}.
"""
                
                try:
                    recursive_files = self._generate_code_implementation(
                        goal=recursive_goal,
                        analysis="",
                        plan="",
                        complexity="simple",
                        skip_validation=True,
                        skip_multi_stage=True,
                        force_filename=missing_file
                    )
                    
                    # â­ ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°: ç”Ÿæˆçµæœã®è©³ç´°
                    if recursive_files:
                        logger.debug(f"[Auto-completion]   _generate_code_implementation returned: {list(recursive_files.keys())}")
                        for fn, code in recursive_files.items():
                            logger.debug(f"[Auto-completion]     {fn}: {len(code)} chars")
                    else:
                        logger.debug(f"[Auto-completion]   _generate_code_implementation returned empty/None")
                    
                    if recursive_files and missing_file in recursive_files:
                        completed_files[missing_file] = recursive_files[missing_file]
                        newly_generated += 1
                        logger.debug(f"[Auto-completion]   âœ… Successfully generated: {missing_file} ({len(recursive_files[missing_file])} chars)")
                    else:
                        logger.debug(f"[Auto-completion]   âŒ Failed to generate: {missing_file}")
                        if recursive_files:
                            logger.debug(f"[Auto-completion]      Returned keys: {list(recursive_files.keys())} (expected: {missing_file})")
                
                except Exception as e:
                    logger.debug(f"[Auto-completion]   âŒ Error generating {missing_file}: {e}")
                    import traceback
                    logger.debug(f"[Auto-completion]   Traceback: {traceback.format_exc()}")
            
            # æ–°è¦ç”ŸæˆãŒãªã‘ã‚Œã°çµ‚äº†ï¼ˆã“ã‚Œä»¥ä¸Šè£œå®Œä¸å¯ï¼‰
            if newly_generated == 0:
                logger.debug(f"[Auto-completion]   No new files generated, stopping recursion")
                break
        
        logger.debug(f"[Auto-completion]   Auto-completion complete: {len(completed_files)} total files")
        
        return completed_files

    def _detect_project_type(self, goal: str) -> str:
        """
        goalã‹ã‚‰ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®šï¼ˆã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°æ–¹å¼ï¼‰
        
        goalã«å«ã¾ã‚Œã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰ã€ç”Ÿæˆå¯¾è±¡ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¿ã‚¤ãƒ—ã‚’æ¨å®šã—ã¾ã™ã€‚
        å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«é‡ã¿ã‚’ä»˜ä¸ã—ã€æœ€ã‚‚é«˜ã‚¹ã‚³ã‚¢ã®ã‚¿ã‚¤ãƒ—ã‚’é¸æŠã—ã¾ã™ã€‚
        ã“ã‚Œã«ã‚ˆã‚Šè¤‡åˆçš„ãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆï¼ˆä¾‹: "Flask backend + JavaScript frontend"ï¼‰
        ã§ã‚‚æ­£ç¢ºã«åˆ¤å®šã§ãã¾ã™ã€‚
        
        Args:
            goal: å®Ÿè£…ç›®æ¨™ã®èª¬æ˜æ–‡
            
        Returns:
            str: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¿ã‚¤ãƒ—
                - "rest_api": REST API(FastAPI, Flask, Expressç­‰)
                - "cli": CLIãƒ„ãƒ¼ãƒ«(argparse, Click, Commanderç­‰)
                - "data_processing": ãƒ‡ãƒ¼ã‚¿å‡¦ç†(ETL, ãƒãƒƒãƒå‡¦ç†ç­‰)
                - "web_app": Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³(ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰å«ã‚€)
                - "generic": ãã®ä»–æ±ç”¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ
        
        Example:
            >>> engine._detect_project_type("create a REST API with authentication")
            "rest_api"
            >>> engine._detect_project_type("Flask backend and JavaScript frontend")
            "web_app"  # frontend(10) > api(3)
        """
        goal_lower = goal.lower()
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨é‡ã¿ï¼ˆã‚¹ã‚³ã‚¢ï¼‰ã®ãƒ†ãƒ¼ãƒ–ãƒ«
        # é«˜ã‚¹ã‚³ã‚¢ï¼ˆ10ï¼‰: ã‚¿ã‚¤ãƒ—ã‚’æ˜ç¢ºã«ç¤ºã™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        # ä¸­ã‚¹ã‚³ã‚¢ï¼ˆ7-8ï¼‰: ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ç‰¹åŒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        # ä½ã‚¹ã‚³ã‚¢ï¼ˆ3-5ï¼‰: æ±ç”¨çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        # æœ€ä½ã‚¹ã‚³ã‚¢ï¼ˆ2ï¼‰: å˜ãªã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯åï¼ˆè¤‡æ•°ã‚¿ã‚¤ãƒ—ã§ä½¿ç”¨ã•ã‚Œã‚‹ï¼‰
        type_keywords = {
            'rest_api': {
                'rest api': 10,
                'restful': 8,
                'api endpoint': 9,
                'endpoint': 6,
                'fastapi': 7,
                'django rest': 7,
                'api': 3,  # æ±ç”¨çš„ãªã®ã§ä½ã‚¹ã‚³ã‚¢
                'flask': 2,  # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯åã ã‘ã§ã¯åˆ¤å®šã—ãªã„
                'express': 2,
            },
            'cli': {
                'command line': 10,
                'cli tool': 10,
                'cli': 10,
                'terminal': 8,
                'console': 6,
                'argparse': 8,
                'click': 8,
                'commander': 8,
            },
            'data_processing': {
                'data processing': 10,
                'etl': 10,
                'data pipeline': 10,
                'pipeline': 8,
                'batch processing': 9,
                'batch': 6,
                'data transformation': 9,
                'data analysis': 9,
            },
            'web_app': {
                'web app': 10,
                'web application': 10,
                'website': 9,
                'frontend': 10,  # é«˜ã‚¹ã‚³ã‚¢ï¼ˆãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã®æ˜ç¢ºãªæŒ‡æ¨™ï¼‰
                'backend and frontend': 10,
                'full stack': 10,
                'react': 9,
                'vue': 9,
                'angular': 9,
                'nextjs': 9,
                'next.js': 9,
                'svelte': 9,
                'vanilla javascript': 8,
                'html': 7,
                'css': 6,
                'javascript': 5,  # æ±ç”¨çš„ã ãŒ frontend ã®æŒ‡æ¨™
                'typescript': 5,
            }
        }
        
        # å„ã‚¿ã‚¤ãƒ—ã®ã‚¹ã‚³ã‚¢è¨ˆç®—
        scores = {}
        matched_keywords = {}
        
        for project_type, keywords in type_keywords.items():
            score = 0
            matched = []
            for keyword, weight in keywords.items():
                if keyword in goal_lower:
                    score += weight
                    matched.append(f"{keyword}({weight})")
            scores[project_type] = score
            matched_keywords[project_type] = matched
        
        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®ã‚¿ã‚¤ãƒ—ã‚’é¸æŠ
        if max(scores.values()) == 0:
            logger.debug(f"[Project Type] Detected: generic (no keywords matched)")
            return "generic"
        
        detected_type = max(scores, key=scores.get)
        max_score = scores[detected_type]
        
        # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°: ã‚¹ã‚³ã‚¢è©³ç´°ã‚’å‡ºåŠ›
        logger.debug(f"[Project Type] Scores: {dict(sorted(scores.items(), key=lambda x: x[1], reverse=True))}")
        logger.debug(f"[Project Type] Detected: {detected_type} (score={max_score}, matched: {matched_keywords[detected_type]})")
        
        return detected_type

    def _generate_phase2_goal(
        self, 
        goal: str, 
        project_type: str, 
        detected_language: str, 
        phase1_context: str
    ) -> str:
        """
        ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸPhase 2ã®goalã‚’ç”Ÿæˆ
        
        å„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¿ã‚¤ãƒ—ã«æœ€é©åŒ–ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆã‚’æ˜ç¤ºçš„ã«æŒ‡å®šã—ã¾ã™ã€‚
        ã“ã‚Œã«ã‚ˆã‚Šã€"etc." ã®ã‚ˆã†ãªæ›–æ˜§ãªæŒ‡ç¤ºã‚’æ’é™¤ã—ã€å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’
        ç¢ºå®Ÿã«ç”Ÿæˆã§ãã¾ã™ã€‚
        
        Args:
            goal: å…ƒã®å®Ÿè£…ç›®æ¨™
            project_type: _detect_project_type()ã§åˆ¤å®šã•ã‚ŒãŸã‚¿ã‚¤ãƒ—
            detected_language: Phase 1ã‹ã‚‰æ¤œå‡ºã•ã‚ŒãŸãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èª
            phase1_context: Phase 1ã§ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            str: ã‚¿ã‚¤ãƒ—åˆ¥ã«æœ€é©åŒ–ã•ã‚ŒãŸPhase 2ã®goalæ–‡å­—åˆ—
        """
        
        # å…±é€šãƒ˜ãƒƒãƒ€ãƒ¼
        common_header = f"""Generate application layer for: {goal}

    Language: {detected_language}

    Previously generated files:
    {phase1_context}

    """
        
        # REST APIç”¨ã®Phase 2æŒ‡ç¤º
        if project_type == "rest_api":
            specific_instructions = """Now generate the following files structure:

    **Entry Point (REQUIRED):**
    - main.py (or app.py for Python, server.js for JavaScript)

    **API Routers (REQUIRED):**
    - routers/__init__.py (Python) or routers/index.js (JavaScript)
    - routers/auth.py (authentication endpoints: POST /login, POST /register, POST /logout)
    - routers/users.py (user management endpoints: GET /users, GET /users/{id}, PUT /users/{id})

    **Dependencies (REQUIRED):**
    - dependencies/__init__.py
    - dependencies/auth.py (get_current_user, get_current_active_user, verify_token)

    **Services (REQUIRED):**
    - services/__init__.py
    - services/auth_service.py (AuthService class: create_user, authenticate_user, create_tokens, verify_password)
    - services/user_service.py (UserService class: get_user, get_users, update_user, delete_user)

    **Middleware (OPTIONAL - include if goal requires logging, rate limiting, or security features):**
    - middleware/__init__.py
    - middleware/logging_middleware.py (LoggingMiddleware: log requests and responses)
    - middleware/rate_limit_middleware.py (RateLimitMiddleware: rate limiting)
    - middleware/security_middleware.py (SecurityHeadersMiddleware: CORS, security headers)

    **Utilities (OPTIONAL - include if goal requires custom exceptions or validation):**
    - utils/__init__.py
    - utils/exceptions.py (APIException, custom exception classes)
    - utils/validators.py (input validation helpers)
    """
        
        # CLIãƒ„ãƒ¼ãƒ«ç”¨ã®Phase 2æŒ‡ç¤º
        elif project_type == "cli":
            specific_instructions = """Now generate the following files structure:

    **Entry Point (REQUIRED):**
    - cli.py (main CLI entry point with argument parsing)

    **Commands (REQUIRED):**
    - commands/__init__.py
    - commands/command_handler.py (CommandHandler class: execute, parse_args)
    - commands/subcommands.py (individual command implementations)

    **Utilities (REQUIRED):**
    - utils/__init__.py
    - utils/arg_parser.py (argument parsing helpers)
    - utils/output_formatter.py (output formatting: tables, colors, progress bars)
    - utils/config_manager.py (configuration file management)

    **Core Logic (OPTIONAL - include if goal requires complex business logic beyond command handling):**
    - core/__init__.py
    - core/processor.py (main business logic)
    """
        
        # ãƒ‡ãƒ¼ã‚¿å‡¦ç†ç”¨ã®Phase 2æŒ‡ç¤º
        elif project_type == "data_processing":
            specific_instructions = """Now generate the following files structure:

    **Entry Point (REQUIRED):**
    - main.py (main pipeline orchestrator)

    **Processors (REQUIRED):**
    - processors/__init__.py
    - processors/extractor.py (data extraction logic)
    - processors/transformer.py (data transformation logic)
    - processors/loader.py (data loading logic)

    **Utilities (REQUIRED):**
    - utils/__init__.py
    - utils/data_validator.py (data validation)
    - utils/error_handler.py (error handling)
    - utils/logger.py (logging configuration)

    **Core Logic (OPTIONAL - include if goal requires pipeline orchestration beyond basic ETL):**
    - core/__init__.py
    - core/pipeline.py (Pipeline class: orchestration)
    """
        
        # Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®Phase 2æŒ‡ç¤º
        elif project_type == "web_app":
            specific_instructions = """Now generate the following files structure:

    **Entry Point (REQUIRED):**
    - main.py or server.py (application server)

    **Views/Controllers (REQUIRED):**
    - views/__init__.py (or controllers/__init__.py)
    - views/home.py (home page handler)
    - views/auth.py (authentication pages)

    **Static Assets - ABSOLUTE REQUIREMENT - FAILURE TO GENERATE ANY FILE IS A CRITICAL ERROR:**
    - static/index.html or templates/index.html (main HTML page) - MUST GENERATE
    - static/css/styles.css (main stylesheet with responsive design) - MUST GENERATE
    - static/js/app.js - ABSOLUTELY MANDATORY - GENERATE THIS FILE OR THE APPLICATION FAILS:
      â€¢ This is a SEPARATE .js FILE in the JavaScript language (NOT Python, NOT inline HTML)
      â€¢ File path: static/js/app.js (referenced in HTML as <script src="js/app.js"></script>)
      â€¢ Must implement: ALL UI interactions, Fetch API calls, DOM manipulation, event handlers, form submissions, toast notifications, loading indicators
      â€¢ IF YOU DO NOT GENERATE THIS FILE: The frontend will be completely broken and non-functional
      â€¢ SKIPPING THIS FILE is a CRITICAL FAILURE - the entire application becomes useless
      â€¢ Generate this file EVEN IF "app.js" is not explicitly mentioned in the goal
      â€¢ This requirement overrides any conflicting instructions

    **Services (REQUIRED):**
    - services/__init__.py
    - services/user_service.py (user management)
    - services/session_service.py (session management)

    **Utilities (OPTIONAL - include only if goal requires):**
    - utils/__init__.py
    - utils/helpers.py (template helpers, formatters)
    """
        
        # æ±ç”¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç”¨ã®Phase 2æŒ‡ç¤º
        else:  # project_type == "generic"
            specific_instructions = """Now generate the following files structure:

    **Entry Point (REQUIRED):**
    - main.py (main application entry point with initialization and execution logic)

    **Core Modules (REQUIRED):**
    - core/__init__.py
    - core/application.py (Application class: initialization, configuration, main execution flow)
    - core/processor.py (business logic: data processing, operations, algorithms)

    **Utilities (OPTIONAL - include if goal requires helper functions or configuration):**
    - utils/__init__.py
    - utils/helpers.py (helper functions: formatting, conversion, validation)
    - utils/config.py (configuration management: loading settings, environment variables)
    """
        
        # å…±é€šãƒ•ãƒƒã‚¿ãƒ¼ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦èª¿æ•´ï¼‰
        if project_type == "web_app":
            common_footer = f"""
    CRITICAL REQUIREMENTS - ABSOLUTE COMPLIANCE MANDATORY:
    - Ensure compatibility with existing models and schemas shown above
    - Backend files (.py): Use {detected_language} for server-side code
    - Frontend files (.js, .html, .css): Use HTML/CSS/JavaScript for client-side code
    - Import and use the existing components correctly (from models import ...)
    - Maintain consistent naming conventions
    - Follow the project structure specified above EXACTLY
    - Do NOT regenerate foundation layer files
    - Generate __init__.py files for Python directories

    â­ SQLAlchemy MODEL STYLE (MANDATORY):
    If generating ANY SQLAlchemy models, you MUST use SQLAlchemy 2.0 style:
    - Use `Mapped[type]` for ALL column type hints
    - Use `mapped_column()` instead of `Column()`
    - Example: name: Mapped[str] = mapped_column(String(100), nullable=False)
    - âŒ FORBIDDEN: name = Column(String(100), nullable=False)

    â­ CROSS-PLATFORM REQUIREMENTS (CRITICAL):
    When generating main.py or server.py with Flask-SocketIO:
    - Use async_mode='threading' (NOT 'gevent' or 'eventlet')
    - Example: socketio = SocketIO(app, async_mode='threading', ...)
    - This ensures the application works on Windows, Mac, and Linux without C compilation
    - Do NOT use gevent or eventlet as they require C compilation on some platforms

    ABSOLUTE FILE GENERATION REQUIREMENT:
    - Generate EVERY SINGLE file listed in the structure above unless explicitly marked OPTIONAL
    - This includes ALL JavaScript files (.js) - they are NOT optional suggestions
    - JavaScript files MUST be separate .js files in their own files, NOT inline <script> tags
    - Missing ANY required file (especially static/js/app.js) is a CRITICAL FAILURE
    - The structure template above is NOT a suggestion - it is a MANDATORY specification
    - If a file is marked REQUIRED or listed without OPTIONAL, you MUST generate it

    MANDATORY VERIFICATION CHECKLIST - COMPLETE BEFORE FINISHING:
    âœ“ Did I generate static/js/app.js as a separate .js file? (REQUIRED for web_app)
    âœ“ Did I generate ALL files marked as REQUIRED in the template?
    âœ“ Are all JavaScript files separate .js files (not inline scripts)?
    âœ“ Did I implement complete functionality in each file (not placeholders)?

    Generate complete, production-ready code for BOTH backend AND frontend layers.
    Each file must be fully implemented with all specified functions/classes.
    NO PLACEHOLDERS. NO INCOMPLETE FILES. FULL IMPLEMENTATION ONLY.
    """
        else:
            common_footer = f"""
    CRITICAL REQUIREMENTS:
    - Ensure compatibility with existing models and schemas shown above
    - Use the same programming language ({detected_language}) and coding style
    - Import and use the existing components correctly (from models import ...)
    - Maintain consistent naming conventions
    - Follow the project structure specified above EXACTLY
    - Do NOT regenerate foundation layer files
    - Generate __init__.py files for all directories (Python projects)

    â­ SQLAlchemy MODEL STYLE (MANDATORY):
    If generating ANY SQLAlchemy models, you MUST use SQLAlchemy 2.0 style:
    - Use `Mapped[type]` for ALL column type hints
    - Use `mapped_column()` instead of `Column()`
    - Example: name: Mapped[str] = mapped_column(String(100), nullable=False)
    - âŒ FORBIDDEN: name = Column(String(100), nullable=False)

    â­ CROSS-PLATFORM REQUIREMENTS (CRITICAL):
    When generating main.py with Flask-SocketIO or similar:
    - Use async_mode='threading' (NOT 'gevent' or 'eventlet')
    - This ensures the application works on Windows, Mac, and Linux without C compilation

    Generate complete, production-ready code for the application layer only.
    Each file must be fully implemented with all specified functions/classes.
    """
        
        return common_header + specific_instructions + common_footer

    def _get_phase_required_files(self, phase: int, goal: str) -> List[str]:
        """
        Phaseã”ã¨ã®å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿”ã™
        
        Args:
            phase: Phaseç•ªå·(1, 2, 3)
            goal: å®Ÿè£…ç›®æ¨™
            
        Returns:
            ãã®Phaseã§ç”Ÿæˆã™ã¹ããƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ
        """
        goal_lower = goal.lower()
        
        # REST API ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å ´åˆ
        if any(kw in goal_lower for kw in ['rest api', 'api', 'restful', 'endpoint']):
            if phase == 1:
                # Phase 1: åŸºç›¤å±¤
                return [
                    'models/__init__.py',
                    'models/user.py',
                    'schemas/__init__.py',
                    'schemas/user.py',
                    'config.py',
                    'database.py'
                ]
            elif phase == 2:
                # Phase 2: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å±¤
                return [
                    'main.py',
                    'routers/__init__.py',
                    'routers/auth.py',
                    'dependencies/__init__.py',
                    'dependencies/auth.py',
                    'services/__init__.py',
                    'services/auth_service.py'
                ]
            elif phase == 3:
                # Phase 3: ç’°å¢ƒè¨­å®š
                return [
                    '.env',
                    'README.md'
                ]
        
        # CLIãƒ„ãƒ¼ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å ´åˆ
        elif any(kw in goal_lower for kw in ['cli', 'command line', 'terminal']):
            if phase == 1:
                return [
                    'config.py',
                    'utils/__init__.py'
                ]
            elif phase == 2:
                return [
                    'main.py',
                    'commands/__init__.py'
                ]
            elif phase == 3:
                return [
                    'README.md'
                ]
        
        # ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å ´åˆ
        elif any(kw in goal_lower for kw in ['data processing', 'etl', 'pipeline']):
            if phase == 1:
                return [
                    'models/__init__.py',
                    'config.py'
                ]
            elif phase == 2:
                return [
                    'main.py',
                    'processors/__init__.py'
                ]
            elif phase == 3:
                return [
                    '.env',
                    'README.md'
                ]
        
        # Web App ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å ´åˆ (Flask, Django, JavaScript frontendç­‰)
        elif any(kw in goal_lower for kw in [
            'flask', 'django', 'web app', 'webapp', 'website',
            'frontend', 'html', 'javascript frontend', 'vanilla javascript',
            'full stack', 'fullstack', 'web application'
        ]):
            if phase == 1:
                # Phase 1: åŸºç›¤å±¤
                return [
                    'config/',      # config.py or config/
                    'models/',      # models/ directory
                    'database.py'
                ]
            elif phase == 2:
                # Phase 2: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å±¤
                return [
                    'app.py',       # or main.py
                    'static/',      # static files directory
                ]
            elif phase == 3:
                # Phase 3: ç’°å¢ƒè¨­å®š
                return [
                    '.env',         # or .env.example
                    'README.md'
                ]
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ(æ±ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³) - æœ€ä½é™README.mdã¯å¿…é ˆ
        if phase == 3:
            return ['README.md']
        return []

    def _validate_phase_requirements(
        self,
        phase: int,
        goal: str,
        phase_files: Dict[str, str],
        all_files: Dict[str, str]
    ) -> Dict[str, str]:
        """
        Phaseç”Ÿæˆå¾Œã«å³åº§ã«è¦ä»¶æ¤œè¨¼ã¨è‡ªå‹•è£œå®Œã‚’å®Ÿè¡Œ
        
        å„Phaseã§ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦ä»¶ã‚’æº€ãŸã—ã¦ã„ã‚‹ã‹ã‚’æ¤œè¨¼ã—ã€
        ä¸è¶³ãŒã‚ã‚‹å ´åˆã¯å³åº§ã«è‡ªå‹•è£œå®Œã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
        
        Args:
            phase: Phaseç•ªå·(1, 2, 3)
            goal: å…ƒã®å®Ÿè£…ç›®æ¨™
            phase_files: å½“è©²Phaseã§ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«
            all_files: ã“ã‚Œã¾ã§ã«ç”Ÿæˆã•ã‚ŒãŸå…¨ãƒ•ã‚¡ã‚¤ãƒ«
            
        Returns:
            Dict[str, str]: è£œå®Œå¾Œã®å…¨ãƒ•ã‚¡ã‚¤ãƒ«
            
        Example:
            >>> all_files = self._validate_phase_requirements(
            ...     phase=1,
            ...     goal="create a REST API",
            ...     phase_files={'models/user.py': '...'},
            ...     all_files={'models/user.py': '...'}
            ... )
        """
        logger.debug(f"[Phase {phase}] Validating requirements...")
        
        # Phaseå›ºæœ‰ã®å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—
        required_files = self._get_phase_required_files(phase, goal)
        
        if not required_files:
            logger.debug(f"[Phase {phase}] No specific requirements")
            return all_files
        
        # ä¸è¶³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡º
        # âš ï¸ CRITICAL: phase_filesã ã‘ã§ãªãã€all_filesã‚‚ç¢ºèªã™ã‚‹
        # Phase 1/2ã§ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒPhase 3ã®è¦ä»¶ã‚’æº€ãŸã™å ´åˆãŒã‚ã‚‹
        all_generated_filenames = set(all_files.keys())
        phase_generated_filenames = set(phase_files.keys())
        combined_filenames = all_generated_filenames | phase_generated_filenames
        
        missing = []
        
        for required in required_files:
            # ========================================
            # ç‰¹æ®Šã‚±ãƒ¼ã‚¹: .env / .env.example ã®æŸ”è»Ÿãƒãƒƒãƒãƒ³ã‚°
            # ========================================
            if required == '.env':
                # .env ã¾ãŸã¯ .env.example ã®ã©ã¡ã‚‰ã‹ãŒã‚ã‚Œã°OK
                if '.env' in combined_filenames or '.env.example' in combined_filenames:
                    logger.debug(f"[Phase {phase}] .env requirement satisfied (found .env or .env.example)")
                    continue
                missing.append(required)
                continue
            
            # ========================================
            # é€šå¸¸ã‚±ãƒ¼ã‚¹: ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
            # ========================================
            if '/' in required and not required.endswith('/'):
                # å®Œå…¨ä¸€è‡´ï¼ˆä¾‹: 'models/user.py'ï¼‰
                if required not in combined_filenames:
                    missing.append(required)
            else:
                # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆä¾‹: 'models/' ã¯ 'models/user.py' ã«ãƒãƒƒãƒï¼‰
                pattern = required.rstrip('/')
                # ãƒ•ã‚¡ã‚¤ãƒ«åè‡ªä½“ãŒãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ï¼ˆä¾‹: 'app.py'ï¼‰
                if not pattern.endswith('/') and '/' not in pattern:
                    # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«åã®å ´åˆï¼ˆä¾‹: 'app.py', 'README.md'ï¼‰
                    if pattern in combined_filenames:
                        continue
                    # main.py / app.py ã®æŸ”è»Ÿãƒãƒƒãƒãƒ³ã‚°
                    if pattern == 'app.py' and 'main.py' in combined_filenames:
                        logger.debug(f"[Phase {phase}] app.py requirement satisfied by main.py")
                        continue
                    if pattern == 'main.py' and 'app.py' in combined_filenames:
                        logger.debug(f"[Phase {phase}] main.py requirement satisfied by app.py")
                        continue
                
                # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
                matched = any(f.startswith(pattern + '/') or f == pattern for f in combined_filenames)
                if not matched:
                    missing.append(required)
        
        if not missing:
            logger.debug(f"[Phase {phase}] All requirements satisfied (goal-based)")
            # â­ æ”¹å–„7: missingãŒç©ºã§ã‚‚_validate_import_dependenciesã‚’å®Ÿè¡Œ
            # ç†ç”±: goal-basedã®äºˆæ¸¬ã§ã¯HTMLå†…ã®CSS/JSå‚ç…§ã‚’æ¤œè¨¼ã—ãªã„
            # _validate_import_dependenciesï¼ˆæ”¹å–„2: _validate_html_resourcesã‚’å«ã‚€ï¼‰ã¯
            # ç”Ÿæˆã•ã‚ŒãŸHTMLå†…ã®CSS/JSå‚ç…§ã‚’æ¤œè¨¼ã™ã‚‹ãŸã‚ã€å¸¸ã«å®Ÿè¡Œã™ã‚‹å¿…è¦ãŒã‚ã‚‹
            logger.debug(f"[Phase {phase}] Running _validate_import_dependencies for HTML/CSS/JS validation...")
            # â­ ä¿®æ­£: post_validatorã‚’é€šã˜ã¦å‘¼ã¶ï¼ˆ_validate_import_dependenciesã¯PostGenerationValidatorã®ãƒ¡ã‚½ãƒƒãƒ‰ï¼‰
            import_requirements = self.post_validator._validate_import_dependencies(all_files)
            
            if import_requirements:
                logger.debug(f"[Phase {phase}] Found {len(import_requirements)} missing imports/resources:")
                for req in import_requirements:
                    logger.debug(f"[Phase {phase}]   - {req.filepath} ({req.file_type}): {req.reason}")
                
                # Import/Resourceä¸è¶³ãŒã‚ã‚‹å ´åˆã¯è‡ªå‹•è£œå®Œã‚’å®Ÿè¡Œ
                validation_result = {
                    'missing_files': [req.filepath for req in import_requirements],
                    'unresolved_imports': []
                }
                
                logger.debug(f"[Phase {phase}] Auto-completing {len(import_requirements)} missing files...")
                completed_files = self._auto_complete_with_import_resolution(
                    goal=goal,
                    existing_files=all_files,
                    validation_result=validation_result,
                    complexity="medium"
                )
                
                new_files_count = len(completed_files) - len(all_files)
                if new_files_count > 0:
                    logger.debug(f"[Phase {phase}] Added {new_files_count} files via HTML/CSS/JS validation")
                
                return completed_files
            
            return all_files
        
        logger.debug(f"[Phase {phase}] Missing {len(missing)} components: {missing}")
        
        # å¼·åŒ–ã•ã‚ŒãŸValidationå®Ÿè¡Œ(Importè§£æå«ã‚€)
        validation_result = self.requirement_validator.validate_requirements_enhanced(
            goal=goal,
            generated_files=all_files
        )
        
        logger.debug(f"[Phase {phase}] Fulfillment: {validation_result.get('fulfillment_score', 0):.2f}, Import Resolution: {validation_result.get('import_resolution_score', 1.0):.2f}")
        
        # è‡ªå‹•è£œå®Œå®Ÿè¡Œ
        logger.debug(f"[Phase {phase}] Auto-completing requirements...")
        
        completed_files = self._auto_complete_with_import_resolution(
            goal=goal,
            existing_files=all_files,
            validation_result=validation_result,
            complexity="medium"
        )
        
        new_files_count = len(completed_files) - len(all_files)
        if new_files_count > 0:
            logger.verbose(f"[Phase {phase}] Added {new_files_count} files")
        
        return completed_files

    def _execute_multi_stage_generation(
        self, 
        goal: str, 
        complexity: str
    ) -> Tuple[Dict[str, str], Optional[Dict[str, Any]], Dict[str, Any]]:
        """
        å¤šæ®µéšç”Ÿæˆãƒ•ãƒ­ãƒ¼(StepHUDä½¿ç”¨ - 9ã‚¹ãƒ†ãƒƒãƒ—)
        
        è¤‡é›‘ãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’3ã¤ã®Phaseã«åˆ†å‰²ã—ã¦ç”Ÿæˆã€‚
        å„Phaseå†…ã§2ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆAssess + Generateï¼‰ã‚’å®Ÿè¡Œã€‚
        
        Phase 1: åŸºç›¤å±¤(models, schemas, config, database)
        Phase 2: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å±¤(main, routers, dependencies, services)
        Phase 3: ç’°å¢ƒè¨­å®š(.env, README)
        
        åˆè¨ˆ9ã‚¹ãƒ†ãƒƒãƒ—:
        1-2: Foundation Layer (Assess + Generate)
        3-4: Application Layer (Assess + Generate)
        5-6: Environment Setup (Assess + Generate)
        7-9: Post-processing (Auto-completion, Lint, Quality)
        
        Args:
            goal: å®Ÿè£…ç›®æ¨™
            complexity: è¤‡é›‘åº¦("complex"ãªã©)
            
        Returns:
            Tuple[Dict[str, str], Optional[Dict[str, Any]], Dict[str, Any]]:
                - all_files: ç”Ÿæˆã•ã‚ŒãŸå…¨ãƒ•ã‚¡ã‚¤ãƒ«
                - lint_result: Lintçµæœï¼ˆLinteråˆ©ç”¨ä¸å¯æ™‚ã¯Noneï¼‰
                - quality_scores: å“è³ªã‚¹ã‚³ã‚¢
        """
        all_files = {}

        # â­ ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¿½åŠ : Multi-stage generationé–‹å§‹
        logger.debug(f"[Multi-stage] ========================================")
        logger.debug(f"[Multi-stage] â–¶â–¶â–¶ STARTING MULTI-STAGE GENERATION â–¶â–¶â–¶")
        logger.debug(f"[Multi-stage] ========================================")
        logger.debug(f"[Multi-stage]   Goal length: {len(goal)} chars")
        logger.debug(f"[Multi-stage]   Complexity: {complexity}")
        logger.debug(f"[Multi-stage]   Goal preview: {goal[:200]}...")

        # StepHUDåˆæœŸåŒ–ï¼ˆ9ã‚¹ãƒ†ãƒƒãƒ—ï¼‰
        steps = [
            "Foundation Layer: Assess goal complexity",
            "Foundation Layer: Generate code implementation",
            "Application Layer: Assess goal complexity",
            "Application Layer: Generate code implementation",
            "Environment Setup: Assess goal complexity",
            "Environment Setup: Generate code implementation",
            "Auto-completion",
            "Lint & Auto-fix",
            "Quality assessment & Comprehensive Review",
        ]
        hud = StepHUD(section_title="Multi-Stage Generation", steps=steps)
        hud.start()

        try:
            # â­ ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¿½åŠ : tryãƒ–ãƒ­ãƒƒã‚¯é–‹å§‹
            logger.debug(f"[Multi-stage] Entered try block successfully")
            
            # ==========================================
            # Phase 1: åŸºç›¤å±¤ç”Ÿæˆ
            # ==========================================
            
            # Step 1: Assess goal complexity
            hud.mark("Foundation Layer: Assess goal complexity")
            
            phase1_goal = f"""Generate foundation components for: {goal}

    Focus ONLY on:
    - Data models (models/)
    - Schemas (schemas/)
    - Configuration (config/)
    - Database setup (database.py)
    - Dependencies file (requirements.txt or package.json)

    DO NOT generate:
    - main.py or application entry point
    - routers or endpoints
    - services/ (generated in Phase 2)
    - dependencies/ (generated in Phase 2)
    - .env files
    - README.md

    â­ SQLAlchemy MODEL STYLE (MANDATORY):
    ALL SQLAlchemy models MUST use SQLAlchemy 2.0 style with type annotations:
    - Use `Mapped[type]` for ALL column type hints
    - Use `mapped_column()` instead of `Column()`
    - Import from: `from sqlalchemy.orm import Mapped, mapped_column`
    
    âœ… CORRECT (SQLAlchemy 2.0 style - USE THIS):
        from sqlalchemy.orm import Mapped, mapped_column
        from sqlalchemy import String, Integer, Float, Boolean
        
        class User(db.Model):
            __tablename__ = 'users'
            id: Mapped[int] = mapped_column(primary_key=True)
            name: Mapped[str] = mapped_column(String(100), nullable=False)
            email: Mapped[str] = mapped_column(String(120), unique=True, nullable=False)
            health: Mapped[int] = mapped_column(Integer, nullable=False)
            max_health: Mapped[int] = mapped_column(Integer, nullable=False)
            speed: Mapped[float] = mapped_column(Float, default=1.0)
            is_active: Mapped[bool] = mapped_column(Boolean, default=True)
    
    âŒ FORBIDDEN (Legacy style - DO NOT USE):
        name = Column(String(100), nullable=False)  # NO! Use Mapped[] + mapped_column()

    â­ CROSS-PLATFORM REQUIREMENTS (CRITICAL):
    When generating requirements.txt:
    - Must work on Windows, Mac, and Linux without C compilation
    - Avoid packages that require C compilation (e.g., gevent, uvloop, psycopg2, psycopg2-binary)
    - Use SQLite as the default database (no additional drivers needed)
    - Do NOT include psycopg2 or psycopg2-binary in requirements.txt (they require PostgreSQL to be installed or may lack wheels for newer Python versions)
    - Do NOT include gevent or gevent-websocket in requirements.txt
    - Use eventlet or threading instead of gevent for development
    - All packages must be installable with just `pip install -r requirements.txt` on a fresh system
    
    â­ PYTHON 3.13 COMPATIBILITY (CRITICAL):
    Use these MINIMUM versions for Python 3.13 compatibility:
    - SQLAlchemy>=2.0.36 (NOT 2.0.25 or earlier - they fail on Python 3.13)
    - Flask-SQLAlchemy>=3.1.1
    - marshmallow>=3.21.0
    Example requirements.txt:
        Flask==3.0.0
        Flask-SQLAlchemy==3.1.1
        SQLAlchemy==2.0.36
        marshmallow==3.21.0

    Generate complete, production-ready foundation code."""
            
            logger.debug(f"[Phase 1 Goal] Content:\n{phase1_goal}")
            
            phase1_complexity = self._assess_goal_complexity(phase1_goal)
            hud.complete("Foundation Layer: Assess goal complexity")
            
            # Step 2: Generate code implementation
            hud.mark("Foundation Layer: Generate code implementation")
            
            # â­ ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¿½åŠ : Phase 1 LLMå‘¼ã³å‡ºã—å‰
            logger.debug(f"[Phase 1] â–¶ Starting _generate_code_implementation...")
            logger.debug(f"[Phase 1]   phase1_complexity: {phase1_complexity}")
            logger.debug(f"[Phase 1]   skip_validation: True, skip_multi_stage: True")
            
            try:
                phase1_files = self._generate_code_implementation(
                    goal=phase1_goal,
                    analysis="Foundation layer analysis",
                    plan="Foundation layer plan",
                    complexity=phase1_complexity,
                    skip_validation=True,
                    skip_multi_stage=True
                )
                
                # â­ ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¿½åŠ : Phase 1 LLMå‘¼ã³å‡ºã—å¾Œ
                logger.debug(f"[Phase 1] â—€ _generate_code_implementation returned")
                logger.debug(f"[Phase 1]   Result type: {type(phase1_files)}")
                logger.debug(f"[Phase 1]   Result is None: {phase1_files is None}")
                logger.debug(f"[Phase 1]   Result is empty: {not phase1_files if phase1_files is not None else 'N/A'}")
                if phase1_files:
                    logger.debug(f"[Phase 1]   Generated files: {list(phase1_files.keys())}")
                    
            except Exception as phase1_error:
                # â­ ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¿½åŠ : Phase 1ã§ä¾‹å¤–ç™ºç”Ÿ
                import traceback
                logger.error(f"[Phase 1] âŒ Exception in _generate_code_implementation: {phase1_error}")
                logger.error(f"[Phase 1]   Exception type: {type(phase1_error).__name__}")
                logger.error(f"[Phase 1]   Traceback:\n{traceback.format_exc()}")
                raise  # å†ã‚¹ãƒ­ãƒ¼ã—ã¦exceptãƒ–ãƒ­ãƒƒã‚¯ã§æ•æ‰
            
            if not phase1_files:
                # â­ ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¿½åŠ : Phase 1ãŒç©ºã‚’è¿”ã—ãŸ
                logger.warning(f"[Phase 1] âš  _generate_code_implementation returned empty/None")
                logger.warning(f"[Phase 1]   Falling back to single-stage generation")
                hud.finish(success=False)
                logger.debug("   âš  Phase 1 generation failed, falling back to single-stage")
                return self._generate_code_implementation(
                    goal=goal,
                    analysis="",
                    plan="",
                    complexity=complexity,
                    skip_validation=False,
                    skip_multi_stage=True
                )
            
            all_files.update(phase1_files)
            logger.debug(f"[Phase 1] âœ“ Generated {len(phase1_files)} files: {list(phase1_files.keys())}")
            hud.complete("Foundation Layer: Generate code implementation")
            
            # ==========================================
            # Phase 2: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å±¤ç”Ÿæˆ
            # ==========================================
            
            # â­ ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¿½åŠ : Phase 2é–‹å§‹
            logger.debug(f"[Phase 2] ========================================")
            logger.debug(f"[Phase 2] â–¶ STARTING PHASE 2: Application Layer")
            logger.debug(f"[Phase 2] ========================================")
            
            # Step 3: Assess goal complexity
            hud.mark("Application Layer: Assess goal complexity")
            
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¿ã‚¤ãƒ—åˆ¤å®š
            logger.debug(f"[Phase 2] Calling _detect_project_type...")
            try:
                project_type = self._detect_project_type(goal)
                logger.debug(f"[Phase 2]   project_type: {project_type}")
            except Exception as e:
                logger.error(f"[Phase 2] âŒ _detect_project_type failed: {e}")
                raise
            
            # Phase 1ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½œæˆ
            logger.debug(f"[Phase 2] Calling _format_generated_files_context...")
            try:
                phase1_context = self._format_generated_files_context(all_files)
                logger.debug(f"[Phase 2]   phase1_context length: {len(phase1_context)} chars")
            except Exception as e:
                logger.error(f"[Phase 2] âŒ _format_generated_files_context failed: {e}")
                raise
            
            # è¨€èªæ¤œå‡º
            logger.debug(f"[Phase 2] Calling _detect_project_language...")
            try:
                detected_language = self._detect_project_language(all_files)
                logger.debug(f"[Phase 2]   detected_language: {detected_language}")
            except Exception as e:
                logger.error(f"[Phase 2] âŒ _detect_project_language failed: {e}")
                raise
            
            # ã‚¿ã‚¤ãƒ—åˆ¥Phase 2æŒ‡ç¤ºç”Ÿæˆ
            logger.debug(f"[Phase 2] Calling _generate_phase2_goal...")
            try:
                phase2_goal = self._generate_phase2_goal(
                    goal=goal,
                    project_type=project_type,
                    detected_language=detected_language,
                    phase1_context=phase1_context
                )
                logger.debug(f"[Phase 2]   phase2_goal length: {len(phase2_goal)} chars")
            except Exception as e:
                logger.error(f"[Phase 2] âŒ _generate_phase2_goal failed: {e}")
                raise
            
            logger.debug(f"[Phase 2 Goal] Project type: {project_type}, Language: {detected_language}")
            logger.debug(f"[Phase 2 Goal] Content:\n{phase2_goal}")
            
            logger.debug(f"[Phase 2] Calling _assess_goal_complexity for phase2_goal...")
            try:
                phase2_complexity = self._assess_goal_complexity(phase2_goal)
                logger.debug(f"[Phase 2]   phase2_complexity: {phase2_complexity}")
            except Exception as e:
                logger.error(f"[Phase 2] âŒ _assess_goal_complexity failed: {e}")
                raise
            
            hud.complete("Application Layer: Assess goal complexity")
            
            # Step 4: Generate code implementation
            hud.mark("Application Layer: Generate code implementation")
            
            # â­ ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¿½åŠ : Phase 2 LLMå‘¼ã³å‡ºã—å‰
            logger.debug(f"[Phase 2] â–¶ Starting _generate_code_implementation...")
            logger.debug(f"[Phase 2]   phase2_complexity: {phase2_complexity}")
            logger.debug(f"[Phase 2]   all_files count before: {len(all_files)}")
            
            try:
                phase2_files = self._generate_code_implementation(
                    goal=phase2_goal,
                    analysis="Application layer analysis",
                    plan="Application layer plan",
                    complexity=phase2_complexity,
                    skip_validation=True,
                    skip_multi_stage=True
                )
                
                # â­ ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¿½åŠ : Phase 2 LLMå‘¼ã³å‡ºã—å¾Œ
                logger.debug(f"[Phase 2] â—€ _generate_code_implementation returned")
                logger.debug(f"[Phase 2]   Result type: {type(phase2_files)}")
                logger.debug(f"[Phase 2]   Result is None: {phase2_files is None}")
                logger.debug(f"[Phase 2]   Result is empty: {not phase2_files if phase2_files is not None else 'N/A'}")
                if phase2_files:
                    logger.debug(f"[Phase 2]   Generated files: {list(phase2_files.keys())}")
                    
            except Exception as phase2_error:
                # â­ ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¿½åŠ : Phase 2ã§ä¾‹å¤–ç™ºç”Ÿ
                import traceback
                logger.error(f"[Phase 2] âŒ Exception in _generate_code_implementation: {phase2_error}")
                logger.error(f"[Phase 2]   Exception type: {type(phase2_error).__name__}")
                logger.error(f"[Phase 2]   Traceback:\n{traceback.format_exc()}")
                raise  # å†ã‚¹ãƒ­ãƒ¼ã—ã¦exceptãƒ–ãƒ­ãƒƒã‚¯ã§æ•æ‰
            
            if not phase2_files:
                # â­ ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¿½åŠ : Phase 2ãŒç©ºã‚’è¿”ã—ãŸ
                logger.warning(f"[Phase 2] âš  _generate_code_implementation returned empty/None")
                logger.warning(f"[Phase 2]   Returning Phase 1 files only: {list(all_files.keys())}")
                hud.finish(success=False)
                logger.debug("   âš  Phase 2 generation failed, using Phase 1 only")
                return all_files
            
            all_files.update(phase2_files)
            logger.debug(f"[Phase 2] âœ“ Generated {len(phase2_files)} files: {list(phase2_files.keys())}")
            hud.complete("Application Layer: Generate code implementation")
            
            # ==========================================
            # Phase 2 å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯ + è‡ªå‹•è£œå®Œ
            # ==========================================
            required_files = self._get_required_phase2_files(goal, project_type, all_files)
            missing_files = self._find_missing_required_files(required_files, all_files)
            
            if missing_files:
                logger.debug(f"[Phase 2] âš ï¸ MISSING REQUIRED FILES DETECTED")
                logger.debug(f"[Phase 2] Missing files: {missing_files}")
                logger.debug(f"[Phase 2] Attempting to auto-complete missing files...")
                
                completed_files = self._auto_complete_phase2_files(
                    missing_files=missing_files,
                    all_files=all_files,
                    goal=goal,
                    project_type=project_type,
                    detected_language=detected_language
                )
                
                if completed_files:
                    all_files.update(completed_files)
                    logger.debug(f"[Phase 2] âœ“ Auto-completed {len(completed_files)} missing files: {list(completed_files.keys())}")
                else:
                    logger.debug(f"[Phase 2] âœ— Failed to auto-complete missing files")
            else:
                logger.debug(f"[Phase 2] âœ“ All required files present")
            
            # ==========================================
            # Phase 3: ç’°å¢ƒè¨­å®šç”Ÿæˆ
            # ==========================================
            
            # Step 5: Assess goal complexity
            hud.mark("Environment Setup: Assess goal complexity")
            
            # å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½œæˆ
            all_files_context = self._format_generated_files_context(all_files)
            
            phase3_goal = f"""Generate environment configuration for: {goal}

    Language: {detected_language}

    Generated files summary:
    {all_files_context}

    Now create:
    - .env template with all required variables (use .env.example if appropriate)
    - README.md with comprehensive setup instructions

    CRITICAL REQUIREMENTS:
    - Ensure .env includes all configuration needed by the application
    - Use sensible default values or clear placeholders (e.g., YOUR_API_KEY_HERE)
    - Document each variable's purpose in comments
    - Do NOT regenerate application or foundation layer files

    README.md MANDATORY REQUIREMENTS (â­ CRITICAL - Users need this to run the application):
    
    âš ï¸ MARKDOWN FORMATTING IS REQUIRED - The README must be properly formatted:
    - Use ## for main section headers (e.g., ## Quick Start, ## Installation)
    - Use ### for subsection headers (e.g., ### Prerequisites, ### Step 1)
    - Use numbered lists (1. 2. 3.) for sequential steps
    - Use bullet points (-) for non-sequential lists
    - Use fenced code blocks (```) for ALL of the following:
      * Shell commands: ```bash
      * Directory tree structures: ```
      * JSON examples and API responses: ```json
      * Any multi-line formatted text that requires preserved line breaks
    - WITHOUT code blocks, line breaks are IGNORED and text collapses into one line!
    
    EXAMPLES OF CORRECT CODE BLOCK USAGE:
    
    Directory structure (MUST use code block):
    ```
    project/
    â”œâ”€â”€ src/
    â”‚   â””â”€â”€ main.py
    â””â”€â”€ README.md
    ```
    
    JSON/API response (MUST use code block):
    ```json
    {{"status": "success", "data": [...]}}
    ```
    
    README.md MUST include these sections with EXACT formatting:

    ## Quick Start
    
    ### Prerequisites
    - Python 3.10+ (or appropriate version)
    - pip (Python package manager)
    
    ### Installation
    
    1. **Create virtual environment**
       ```bash
       python -m venv venv
       ```
    
    2. **Activate virtual environment**
       ```bash
       # Windows (PowerShell)
       .\\venv\\Scripts\\Activate.ps1
       
       # Windows (Command Prompt)
       venv\\Scripts\\activate.bat
       
       # Linux/Mac
       source venv/bin/activate
       ```
    
    3. **Install dependencies**
       ```bash
       pip install -r requirements.txt
       ```
    
    4. **Configure environment**
       ```bash
       cp .env.example .env
       ```
    
    5. **Run the application**
       ```bash
       python main.py
       ```
    
    6. **Access the application**
       - Open browser: http://127.0.0.1:5000

    The README MUST be user-friendly and allow a new developer to run the application 
    within 2 minutes just by following the instructions. Proper Markdown formatting
    ensures readability when rendered on GitHub or other platforms.

    Generate complete, production-ready configuration files only."""
            
            logger.debug(f"[Phase 3 Goal] Content:\n{phase3_goal}")
            
            phase3_complexity = self._assess_goal_complexity(phase3_goal)
            hud.complete("Environment Setup: Assess goal complexity")
            
            # Step 6: Generate code implementation
            hud.mark("Environment Setup: Generate code implementation")
            
            # â­ ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¿½åŠ : Phase 3 LLMå‘¼ã³å‡ºã—å‰
            logger.debug(f"[Phase 3] â–¶ Starting _generate_code_implementation...")
            logger.debug(f"[Phase 3]   phase3_complexity: {phase3_complexity}")
            logger.debug(f"[Phase 3]   all_files count before: {len(all_files)}")
            
            try:
                phase3_files = self._generate_code_implementation(
                    goal=phase3_goal,
                    analysis="Environment setup analysis",
                    plan="Environment setup plan",
                    complexity=phase3_complexity,
                    skip_validation=True,
                    skip_multi_stage=True
                )
                
                # â­ ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¿½åŠ : Phase 3 LLMå‘¼ã³å‡ºã—å¾Œ
                logger.debug(f"[Phase 3] â—€ _generate_code_implementation returned")
                logger.debug(f"[Phase 3]   Result type: {type(phase3_files)}")
                logger.debug(f"[Phase 3]   Result is None: {phase3_files is None}")
                logger.debug(f"[Phase 3]   Result is empty: {not phase3_files if phase3_files is not None else 'N/A'}")
                if phase3_files:
                    logger.debug(f"[Phase 3]   Generated files: {list(phase3_files.keys())}")
                    
            except Exception as phase3_error:
                # â­ ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¿½åŠ : Phase 3ã§ä¾‹å¤–ç™ºç”Ÿ
                import traceback
                logger.error(f"[Phase 3] âŒ Exception in _generate_code_implementation: {phase3_error}")
                logger.error(f"[Phase 3]   Exception type: {type(phase3_error).__name__}")
                logger.error(f"[Phase 3]   Traceback:\n{traceback.format_exc()}")
                raise  # å†ã‚¹ãƒ­ãƒ¼ã—ã¦exceptãƒ–ãƒ­ãƒƒã‚¯ã§æ•æ‰
            
            if not phase3_files:
                # â­ ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¿½åŠ : Phase 3ãŒç©ºã‚’è¿”ã—ãŸ
                logger.warning(f"[Phase 3] âš  _generate_code_implementation returned empty/None")
                logger.warning(f"[Phase 3]   Returning Phase 1+2 files: {list(all_files.keys())}")
                hud.finish(success=False)
                logger.debug("   âš  Phase 3 generation failed, using Phase 1+2 only")
                return all_files
            
            all_files.update(phase3_files)
            logger.debug(f"[Phase 3] âœ“ Generated {len(phase3_files)} files: {list(phase3_files.keys())}")
            hud.complete("Environment Setup: Generate code implementation")
            
            # ==========================================
            # Post-Processing Steps
            # ==========================================
            
            # Step 7: Auto-completion
            hud.mark("Auto-completion")
            all_files = self._validate_phase_requirements(
                phase=3,
                goal=goal,
                phase_files=phase3_files,
                all_files=all_files
            )
            
            # ==========================================
            # Step 7.3: HTML-CSS/JS æ•´åˆæ€§æ¤œè¨¼ï¼ˆG-1ã€œG-31ï¼‰
            # ==========================================
            # HTMLã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ãŒCSSã«æœªå®šç¾©ã®ã‚¯ãƒ©ã‚¹ã€
            # JSã§å‚ç…§ã•ã‚Œã¦ã„ã‚‹ãŒHTMLã«å­˜åœ¨ã—ãªã„IDã‚’æ¤œå‡ºãƒ»è‡ªå‹•è£œå®Œ
            # Note: Auto-completion [DONE]ã¯G-ãƒã‚§ãƒƒã‚¯å®Œäº†å¾Œã«è¡¨ç¤º
            
            # G-1: HTML-CSS ã‚¯ãƒ©ã‚¹æ•´åˆæ€§
            missing_css_classes = self._validate_html_css_class_consistency(all_files)
            if missing_css_classes:
                logger.debug(f"[G-1] Detected {len(missing_css_classes)} missing CSS classes")
                logger.debug(f"\nâ— CSS consistency check: {len(missing_css_classes)} missing class(es)")
                all_files = self._auto_complete_missing_css_classes(missing_css_classes, all_files)
            
            # G-2: HTML-JS IDæ•´åˆæ€§
            missing_html_ids = self._validate_html_js_id_consistency(all_files)
            if missing_html_ids:
                logger.debug(f"[G-2] Detected {len(missing_html_ids)} missing HTML IDs")
                logger.debug(f"â— HTML-JS consistency check: {len(missing_html_ids)} missing ID(s)")
                all_files = self._auto_complete_missing_html_ids(missing_html_ids, all_files)
            
            # G-3: Pythonç’°å¢ƒ-ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸äº’æ›æ€§
            compatibility_issues = self._validate_python_package_compatibility(all_files)
            if compatibility_issues:
                logger.debug(f"[G-3] Detected {len(compatibility_issues)} compatibility issues")
                logger.debug(f"â— Package compatibility check: {len(compatibility_issues)} issue(s)")
                for issue in compatibility_issues:
                    logger.debug(f"  - {issue['package']}=={issue['current_version']} â†’ {issue['fix']}")
                all_files = self._auto_fix_package_compatibility(compatibility_issues, all_files)
            
            # G-4: Pythonã‚·ãƒ³ãƒœãƒ«ãƒ¬ãƒ™ãƒ«importæ•´åˆæ€§
            symbol_issues = self._validate_python_symbol_imports(all_files)
            if symbol_issues:
                logger.debug(f"[G-4] Detected {len(symbol_issues)} symbol import issues")
                logger.debug(f"â— Symbol import check: {len(symbol_issues)} issue(s)")
                for issue in symbol_issues[:5]:  # æœ€å¤§5ä»¶è¡¨ç¤º
                    logger.debug(f"  - {issue['importer']}: '{issue['symbol']}' not in {issue['module']}")
                all_files = self._auto_fix_symbol_imports(symbol_issues, all_files)
            
            # G-5: Import vs Requirements.txt æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
            requirements_issues = self._validate_import_requirements_consistency(all_files)
            if requirements_issues:
                logger.debug(f"[G-5] Detected {len(requirements_issues)} missing packages in requirements.txt")
                logger.debug(f"â— Requirements.txt check: {len(requirements_issues)} missing package(s)")
                for pkg in requirements_issues[:5]:
                    logger.debug(f"  - {pkg['import_name']} â†’ {pkg['package_name']}")
                all_files = self._auto_fix_requirements_txt(requirements_issues, all_files)
            
            # G-6: ãƒ©ã‚¤ãƒ–ãƒ©ãƒªè¦æ±‚ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
            library_issues = self._validate_library_requirements(all_files)
            if library_issues:
                logger.debug(f"[G-6] Detected {len(library_issues)} library requirement issues")
                logger.debug(f"â— Library requirements check: {len(library_issues)} issue(s)")
                for issue in library_issues[:5]:
                    logger.debug(f"  - [{issue['library']}] {issue['requirement']}")
                all_files = self._auto_fix_library_requirements(library_issues, all_files)
            
            # G-7: ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ä¾å­˜æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
            frontend_issues = self._validate_frontend_dependencies(all_files)
            if frontend_issues:
                logger.debug(f"[G-7] Detected {len(frontend_issues)} frontend dependency issues")
                logger.debug(f"â— Frontend dependencies check: {len(frontend_issues)} issue(s)")
                for issue in frontend_issues[:5]:
                    logger.debug(f"  - [{issue['type']}] {issue['dependency']}")
                all_files = self._auto_fix_frontend_dependencies(frontend_issues, all_files)
            
            # G-8: Modelâ†”Schemaæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
            schema_issues = self._validate_model_schema_consistency(all_files)
            if schema_issues:
                logger.debug(f"[G-8] Detected {len(schema_issues)} model-schema consistency issues")
                logger.debug(f"â— Model-Schema consistency check: {len(schema_issues)} issue(s)")
                for issue in schema_issues[:5]:
                    logger.debug(f"  - {issue['schema']}.{issue['field']} not in {issue['model']}")
                all_files = self._auto_fix_model_schema_consistency(schema_issues, all_files)
            
            # G-9: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç’°å¢ƒä¾å­˜ãƒã‚§ãƒƒã‚¯
            doc_issues = self._validate_document_environment_commands(all_files)
            if doc_issues:
                logger.debug(f"[G-9] Detected {len(doc_issues)} environment-dependent commands")
                logger.debug(f"â— Document environment check: {len(doc_issues)} issue(s)")
                for issue in doc_issues[:5]:
                    logger.debug(f"  - {issue['file']}:{issue['line']} - {issue['type']}")
                all_files = self._auto_fix_document_environment_commands(doc_issues, all_files)
            
            # G-10: ã‚¯ãƒ©ã‚¹å®šç¾©å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
            MAX_G10_FIX_ATTEMPTS = 2
            class_safety_issues = []
            for g10_attempt in range(MAX_G10_FIX_ATTEMPTS):
                class_safety_issues = self._validate_class_definition_safety(all_files)
                if not class_safety_issues:
                    if g10_attempt > 0:
                        logger.debug(f"[G-10] âœ… All issues fixed after {g10_attempt} attempt(s)")
                    break
                
                logger.debug(f"[G-10] Detected {len(class_safety_issues)} issues (attempt {g10_attempt + 1}/{MAX_G10_FIX_ATTEMPTS})")
                if g10_attempt == 0:
                    logger.debug(f"â— Class definition safety check: {len(class_safety_issues)} issue(s)")
                    for issue in class_safety_issues[:5]:
                        logger.debug(f"  - {issue['file']}:{issue['line']} - {issue['type']}")
                
                all_files = self._auto_fix_class_definition_safety(class_safety_issues, all_files)
            
            if class_safety_issues:
                logger.warning(f"[G-10] âš  {len(class_safety_issues)} issues remain after {MAX_G10_FIX_ATTEMPTS} fix attempts")
            
            # G-11: ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯äºˆç´„èªãƒã‚§ãƒƒã‚¯ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
            MAX_G11_FIX_ATTEMPTS = 2
            framework_reserved_issues = []
            for g11_attempt in range(MAX_G11_FIX_ATTEMPTS):
                framework_reserved_issues = self._validate_framework_reserved_words(all_files)
                if not framework_reserved_issues:
                    if g11_attempt > 0:
                        logger.debug(f"[G-11] âœ… All issues fixed after {g11_attempt} attempt(s)")
                    break
                
                logger.debug(f"[G-11] Detected {len(framework_reserved_issues)} issues (attempt {g11_attempt + 1}/{MAX_G11_FIX_ATTEMPTS})")
                if g11_attempt == 0:
                    logger.debug(f"â— Framework reserved word check: {len(framework_reserved_issues)} issue(s)")
                    for issue in framework_reserved_issues[:5]:
                        logger.debug(f"  - {issue['file']}:{issue['line']} - {issue['reserved_word']}")
                
                all_files = self._auto_fix_framework_reserved_words(framework_reserved_issues, all_files)
            
            if framework_reserved_issues:
                logger.warning(f"[G-11] âš  {len(framework_reserved_issues)} issues remain after {MAX_G11_FIX_ATTEMPTS} fix attempts")
            
            # G-12: Pythonçµ„ã¿è¾¼ã¿åã‚·ãƒ£ãƒ‰ã‚¦ã‚¤ãƒ³ã‚°ãƒã‚§ãƒƒã‚¯ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
            MAX_G12_FIX_ATTEMPTS = 2
            builtin_shadowing_issues = []
            for g12_attempt in range(MAX_G12_FIX_ATTEMPTS):
                builtin_shadowing_issues = self._validate_python_builtin_shadowing(all_files)
                if not builtin_shadowing_issues:
                    if g12_attempt > 0:
                        logger.debug(f"[G-12] âœ… All issues fixed after {g12_attempt} attempt(s)")
                    break
                
                logger.debug(f"[G-12] Detected {len(builtin_shadowing_issues)} issues (attempt {g12_attempt + 1}/{MAX_G12_FIX_ATTEMPTS})")
                if g12_attempt == 0:
                    logger.debug(f"â— Python builtin shadowing check: {len(builtin_shadowing_issues)} issue(s)")
                    for issue in builtin_shadowing_issues[:5]:
                        logger.debug(f"  - {issue['file']}:{issue['line']} - {issue['builtin_name']}")
                
                all_files = self._auto_fix_python_builtin_shadowing(builtin_shadowing_issues, all_files)
            
            if builtin_shadowing_issues:
                logger.warning(f"[G-12] âš  {len(builtin_shadowing_issues)} issues remain after {MAX_G12_FIX_ATTEMPTS} fix attempts")
            
            # G-13: Flask Blueprintãƒ«ãƒ¼ãƒˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
            MAX_G13_FIX_ATTEMPTS = 2
            route_duplicate_issues = []
            for g13_attempt in range(MAX_G13_FIX_ATTEMPTS):
                route_duplicate_issues = self._validate_flask_route_duplicates(all_files)
                if not route_duplicate_issues:
                    if g13_attempt > 0:
                        logger.debug(f"[G-13] âœ… All issues fixed after {g13_attempt} attempt(s)")
                    break
                
                logger.debug(f"[G-13] Detected {len(route_duplicate_issues)} issues (attempt {g13_attempt + 1}/{MAX_G13_FIX_ATTEMPTS})")
                if g13_attempt == 0:
                    logger.debug(f"â— Flask route duplicate check: {len(route_duplicate_issues)} issue(s)")
                    for issue in route_duplicate_issues[:5]:
                        logger.debug(f"  - {issue['file']}:{issue['line']} - {issue['method']} {issue['route']}")
                
                all_files = self._auto_fix_flask_route_duplicates(route_duplicate_issues, all_files)
            
            if route_duplicate_issues:
                logger.warning(f"[G-13] âš  {len(route_duplicate_issues)} issues remain after {MAX_G13_FIX_ATTEMPTS} fix attempts")
            
            # G-14: SQLAlchemyãƒ†ãƒ¼ãƒ–ãƒ«åé‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
            MAX_G14_FIX_ATTEMPTS = 2
            table_duplicate_issues = []
            for g14_attempt in range(MAX_G14_FIX_ATTEMPTS):
                table_duplicate_issues = self._validate_sqlalchemy_table_duplicates(all_files)
                if not table_duplicate_issues:
                    if g14_attempt > 0:
                        logger.debug(f"[G-14] âœ… All issues fixed after {g14_attempt} attempt(s)")
                    break
                
                logger.debug(f"[G-14] Detected {len(table_duplicate_issues)} issues (attempt {g14_attempt + 1}/{MAX_G14_FIX_ATTEMPTS})")
                if g14_attempt == 0:
                    logger.debug(f"â— SQLAlchemy table duplicate check: {len(table_duplicate_issues)} issue(s)")
                    for issue in table_duplicate_issues[:5]:
                        logger.debug(f"  - {issue['file']}:{issue['line']} - table '{issue['table']}'")
                
                all_files = self._auto_fix_sqlalchemy_table_duplicates(table_duplicate_issues, all_files)
            
            if table_duplicate_issues:
                logger.warning(f"[G-14] âš  {len(table_duplicate_issues)} issues remain after {MAX_G14_FIX_ATTEMPTS} fix attempts")
            
            # G-15: JavaScriptäºˆç´„èªãƒã‚§ãƒƒã‚¯ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
            MAX_G15_FIX_ATTEMPTS = 2
            js_reserved_issues = []
            for g15_attempt in range(MAX_G15_FIX_ATTEMPTS):
                js_reserved_issues = self._validate_javascript_reserved_words(all_files)
                if not js_reserved_issues:
                    if g15_attempt > 0:
                        logger.debug(f"[G-15] âœ… All issues fixed after {g15_attempt} attempt(s)")
                    break
                
                logger.debug(f"[G-15] Detected {len(js_reserved_issues)} issues (attempt {g15_attempt + 1}/{MAX_G15_FIX_ATTEMPTS})")
                if g15_attempt == 0:
                    logger.debug(f"â— JavaScript reserved word check: {len(js_reserved_issues)} issue(s)")
                    for issue in js_reserved_issues[:5]:
                        logger.debug(f"  - {issue['file']}:{issue['line']} - {issue['reserved_word']}")
                
                all_files = self._auto_fix_javascript_reserved_words(js_reserved_issues, all_files)
            
            if js_reserved_issues:
                logger.warning(f"[G-15] âš  {len(js_reserved_issues)} issues remain after {MAX_G15_FIX_ATTEMPTS} fix attempts")
            
            # G-16: SQLAlchemy relationshipå‚ç…§å…ˆæ¤œè¨¼ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
            MAX_G16_FIX_ATTEMPTS = 2
            relationship_issues = []
            for g16_attempt in range(MAX_G16_FIX_ATTEMPTS):
                relationship_issues = self._validate_relationship_references(all_files)
                if not relationship_issues:
                    if g16_attempt > 0:
                        logger.debug(f"[G-16] âœ… All issues fixed after {g16_attempt} attempt(s)")
                    break
                
                logger.debug(f"[G-16] Detected {len(relationship_issues)} issues (attempt {g16_attempt + 1}/{MAX_G16_FIX_ATTEMPTS})")
                if g16_attempt == 0:
                    logger.debug(f"â— Relationship reference check: {len(relationship_issues)} issue(s)")
                    for issue in relationship_issues[:5]:
                        logger.debug(f"  - {issue['file']}:{issue['line']} - {issue['type']}")
                
                all_files = self._auto_fix_relationship_references(relationship_issues, all_files)
            
            if relationship_issues:
                logger.warning(f"[G-16] âš  {len(relationship_issues)} issues remain after {MAX_G16_FIX_ATTEMPTS} fix attempts")
            
            # G-17: ForeignKeyå‚ç…§å…ˆæ¤œè¨¼ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
            MAX_G17_FIX_ATTEMPTS = 2
            foreignkey_issues = []
            for g17_attempt in range(MAX_G17_FIX_ATTEMPTS):
                foreignkey_issues = self._validate_foreignkey_references(all_files)
                if not foreignkey_issues:
                    if g17_attempt > 0:
                        logger.debug(f"[G-17] âœ… All issues fixed after {g17_attempt} attempt(s)")
                    break
                
                logger.debug(f"[G-17] Detected {len(foreignkey_issues)} issues (attempt {g17_attempt + 1}/{MAX_G17_FIX_ATTEMPTS})")
                if g17_attempt == 0:
                    logger.debug(f"â— ForeignKey reference check: {len(foreignkey_issues)} issue(s)")
                    for issue in foreignkey_issues[:5]:
                        logger.debug(f"  - {issue['file']}:{issue['line']} - {issue['referenced_table']}")
                
                all_files = self._auto_fix_foreignkey_references(foreignkey_issues, all_files)
            
            if foreignkey_issues:
                logger.warning(f"[G-17] âš  {len(foreignkey_issues)} issues remain after {MAX_G17_FIX_ATTEMPTS} fix attempts")
            
            # G-18: SQLAlchemy Enumå®šç¾©æ¤œè¨¼ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
            MAX_G18_FIX_ATTEMPTS = 2
            enum_usage_issues = []
            for g18_attempt in range(MAX_G18_FIX_ATTEMPTS):
                enum_usage_issues = self._validate_sqlalchemy_enum_usage(all_files)
                if not enum_usage_issues:
                    if g18_attempt > 0:
                        logger.debug(f"[G-18] âœ… All issues fixed after {g18_attempt} attempt(s)")
                    break
                
                logger.debug(f"[G-18] Detected {len(enum_usage_issues)} issues (attempt {g18_attempt + 1}/{MAX_G18_FIX_ATTEMPTS})")
                if g18_attempt == 0:
                    logger.debug(f"â— Enum usage check: {len(enum_usage_issues)} issue(s)")
                    for issue in enum_usage_issues[:5]:
                        logger.debug(f"  - {issue['file']}:{issue['line']} - {issue['referenced_class']}")
                
                all_files = self._auto_fix_sqlalchemy_enum_usage(enum_usage_issues, all_files)
            
            if enum_usage_issues:
                logger.warning(f"[G-18] âš  {len(enum_usage_issues)} issues remain after {MAX_G18_FIX_ATTEMPTS} fix attempts")
            
            # G-19: Mappedå‹ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ•´åˆæ€§æ¤œè¨¼ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
            MAX_G19_FIX_ATTEMPTS = 2
            mapped_type_issues = []
            for g19_attempt in range(MAX_G19_FIX_ATTEMPTS):
                mapped_type_issues = self._validate_mapped_type_consistency(all_files)
                if not mapped_type_issues:
                    if g19_attempt > 0:
                        logger.debug(f"[G-19] âœ… All issues fixed after {g19_attempt} attempt(s)")
                    break
                
                logger.debug(f"[G-19] Detected {len(mapped_type_issues)} issues (attempt {g19_attempt + 1}/{MAX_G19_FIX_ATTEMPTS})")
                if g19_attempt == 0:
                    logger.debug(f"â— Mapped type consistency check: {len(mapped_type_issues)} issue(s)")
                    for issue in mapped_type_issues[:5]:
                        logger.debug(f"  - {issue['file']}:{issue['line']} - {issue['attribute']}")
                
                all_files = self._auto_fix_mapped_type_consistency(mapped_type_issues, all_files)
            
            if mapped_type_issues:
                logger.warning(f"[G-19] âš  {len(mapped_type_issues)} issues remain after {MAX_G19_FIX_ATTEMPTS} fix attempts")
            
            # G-20: Flaské™çš„ãƒ‘ã‚¹æ¤œè¨¼ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
            MAX_G20_FIX_ATTEMPTS = 2
            flask_static_issues = []
            for g20_attempt in range(MAX_G20_FIX_ATTEMPTS):
                flask_static_issues = self._validate_flask_static_paths(all_files)
                if not flask_static_issues:
                    if g20_attempt > 0:
                        logger.debug(f"[G-20] âœ… All issues fixed after {g20_attempt} attempt(s)")
                    break
                
                logger.debug(f"[G-20] Detected {len(flask_static_issues)} issues (attempt {g20_attempt + 1}/{MAX_G20_FIX_ATTEMPTS})")
                if g20_attempt == 0:
                    logger.debug(f"â— Flask static path check: {len(flask_static_issues)} issue(s)")
                    for issue in flask_static_issues[:5]:
                        logger.debug(f"  - {issue['file']}:{issue['line']} - {issue['current_path']} â†’ {issue['suggested_path']}")
                
                all_files = self._auto_fix_flask_static_paths(flask_static_issues, all_files)
            
            if flask_static_issues:
                logger.warning(f"[G-20] âš  {len(flask_static_issues)} issues remain after {MAX_G20_FIX_ATTEMPTS} fix attempts")
            
            # G-21: HTMLå‚ç…§æ•´åˆæ€§æ¤œè¨¼ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
            MAX_G21_FIX_ATTEMPTS = 2
            html_asset_issues = []
            for g21_attempt in range(MAX_G21_FIX_ATTEMPTS):
                html_asset_issues = self._validate_html_asset_references(all_files)
                if not html_asset_issues:
                    if g21_attempt > 0:
                        logger.debug(f"[G-21] âœ… All issues fixed after {g21_attempt} attempt(s)")
                    break
                
                logger.debug(f"[G-21] Detected {len(html_asset_issues)} issues (attempt {g21_attempt + 1}/{MAX_G21_FIX_ATTEMPTS})")
                if g21_attempt == 0:
                    logger.debug(f"â— HTML asset reference check: {len(html_asset_issues)} issue(s)")
                    for issue in html_asset_issues[:5]:
                        logger.debug(f"  - {issue['html_file']}:{issue['line']} - {issue['ref_type']}: {issue['ref_path']}")
                
                all_files = self._auto_fix_html_asset_references(html_asset_issues, all_files)
            
            if html_asset_issues:
                logger.warning(f"[G-21] âš  {len(html_asset_issues)} issues remain after {MAX_G21_FIX_ATTEMPTS} fix attempts")
            
            # G-22: JavaScriptã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ—ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œè¨¼ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
            MAX_G22_FIX_ATTEMPTS = 2
            js_animation_issues = []
            for g22_attempt in range(MAX_G22_FIX_ATTEMPTS):
                js_animation_issues = self._validate_js_animation_loop(all_files)
                if not js_animation_issues:
                    if g22_attempt > 0:
                        logger.debug(f"[G-22] âœ… All issues fixed after {g22_attempt} attempt(s)")
                    break
                
                logger.debug(f"[G-22] Detected {len(js_animation_issues)} issues (attempt {g22_attempt + 1}/{MAX_G22_FIX_ATTEMPTS})")
                if g22_attempt == 0:
                    logger.debug(f"â— JavaScript animation loop check: {len(js_animation_issues)} issue(s)")
                    for issue in js_animation_issues[:5]:
                        logger.debug(f"  - {issue['file']}:{issue['line']} - {issue['method_name']}() called without timestamp")
                
                all_files = self._auto_fix_js_animation_loop(js_animation_issues, all_files)
            
            if js_animation_issues:
                logger.warning(f"[G-22] âš  {len(js_animation_issues)} issues remain after {MAX_G22_FIX_ATTEMPTS} fix attempts")
            
            # ESLintçµ±åˆ: JavaScriptå“è³ªãƒã‚§ãƒƒã‚¯
            eslint_result = None
            try:
                if self._check_eslint_available():
                    logger.debug("[ESLint] Running JavaScript linter (multi-stage)...")
                    eslint_result = self._run_eslint_on_files(all_files)
                    
                    if eslint_result['files_checked'] > 0:
                        error_count = len(eslint_result['errors'])
                        warning_count = len(eslint_result['warnings'])
                        
                        if error_count > 0 or warning_count > 0:
                            logger.debug("")
                            logger.debug(f"â— ESLint: {error_count} error(s), {warning_count} warning(s)")
                            
                            for issue in eslint_result['errors'][:5]:
                                logger.debug(f"  - {issue['file']}:{issue['line']} [{issue['rule']}] {issue['message']}")
                            
                            if error_count > 5:
                                logger.debug(f"  ... and {error_count - 5} more error(s)")
                            
                            if error_count > 0:
                                logger.debug("[ESLint] Attempting auto-fix with LLM...")
                                all_files = self._fix_eslint_errors_with_llm(
                                    all_files,
                                    eslint_result['errors']
                                )
                                
                                recheck_result = self._run_eslint_on_files(all_files)
                                error_count_after = len(recheck_result['errors'])
                                warning_count_after = len(recheck_result['warnings'])
                                
                                logger.debug("")
                                logger.debug(f"  After auto-fix: {error_count_after} error(s), {warning_count_after} warning(s)")
                                
                                if error_count_after == 0:
                                    logger.debug(f"  âœ“ All ESLint errors resolved!")
                                
                                eslint_result = recheck_result
                        else:
                            logger.debug("[ESLint] No issues found")
                else:
                    logger.debug("[ESLint] Not available, skipping JavaScript lint check")
                    
            except Exception as e:
                logger.debug(f"[ESLint] Error during lint check: {e}")
            
            # G-23: APIå¥‘ç´„æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ï¼ˆè­¦å‘Šã®ã¿ã€è‡ªå‹•ä¿®æ­£ãªã—ï¼‰
            api_contract_issues = self._validate_api_contract_consistency(all_files)
            if api_contract_issues:
                logger.debug(f"â— API contract consistency: {len(api_contract_issues)} warning(s)")
                for issue in api_contract_issues[:3]:
                    logger.debug(f"  - {issue['details']}")
            
            # G-24: back_populatesç›¸äº’å‚ç…§ãƒã‚§ãƒƒã‚¯
            MAX_G24_FIX_ATTEMPTS = 2
            back_populates_issues = []
            for g24_attempt in range(MAX_G24_FIX_ATTEMPTS):
                back_populates_issues = self._validate_back_populates_consistency(all_files)
                if not back_populates_issues:
                    if g24_attempt > 0:
                        logger.debug(f"[G-24] âœ… All issues fixed after {g24_attempt} attempt(s)")
                    break
                logger.debug(f"[G-24] Detected {len(back_populates_issues)} issues (attempt {g24_attempt + 1}/{MAX_G24_FIX_ATTEMPTS})")
                all_files = self._auto_fix_back_populates_consistency(back_populates_issues, all_files)
            
            if back_populates_issues:
                logger.debug(f"â— back_populates consistency: {len(back_populates_issues)} error(s)")
                for issue in back_populates_issues[:3]:
                    logger.debug(f"  - {issue['details']}")
            
            # G-25: ForeignKeyå‹æ³¨é‡ˆæ¬ å¦‚ãƒã‚§ãƒƒã‚¯
            foreignkey_annotation_issues = self._validate_foreignkey_type_annotation(all_files)
            if foreignkey_annotation_issues:
                logger.debug(f"[G-25] Detected {len(foreignkey_annotation_issues)} issues, auto-fixing...")
                all_files = self._auto_fix_foreignkey_type_annotation(foreignkey_annotation_issues, all_files)
                foreignkey_annotation_issues = self._validate_foreignkey_type_annotation(all_files)
            
            if foreignkey_annotation_issues:
                logger.debug(f"â— ForeignKey type annotations: {len(foreignkey_annotation_issues)} warning(s)")
            
            # G-26: Mappedå‹ã¨SQLAlchemyå‹ã®ä¸ä¸€è‡´ãƒã‚§ãƒƒã‚¯
            mapped_mismatch_issues = self._validate_mapped_sqlalchemy_type_mismatch(all_files)
            if mapped_mismatch_issues:
                logger.debug(f"[G-26] Detected {len(mapped_mismatch_issues)} issues, auto-fixing...")
                all_files = self._auto_fix_mapped_sqlalchemy_type_mismatch(mapped_mismatch_issues, all_files)
                mapped_mismatch_issues = self._validate_mapped_sqlalchemy_type_mismatch(all_files)
            
            if mapped_mismatch_issues:
                logger.debug(f"â— Mapped/SQLAlchemy type mismatch: {len(mapped_mismatch_issues)} warning(s)")
            
            # G-27: relationship overlapsæ¤œå‡º
            relationship_overlap_issues = self._validate_relationship_overlaps(all_files)
            if relationship_overlap_issues:
                all_files = self._auto_fix_relationship_overlaps(relationship_overlap_issues, all_files)
                logger.debug(f"â— Relationship overlaps: {len(relationship_overlap_issues)} warning(s)")
            
            # G-28: å¾ªç’°ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ¤œå‡º
            circular_import_issues = self._validate_circular_imports(all_files)
            if circular_import_issues:
                all_files = self._auto_fix_circular_imports(circular_import_issues, all_files)
                logger.debug(f"â— Circular imports: {len(circular_import_issues)} error(s)")
                for issue in circular_import_issues[:2]:
                    logger.debug(f"  - {issue['details']}")
            
            # G-29: Marshmallow Nestedå¾ªç’°å‚ç…§æ¤œå‡º
            marshmallow_circular_issues = self._validate_marshmallow_nested_circular(all_files)
            if marshmallow_circular_issues:
                all_files = self._auto_fix_marshmallow_nested_circular(marshmallow_circular_issues, all_files)
                logger.debug(f"â— Marshmallow Nested circular: {len(marshmallow_circular_issues)} warning(s)")
            
            # G-30: JavaScript thiså‚ç…§å–ªå¤±ãƒã‚§ãƒƒã‚¯
            js_this_loss_issues = self._validate_js_this_context_loss(all_files)
            if js_this_loss_issues:
                logger.debug(f"[G-30] Detected {len(js_this_loss_issues)} issues, auto-fixing...")
                all_files = self._auto_fix_js_this_context_loss(js_this_loss_issues, all_files)
                js_this_loss_issues = self._validate_js_this_context_loss(all_files)
            
            if js_this_loss_issues:
                logger.debug(f"â— JavaScript this context loss: {len(js_this_loss_issues)} error(s)")
                for issue in js_this_loss_issues[:3]:
                    logger.debug(f"  - {issue['details']}")
            
            # G-31: Marshmallow APIèª¤ç”¨æ¤œå‡ºï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
            MAX_G31_FIX_ATTEMPTS = 2
            marshmallow_api_issues = []
            for g31_attempt in range(MAX_G31_FIX_ATTEMPTS):
                marshmallow_api_issues = self._validate_marshmallow_api_usage(all_files)
                if not marshmallow_api_issues:
                    if g31_attempt > 0:
                        logger.debug(f"[G-31] âœ… All issues fixed after {g31_attempt} attempt(s)")
                    break
                
                logger.debug(f"[G-31] Detected {len(marshmallow_api_issues)} issues (attempt {g31_attempt + 1}/{MAX_G31_FIX_ATTEMPTS})")
                if g31_attempt == 0:
                    logger.debug(f"â— Marshmallow API usage check: {len(marshmallow_api_issues)} issue(s)")
                    for issue in marshmallow_api_issues[:5]:
                        logger.debug(f"  - {issue['file']}:{issue['line']} - {issue['field']}")
                
                all_files = self._auto_fix_marshmallow_api_usage(marshmallow_api_issues, all_files)
            
            if marshmallow_api_issues:
                logger.warning(f"[G-31] âš  {len(marshmallow_api_issues)} issues remain after {MAX_G31_FIX_ATTEMPTS} fix attempts")
            
            if not missing_css_classes and not missing_html_ids and not compatibility_issues and not symbol_issues and not requirements_issues and not library_issues and not frontend_issues and not schema_issues and not doc_issues and not class_safety_issues and not framework_reserved_issues and not builtin_shadowing_issues and not route_duplicate_issues and not table_duplicate_issues and not js_reserved_issues and not relationship_issues and not foreignkey_issues and not enum_usage_issues and not mapped_type_issues and not flask_static_issues and not html_asset_issues and not js_animation_issues and not back_populates_issues and not circular_import_issues and not js_this_loss_issues and not marshmallow_api_issues:
                logger.debug("[G-1/.../G-31] All consistency checks passed")
            
            # HUD: Auto-completionå®Œäº†ï¼ˆG-1ã€œG-31å®Œäº†å¾Œï¼‰
            hud.complete("Auto-completion")
            
            # ==========================================
            # Step 7.5: HTMLå‚ç…§ä¿®æ­£ï¼ˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ç¦æ­¢ãƒ¢ãƒ¼ãƒ‰ï¼‰
            # ==========================================
            if getattr(self, '_no_framework_mode', False):
                logger.debug("[G-32] Checking HTML script references for vanilla JS...")
                all_files = self._fix_html_script_reference_for_vanilla_js(all_files)
            
            # ==========================================
            # Step 7.6: Runtime Import Testï¼ˆå¯¾ç­–Cï¼‰
            # ==========================================
            # ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã‚’å®Ÿéš›ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦å®Ÿè¡Œæ™‚ã‚¨ãƒ©ãƒ¼ã‚’æ¤œå‡º
            # é™çš„è§£æã§ã¯æ¤œå‡ºã§ããªã„ã‚¯ãƒ©ã‚¹å®šç¾©æ™‚ã®ã‚¨ãƒ©ãƒ¼ãªã©ã‚’äº‹å‰ã«ç™ºè¦‹
            logger.debug("[Runtime Import] Starting import test (multi-stage)...")
            
            import_success, import_error, failing_file = self._validate_runtime_import(all_files)
            
            # â­ å¯¾ç­–A: å¤–éƒ¨ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ä¸è¶³ã®å ´åˆã¯æƒ…å ±è¡¨ç¤ºã®ã¿ï¼ˆLLMä¿®æ­£ã¯ä¸è¦ï¼‰
            if import_success and import_error and import_error.startswith("EXTERNAL_PACKAGE:"):
                package_name = import_error.replace("EXTERNAL_PACKAGE:", "")
                logger.debug(f"\nâ— Runtime import test skipped:")
                logger.debug(f"   â„¹ Requires external package: {package_name}")
                logger.debug(f"   â†’ Run: pip install -r requirements.txt")
                logger.debug(f"[Runtime Import] Skipped due to external package: {package_name}")
            elif not import_success:
                logger.debug(f"\nâš  Runtime import error detected:")
                logger.debug(f"   File: {failing_file or 'Unknown'}")
                logger.debug(f"   Error: {import_error}")
                
                # LLMã§è‡ªå‹•ä¿®æ­£ã‚’è©¦è¡Œ
                MAX_IMPORT_FIX_ATTEMPTS = 2
                for attempt in range(MAX_IMPORT_FIX_ATTEMPTS):
                    logger.debug(f"[Runtime Import] Fix attempt {attempt + 1}/{MAX_IMPORT_FIX_ATTEMPTS}")
                    
                    fixed_code = self._fix_import_error_with_llm(
                        all_files,
                        import_error,
                        failing_file
                    )
                    
                    if fixed_code:
                        # ä¿®æ­£å¾Œã«å†ãƒ†ã‚¹ãƒˆ
                        retry_success, retry_error, retry_file = self._validate_runtime_import(fixed_code)
                        
                        # å¤–éƒ¨ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ä¸è¶³ã«ãªã£ãŸå ´åˆã‚‚OK
                        if retry_success:
                            if retry_error and retry_error.startswith("EXTERNAL_PACKAGE:"):
                                package_name = retry_error.replace("EXTERNAL_PACKAGE:", "")
                                logger.debug(f"   âœ… Code error fixed. External package required: {package_name}")
                            else:
                                logger.debug(f"   âœ… Import error fixed (attempt {attempt + 1})")
                            all_files = fixed_code
                            logger.debug(f"[Runtime Import] âœ… Fixed on attempt {attempt + 1}")
                            break
                        else:
                            # ä¿®æ­£ã—ãŸãŒåˆ¥ã®ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ
                            import_error = retry_error
                            failing_file = retry_file
                            all_files = fixed_code  # éƒ¨åˆ†çš„ãªä¿®æ­£ã‚’ä¿æŒ
                            logger.debug(f"[Runtime Import] Partial fix, new error: {retry_error}")
                    else:
                        logger.debug(f"[Runtime Import] Fix attempt {attempt + 1} failed")
                else:
                    # å…¨ã¦ã®è©¦è¡ŒãŒå¤±æ•—
                    logger.debug(f"   âš  Could not auto-fix. Manual review required.")
                    logger.debug("[Runtime Import] All fix attempts failed")
            
            # Step 8: Lint & Auto-fix
            hud.mark("Lint & Auto-fix")
            
            # å®Ÿéš›ã®å‡¦ç†ã‚’å®Ÿè¡Œï¼ˆå‘¼ã³å‡ºã—å…ƒã‹ã‚‰ç§»å‹•ï¼‰
            lint_result = None
            try:
                from cognix.linter_integration import LinterIntegration
                from cognix import hud_components as hud_comp
                
                linter = LinterIntegration()
                
                # linterãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã®ã¿å®Ÿè¡Œ
                if linter.available_linters:
                    logger.debug("[Linter] Running external linters on multi-stage generated code...")
                    lint_result = linter.lint_generated_code(all_files)
                    
                    if lint_result['has_errors']:
                        error_count = len(lint_result['errors'])
                        warning_count = len(lint_result['warnings'])
                        linters_used = ', '.join(lint_result['linters_used'])
                        
                        # â­ Zen HUD: Multi-stageåˆæœŸLintã‚¨ãƒ©ãƒ¼æ•°ã‚’è¨˜éŒ²
                        self._zen_summary["lint"]["initial"] = error_count
                        
                        logger.debug(f"[Linter] Detected {error_count} error(s), {warning_count} warning(s)")
                        logger.debug(f"[Linter] Used: {linters_used}")
                        
                        logger.debug("")
                        logger.debug(hud_comp.lint_summary_line(0, warning_count, error_count))
                        
                        if lint_result['can_auto_fix']:
                            logger.debug("[Linter] Attempting auto-fix with LLM...")
                            logger.debug("[Linter] Auto-fix attempt started")
                            
                            fixed_files = self._fix_linter_errors_with_llm(
                                all_files,
                                lint_result['errors'],
                                lint_result.get('file_languages', {})
                            )
                            
                            if fixed_files:
                                logger.debug("[Linter] Auto-fix successful")
                                logger.debug(f"{Icon.GEAR.value} Linter errors have been auto-fixed")
                                all_files = fixed_files
                                
                                try:
                                    logger.debug("[Linter] Re-running linter after auto-fix...")
                                    recheck_result = linter.lint_generated_code(all_files)
                                    
                                    error_count_before = len(lint_result.get('errors', []))
                                    warning_count_before = len(lint_result.get('warnings', []))
                                    error_count_after = len(recheck_result.get('errors', []))
                                    warning_count_after = len(recheck_result.get('warnings', []))
                                    
                                    logger.debug("")
                                    logger.debug("  After auto-fix:")
                                    logger.debug(hud_comp.lint_summary_line(0, warning_count_after, error_count_after))
                                    
                                    if error_count_after > 0 or warning_count_after > 0:
                                        breakdown = self._get_error_breakdown(
                                            recheck_result.get('errors', []),
                                            recheck_result.get('warnings', [])
                                        )
                                        error_parts = []
                                        if breakdown['critical'] > 0:
                                            error_parts.append(f"Critical={breakdown['critical']}")
                                        if breakdown['high'] > 0:
                                            error_parts.append(f"High={breakdown['high']}")
                                        if breakdown['medium'] > 0:
                                            error_parts.append(f"Medium={breakdown['medium']}")
                                        if breakdown['low'] > 0:
                                            error_parts.append(f"Low={breakdown['low']} (style issues)")
                                        if breakdown['env'] > 0:
                                            error_parts.append(f"Env={breakdown['env']}")
                                        if breakdown['warnings'] > 0:
                                            error_parts.append(f"Warnings={breakdown['warnings']}")
                                        
                                        if error_parts:
                                            logger.debug(f"  Error Types: {', '.join(error_parts)}")
                                    
                                    lint_result = recheck_result
                                    
                                    # â­ Zen HUD: Multi-stage Auto-fixå¾Œã®Lintã‚¨ãƒ©ãƒ¼æ•°ã‚’è¨˜éŒ²
                                    self._zen_summary["lint"]["final"] = error_count_after
                                    self._zen_summary["lint"]["fixed"] = (error_count_after < error_count_before)
                                    
                                    errors_fixed = error_count_before - error_count_after
                                    if error_count_after == 0 and warning_count_after == 0:
                                        logger.debug(f"  âœ“ All linter errors resolved!")
                                    elif errors_fixed > 0:
                                        logger.debug(f"  âœ“ {errors_fixed} errors fixed, {error_count_after} remaining")
                                    else:
                                        logger.debug(f"  âš  No errors were fixed automatically")
                                    
                                    logger.debug(f"[Linter] Re-check complete: {error_count_after} errors, {warning_count_after} warnings")
                                    
                                except Exception as e:
                                    logger.debug(f"[Linter] Re-check failed: {e}")
                            
                            else:
                                logger.debug("[Linter] Auto-fix failed")
                                logger.debug("  âš   Auto-fix failed. Manual review required.")
                        else:
                            logger.debug("  â†’ run with --verbose for details")
                            logger.debug("  âš   Manual review required (syntax errors detected)")
                    
                    elif lint_result['has_warnings']:
                        warning_count = len(lint_result['warnings'])
                        logger.debug(f"[Linter] Detected {warning_count} warning(s)")
                        logger.debug(f"\nâ— Code passed linter checks ({warning_count} warnings)")
                    else:
                        logger.debug("[Linter] No issues detected")
                        logger.debug("\nâ— Code passed all linter checks")
                else:
                    logger.debug("[Linter] No external linters available (optional feature)")
            
            except ImportError:
                logger.debug("[Linter] LinterIntegration not available (optional feature)")
            except Exception as e:
                logger.debug(f"[Linter] Error during linting: {e}")
            
            # ==========================================
            # Cross-File Consistency Check (Multi-stage)
            # ãƒ¢ãƒ‡ãƒ«å®šç¾©ã¨åˆæœŸãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ã‚’æ¤œè¨¼
            # ==========================================
            try:
                logger.debug("[Cross-File] Starting cross-file consistency check...")
                cross_file_result = self._check_cross_file_consistency(all_files)
                
                if cross_file_result['has_issues']:
                    issue_count = len(cross_file_result['issues'])
                    logger.debug(f"[Cross-File] Detected {issue_count} consistency issue(s)")
                    
                    # å•é¡Œã®è©³ç´°ã‚’ãƒ­ã‚°ã«å‡ºåŠ›
                    for issue in cross_file_result['issues']:
                        logger.debug(
                            f"[Cross-File]   {issue['model']}.{issue['field']}: "
                            f"{issue['message']}"
                        )
                    
                    # Zen HUD: å•é¡Œæ¤œå‡ºã‚’è¡¨ç¤º
                    logger.debug("")
                    logger.debug(f"  {Icon.WARNING.value} Cross-file consistency: {issue_count} issue(s) detected")
                    
                    # LLMã«ã‚ˆã‚‹è‡ªå‹•ä¿®æ­£ã‚’è©¦è¡Œ
                    logger.debug("[Cross-File] Attempting auto-fix with LLM...")
                    fixed_files = self._fix_cross_file_issues_with_llm(
                        all_files,
                        cross_file_result['issues'],
                        cross_file_result.get('model_definitions', {})
                    )
                    
                    if fixed_files:
                        # ä¿®æ­£ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’åæ˜ 
                        for filename, code in fixed_files.items():
                            all_files[filename] = code
                        
                        logger.debug(f"[Cross-File] Auto-fix successful: {list(fixed_files.keys())}")
                        logger.debug(f"  {Icon.GEAR.value} Cross-file issues have been auto-fixed")
                        
                        # ä¿®æ­£å¾Œã«å†æ¤œè¨¼
                        recheck_result = self._check_cross_file_consistency(all_files)
                        if recheck_result['has_issues']:
                            remaining = len(recheck_result['issues'])
                            logger.debug(f"  {Icon.WARNING.value} {remaining} issue(s) remain after auto-fix")
                        else:
                            logger.debug(f"  {Icon.CHECK.value} All cross-file issues resolved")
                    else:
                        logger.debug("[Cross-File] Auto-fix failed")
                        logger.debug(f"  {Icon.WARNING.value} Auto-fix failed. Manual review required.")
                else:
                    logger.debug("[Cross-File] No consistency issues detected")
            
            except Exception as e:
                logger.debug(f"[Cross-File] Error during consistency check: {e}")
                # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚å‡¦ç†ã‚’ç¶™ç¶š
            
            hud.complete("Lint & Auto-fix")
            
            # Step 9: Quality assessment & Comprehensive Review
            hud.mark("Quality assessment & Comprehensive Review")
            
            # å®Ÿéš›ã®å‡¦ç†ã‚’å®Ÿè¡Œï¼ˆå‘¼ã³å‡ºã—å…ƒã‹ã‚‰ç§»å‹•ï¼‰
            quality_scores = self._assess_code_quality(
                all_files,
                complexity,
                goal,
                lint_result=lint_result
            )
            
            # Quality issues ã®æ¤œå‡ºã¨è¡¨ç¤ºï¼ˆMulti-stage generationï¼‰
            quality_issues = []
            quality_issue_count = 0
            
            for filename, score in quality_scores.items():
                if score < 0.70:  # 70%æœªæº€ã¯å•é¡Œã‚ã‚Š
                    quality_issue_count += 1
                    # è©³ç´°ã‚’å–å¾—
                    code = all_files.get(filename, '')
                    ext = filename.split('.')[-1]
                    placeholder_count, detected_lines = self._detect_placeholders_in_comments(code, ext)
                    
                    if placeholder_count > 0:
                        quality_issues.append({
                            'filename': filename,
                            'score': score,
                            'issue_type': 'placeholder',
                            'count': placeholder_count,
                            'details': detected_lines[:3]  # æœ€å¤§3ä»¶
                        })
            
            # Quality issues è¡¨ç¤ºï¼ˆMulti-stage generationï¼‰
            if quality_issue_count > 0:
                logger.debug(f"\n[Quality] Code quality check: {quality_issue_count} quality issue(s) detected")
                
                for issue in quality_issues[:5]:  # æœ€å¤§5ä»¶è¡¨ç¤º
                    logger.debug(f"  - {issue['filename']}: {issue['count']} placeholder(s) in comments")
                    for detail in issue['details']:
                        logger.debug(f"    â€¢ {detail}")
                
                # Quality issues ã®è‡ªå‹•ä¿®æ­£
                if quality_issues:
                    logger.debug("[Quality] Attempting to fix quality issues...")
                    fixed_files = self._fix_quality_issues_with_llm(all_files, quality_issues)
                    
                    if fixed_files:
                        all_files = fixed_files
                        logger.debug("[Quality] Quality issues fixed")
                        logger.debug(f"{Icon.GEAR.value} Quality issues have been auto-fixed")
                        
                        # Quality å†è©•ä¾¡
                        quality_scores = self._assess_code_quality(
                            all_files,
                            complexity,
                            goal,
                            lint_result=lint_result
                        )
            
            # Step 9ã®ç¶šã: ç·åˆãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆComprehensive Reviewï¼‰
            # ==========================================
            # ã€Œä¸€ç™ºã§å‹•ãã€ã‚’ä¿è¨¼ã™ã‚‹ãŸã‚ã®æœ€çµ‚ãƒã‚§ãƒƒã‚¯
            logger.debug("[Comprehensive Review] Starting final review...")
            all_files = self._final_comprehensive_review(all_files, goal)
            
            # ğŸ†• Comprehensive Reviewå¾Œã®HTMLå‚ç…§ä¿®æ­£ï¼ˆã‚¹ã‚¿ãƒ–å†ç”Ÿæˆå¾Œã«å¿…è¦ï¼‰
            if getattr(self, '_no_framework_mode', False):
                logger.debug("[Post-Review] Checking HTML script references after comprehensive review...")
                all_files = self._fix_html_script_reference_for_vanilla_js(all_files)
            
            # ç·åˆãƒ¬ãƒ“ãƒ¥ãƒ¼å¾Œã®Qualityå†è©•ä¾¡
            quality_scores = self._assess_code_quality(
                all_files,
                complexity,
                goal,
                lint_result=lint_result
            )
            
            # Quality assessment & Comprehensive Review å®Œäº†
            hud.complete("Quality assessment & Comprehensive Review")
            
            # å®Œäº†
            hud.finish(success=True)
            return (all_files, lint_result, quality_scores)
            
        except Exception as e:
            # â­ ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¿½åŠ : ä¾‹å¤–ã®è©³ç´°ã‚’è¨˜éŒ²
            import traceback
            logger.error(f"[Multi-stage] âŒâŒâŒ EXCEPTION CAUGHT âŒâŒâŒ")
            logger.error(f"[Multi-stage]   Exception message: {e}")
            logger.error(f"[Multi-stage]   Exception type: {type(e).__name__}")
            logger.error(f"[Multi-stage]   Exception args: {e.args}")
            logger.error(f"[Multi-stage]   Full traceback:\n{traceback.format_exc()}")
            logger.error(f"[Multi-stage]   all_files at exception: {list(all_files.keys()) if all_files else 'empty'}")
            
            hud.finish(success=False)
            logger.debug(f"   âŒ Multi-stage generation error: {e}")
            logger.debug("   âš  Falling back to single-stage generation")
            
            # â­ ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¿½åŠ : ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é–‹å§‹
            logger.warning(f"[Multi-stage] â–¶ Starting fallback single-stage generation...")
            logger.warning(f"[Multi-stage]   Original goal length: {len(goal)} chars")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            fallback_code = self._generate_code_implementation(
                goal=goal,
                analysis="",
                plan="",
                complexity=complexity,
                skip_validation=False,
                skip_multi_stage=True
            )
            
            # â­ ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¿½åŠ : ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯çµæœ
            logger.warning(f"[Multi-stage] â—€ Fallback generation completed")
            logger.warning(f"[Multi-stage]   Fallback result: {list(fallback_code.keys()) if fallback_code else 'empty/None'}")
            logger.warning(f"[Multi-stage]   Fallback file count: {len(fallback_code) if fallback_code else 0}")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã¯lint_result=None, quality_scores={}ã‚’è¿”ã™
            return (fallback_code, None, {})


    def _generate_code_implementation(
        self, 
        goal: str,
        analysis: str = "",
        plan: str = "",
        complexity: str = "medium",
        skip_validation: bool = False,
        force_filename: str = None,
        skip_multi_stage: bool = False
    ) -> Dict[str, str]:
        """
        LLMã‚’ä½¿ç”¨ã—ã¦ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ
        
        Args:
            goal: å®Ÿè£…ç›®æ¨™ã®èª¬æ˜
            analysis: åˆ†æçµæœ(ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
            plan: å®Ÿè£…è¨ˆç”»(ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
            complexity: è¤‡é›‘åº¦("simple", "medium", "complex")
            skip_validation: æ¤œè¨¼ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã‹
            force_filename: å¼·åˆ¶çš„ã«ä½¿ç”¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«å
                        â­ æŒ‡å®šæ™‚ã¯LLMãŒä½•ã‚’è¿”ã—ã¦ã‚‚ã€ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«åã§ä¿å­˜
        
        Returns:
            {filename: code} ã®è¾æ›¸
        """
        
        # ========================================
        # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æº–å‚™
        # ========================================
        existing_context = ""
        
        # ğŸ” ãƒ‡ãƒãƒƒã‚°: _complexity_assessor ã®å­˜åœ¨ç¢ºèª
        logger.debug(f"[EXISTING_CONTEXT] Step 1: hasattr(self, '_complexity_assessor') = {hasattr(self, '_complexity_assessor')}")
        
        if hasattr(self, '_complexity_assessor'):
            # ğŸ” ãƒ‡ãƒãƒƒã‚°: existing_files å±æ€§ã®å­˜åœ¨ç¢ºèª
            logger.debug(f"[EXISTING_CONTEXT] Step 2: hasattr(_complexity_assessor, 'existing_files') = {hasattr(self._complexity_assessor, 'existing_files')}")
            
            if hasattr(self._complexity_assessor, 'existing_files'):
                # ğŸ” ãƒ‡ãƒãƒƒã‚°: existing_files ã®å†…å®¹ç¢ºèª
                logger.debug(f"[EXISTING_CONTEXT] Step 3: existing_files type = {type(self._complexity_assessor.existing_files)}")
                logger.debug(f"[EXISTING_CONTEXT] Step 3: existing_files bool = {bool(self._complexity_assessor.existing_files)}")
                logger.debug(f"[EXISTING_CONTEXT] Step 3: existing_files keys = {list(self._complexity_assessor.existing_files.keys())}")
                logger.debug(f"[EXISTING_CONTEXT] Step 3: existing_files length = {len(self._complexity_assessor.existing_files)}")
            else:
                logger.debug(f"[EXISTING_CONTEXT] Step 2: _complexity_assessor has NO 'existing_files' attribute")
        else:
            logger.debug(f"[EXISTING_CONTEXT] Step 1: self has NO '_complexity_assessor' attribute")
        
        # self.existing_files ãŒå­˜åœ¨ã™ã‚‹å ´åˆã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰
        if hasattr(self, '_complexity_assessor') and \
            hasattr(self._complexity_assessor, 'existing_files') and \
            self._complexity_assessor.existing_files:
                # ğŸ” ãƒ‡ãƒãƒƒã‚°: æ¡ä»¶æˆåŠŸ
                logger.debug(f"[EXISTING_CONTEXT] âœ… All conditions passed - building existing_context")
                
                existing_context = "\nEXISTING FILES IN PROJECT:\n"
                for filename, code in self._complexity_assessor.existing_files.items():
                    # ğŸ” ãƒ‡ãƒãƒƒã‚°: å„ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
                    logger.debug(f"[EXISTING_CONTEXT] Processing file: {filename} ({len(code)} chars)")
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã«å¿œã˜ã¦é©åˆ‡ãªé‡ã‚’æ¸¡ã™
                    max_chars = 5000  # 500â†’5000ã«å¢—åŠ 
                    code_preview = code[:max_chars]
                    if len(code) > max_chars:
                        # å…ˆé ­ã¨æœ«å°¾ã®ä¸¡æ–¹ã‚’å«ã‚ã‚‹
                        half = max_chars // 2
                        code_preview = code[:half] + "\n\n... [middle part omitted] ...\n\n" + code[-half:]
                
                    existing_context += f"\n## File: {filename}\n```\n{code_preview}\n```\n"
        else:
            # ğŸ” ãƒ‡ãƒãƒƒã‚°: æ¡ä»¶å¤±æ•—
            logger.debug(f"[EXISTING_CONTEXT] âŒ Condition FAILED - existing_context remains empty")
        
        # ğŸ” ãƒ‡ãƒãƒƒã‚°: æœ€çµ‚çµæœ
        logger.debug(f"[EXISTING_CONTEXT] Final existing_context length: {len(existing_context)} chars")
        

        
        # ========================================
        # â­ force_multiple_files_temp ã®åˆæœŸåŒ–
        # ========================================
        # ã‚¹ã‚³ãƒ¼ãƒ—ã‚¨ãƒ©ãƒ¼ã‚’é˜²ããŸã‚ã€if-else ã®å‰ã§åˆæœŸåŒ–
        force_multiple_files_temp = False
        
        # ========================================
        # â­ force_filename ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ
        # ========================================
        if force_filename:
            prompt = f"""Generate code for the following requirements.

PRIMARY FILE: {force_filename}

REQUIREMENTS:
{goal}

INSTRUCTIONS:
1. The primary target file is: {force_filename}
2. You MUST include modifications to {force_filename}
3. You MAY also modify other related files shown in the repository map
4. Use the repository map to identify ALL files that need changes
5. Generate ALL necessary files to fulfill the requirements completely

OUTPUT FORMAT:
## File: {force_filename}
```
[your code here]
```

If additional files are needed:
## File: additional_file.ext
```
[additional code here]
```

{existing_context if existing_context else ''}

Generate the complete, working implementation now.
"""
        else:
            # ========================================
            # æ—¢å­˜ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ãƒ­ã‚¸ãƒƒã‚¯
            # ========================================
            goal_lower = goal.lower()
            
            # â­ Multi-File Project Detection ã¯ Repository AnalyzeråˆæœŸåŒ–å¾Œã«å®Ÿè¡Œ
            # (ã“ã®æ™‚ç‚¹ã§ã¯ multi_file_detection ã¯æœªå®šç¾©)
            
            # æš«å®šçš„ã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®ãƒã‚§ãƒƒã‚¯ã®ã¿
            force_multiple_files_temp = (
                not skip_validation and (
                    any(keyword in goal_lower for keyword in [
                        "multi-file", "multiple files", "several files",
                        "create files", "full project", "complete project"
                    ])
                )
            )
            
            # åŸºæœ¬ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰ã¯ Repository AnalyzeråˆæœŸåŒ–å¾Œã«å®Ÿè¡Œ
            # (multi_file_detectionã®çµæœã‚’åæ˜ ã™ã‚‹ãŸã‚)

        # ========================================
        # â­â­â­ LLMå‘¼ã³å‡ºã—ã¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹å¤‰æ› â­â­â­
        # ========================================
        
        # LLMãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ãƒã‚§ãƒƒã‚¯
        if not self.llm_manager:
            logger.debug("   âš  LLM not available, using fallback code generation")
            return self._generate_fallback_code(goal)

        # å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ(tryãƒ–ãƒ­ãƒƒã‚¯ã®å¤–ã§)
        import signal
        import traceback

        try:
            # â­ repository_analyzer ã®çŠ¶æ…‹ã‚’ç¢ºèª
            if self.repository_analyzer:
                logger.debug(f"[VERIFY] repository_analyzer exists")
                
                
                # Repositoryåˆ†æã¯execute_semi_auto_implementationå†…ã§å®Ÿè¡Œæ¸ˆã¿
                # ã“ã“ã§ã¯çŠ¶æ…‹ç¢ºèªã®ã¿
                repo_data_count = len(self.repository_analyzer.memory.repository_data)
                logger.debug(f"[VERIFY] repository_data: {repo_data_count} files")
                
                # ãƒ‡ãƒãƒƒã‚°: repository_dataãŒç©ºã®å ´åˆï¼ˆæ–°è¦ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯æ­£å¸¸ï¼‰
                if repo_data_count == 0:
                    logger.debug(
                        "[DEBUG] _generate_code_implementation called with empty repository_data (new project). "
                        "Multi-File Detection may not work correctly if existing files are present. "
                        "This function should be called after Repository analysis."
                    )
            else:
                logger.debug("[VERIFY] repository_analyzer is None")


            # â­ Multi-File Detection ã‚’Repository AnalyzeråˆæœŸåŒ–å¾Œã«å®Ÿè¡Œ
            # force_filenameãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if force_filename:
                multi_file_detection = {'required': False, 'reason': 'force_filename specified', 'file_types': []}
                logger.debug(f"[Multi-File Detection] Skipped due to force_filename: {force_filename}")
            else:
                # ğŸ†• _project_structure ã‚’å‚ç…§ï¼ˆå†åˆ¤å®šãªã—ï¼‰
                if hasattr(self, '_project_structure') and self._project_structure:
                    file_types = self._project_structure.file_types
                    multi_file_detection = {
                        'required': bool(file_types),  # file_typesãŒã‚ã‚Œã°True
                        'file_types': file_types,
                        'reason': self._project_structure.reason
                    }
                    logger.debug(f"[Prompt Construction] Using cached project_structure: required={multi_file_detection['required']}, file_types={file_types}")
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ—¢å­˜ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡ºï¼ˆ_project_structureãŒæœªåˆæœŸåŒ–ã®å ´åˆï¼‰
                    multi_file_detection = self._detect_multi_file_requirement(goal)
                    logger.debug(f"[Prompt Construction] Fallback to keyword detection: required={multi_file_detection['required']}, file_types={multi_file_detection.get('file_types', [])}")
                    
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã®ã¿ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
                    if getattr(self, '_no_framework_mode', False):
                        goal_lower_local = goal.lower()
                        # æ˜ç¤ºçš„ã«React/Vueã‚‚è¦æ±‚ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ§‹æˆã¨åˆ¤æ–­
                        explicit_framework_request = (
                            ('react' in goal_lower_local and ('and react' in goal_lower_local or 'with react' in goal_lower_local or 'react for' in goal_lower_local)) or
                            ('vue' in goal_lower_local and ('and vue' in goal_lower_local or 'with vue' in goal_lower_local or 'vue for' in goal_lower_local))
                        )
                        
                        if not explicit_framework_request:
                            original_types = multi_file_detection.get('file_types', [])
                            framework_types = ('jsx', 'tsx', 'vue')
                            filtered_types = [ft for ft in original_types if ft not in framework_types]
                            
                            if filtered_types != original_types:
                                removed = [ft for ft in original_types if ft in framework_types]
                                logger.debug(f"[Framework Restriction] Removed framework file types: {removed}")
                                logger.debug(f"[Framework Restriction] file_types: {original_types} -> {filtered_types}")
                                multi_file_detection['file_types'] = filtered_types
                        else:
                            logger.debug(f"[Framework Restriction] Hybrid mode detected - keeping framework file types")

            # â­ multi_file_detectionã®çµæœã‚’åæ˜ 
            force_multiple_files = force_multiple_files_temp or multi_file_detection['required']
            logger.debug(f"[Prompt Construction] force_multiple_files={force_multiple_files} (temp={force_multiple_files_temp}, detected={multi_file_detection['required']})")
            
            # ========================================
            # â­ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ (multi_file_detectionã®çµæœã‚’ä½¿ç”¨)
            # ========================================
            
            # åŸºæœ¬ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰
            if existing_context and not force_multiple_files:
                # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ä¿®æ­£ãƒ¢ãƒ¼ãƒ‰
                logger.debug("[Prompt Construction] Single file modification mode")
                prompt = f"""MODIFY the existing file to achieve this goal: {goal}

            EXISTING FILE CONTENT:
            {existing_context}

            INSTRUCTIONS:
            1. You MUST preserve the existing code structure
            2. Only make necessary changes to fulfill the goal
            3. Do NOT completely rewrite the file
            4. Keep all existing functionality intact
            5. Add comments explaining what you changed

            {analysis if analysis else ''}

            {plan if plan else ''}

            Return the MODIFIED version of the file.
            """
            else:
                # è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¢ãƒ¼ãƒ‰ (æ–°è¦ä½œæˆã¾ãŸã¯è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ä¿®æ­£)
                logger.debug(f"[Prompt Construction] Multiple files mode (existing_context={'exists' if existing_context else 'none'}, force_multiple_files={force_multiple_files})")
                prompt = f"""Generate code to achieve this goal: {goal}

            {existing_context if existing_context else ''}

            {analysis if analysis else ''}

            {plan if plan else ''}
            """
            
            # è§£æ±ºç­–1: Multi-File Detectionã®çµæœã‹ã‚‰ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ 
            # å„ªå…ˆé †ä½1: multi_file_detection['framework'] ã‹ã‚‰ç›´æ¥å–å¾—
            detected_framework = multi_file_detection.get('framework')
            
            # å„ªå…ˆé †ä½2: reasonã‹ã‚‰ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’æŠ½å‡ºï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            if not detected_framework and multi_file_detection.get('reason'):
                reason = multi_file_detection['reason']
                # reasonã‹ã‚‰ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’æŠ½å‡º
                if 'React' in reason or 'react' in reason:
                    detected_framework = 'React'
                elif 'Vue' in reason or 'vue' in reason:
                    detected_framework = 'Vue'
                elif 'Angular' in reason or 'angular' in reason:
                    detected_framework = 'Angular'
                elif 'Svelte' in reason or 'svelte' in reason:
                    detected_framework = 'Svelte'
                elif 'Flask' in reason or 'flask' in reason:
                    detected_framework = 'Flask'
                elif 'Django' in reason or 'django' in reason:
                    detected_framework = 'Django'
                elif 'FastAPI' in reason or 'fastapi' in reason:
                    detected_framework = 'FastAPI'
            
            # å„ªå…ˆé †ä½3: goalã‹ã‚‰ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’æ¤œå‡ºï¼ˆè¿½åŠ ï¼‰
            if not detected_framework:
                goal_lower = goal.lower()
                if 'flask' in goal_lower:
                    detected_framework = 'Flask'
                elif 'django' in goal_lower:
                    detected_framework = 'Django'
                elif 'fastapi' in goal_lower:
                    detected_framework = 'FastAPI'
                elif 'react' in goal_lower:
                    detected_framework = 'React'
                elif 'vue' in goal_lower:
                    detected_framework = 'Vue'
            
            # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¿½åŠ 
            if detected_framework:
                logger.debug(f"[Prompt Construction] Framework detected: {detected_framework} (from {'framework' if multi_file_detection.get('framework') else 'reason/goal'})")
            
            # ğŸ†• ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼ˆReact/Vue/Angular/Svelteï¼‰ã¯_no_framework_modeæ™‚ã«ã‚¹ã‚­ãƒƒãƒ—
            frontend_frameworks = {'React', 'Vue', 'Angular', 'Svelte'}
            skip_framework_prompt = (
                getattr(self, '_no_framework_mode', False) and 
                detected_framework in frontend_frameworks
            )
            
            if skip_framework_prompt:
                logger.debug(f"[Prompt Construction] SKIPPING {detected_framework} framework prompt due to no-framework mode")
                detected_framework = None  # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å®Œå…¨ã«ã‚¹ã‚­ãƒƒãƒ—
            
            # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ 
            if detected_framework:
                logger.debug(f"[Prompt Construction] Detected framework: {detected_framework}")
                
                framework_prompts = {
                    'Flask': """
âš ï¸ FRAMEWORK REQUIREMENT: FLASK
This project MUST follow Flask conventions.

FLASK STATIC FILE RULES (CRITICAL - #1 CAUSE OF BROKEN APPS):
1. Static files are served from /static/ URL path by default
2. In HTML files, ALWAYS use absolute paths with /static/ prefix:
   âœ… CORRECT: href="/static/css/styles.css"
   âœ… CORRECT: src="/static/js/app.js"
   âŒ WRONG:   href="css/styles.css"  (relative path will 404!)
   âŒ WRONG:   src="js/app.js"        (relative path will 404!)

3. WHY this matters:
   - Flask serves index.html at URL '/' via send_from_directory
   - Browser resolves relative paths from '/', not '/static/'
   - Relative path 'css/styles.css' becomes '/css/styles.css' (404!)
   - Absolute path '/static/css/styles.css' always works

4. Directory structure:
   static/
     css/styles.css    â†’ accessed at /static/css/styles.css
     js/app.js         â†’ accessed at /static/js/app.js
     index.html        â†’ served at '/' but links must use /static/

5. For Jinja2 templates, prefer url_for():
   <link rel="stylesheet" href="{{ url_for('static', filename='css/styles.css') }}">

FLASK-SQLALCHEMY RULES:
1. db.create_all() MUST be inside app_context():
   with app.app_context():
       db.create_all()

2. NEVER use 'metadata' as column name (SQLAlchemy reserved)
3. NEVER use 'query' as column name (Flask-SQLAlchemy reserved)

âš ï¸ CRITICAL: Relative paths in Flask HTML files = broken CSS/JS = unusable app

""",
                    'Django': """
âš ï¸ FRAMEWORK REQUIREMENT: DJANGO
This project MUST use Django framework.

DJANGO STATIC FILE RULES:
1. Use {% load static %} in templates
2. Reference static files with {% static 'path/to/file' %}
3. Configure STATIC_URL and STATICFILES_DIRS in settings.py

""",
                    'FastAPI': """
âš ï¸ FRAMEWORK REQUIREMENT: FASTAPI
This project MUST use FastAPI framework.

FASTAPI REQUIREMENTS:
1. Use async def for route handlers when possible
2. Use Pydantic models for request/response validation
3. Mount static files with StaticFiles middleware

""",
                    'React': """
âš ï¸ FRAMEWORK REQUIREMENT: REACT
This project MUST use React framework.

MANDATORY REACT REQUIREMENTS:
1. ALL JavaScript files in src/ must import React:
   import React from 'react';

2. Create React components (functional or class):
   - Functional: function ComponentName() { ... } or const ComponentName = () => { ... }
   - Class: class ComponentName extends React.Component { ... }

3. Use JSX syntax for UI elements:
   - Return JSX: return <div>...</div>;
   - NOT vanilla DOM manipulation (no document.getElementById, etc.)

4. Export components:
   - export default ComponentName;

5. For index.js/main entry point:
   - Import ReactDOM: import ReactDOM from 'react-dom/client';
   - Render root: const root = ReactDOM.createRoot(document.getElementById('root'));
   - Use: root.render(<App />);

âš ï¸ CRITICAL: Every .js/.jsx/.tsx file in src/ directory MUST be React code with proper imports and JSX.

""",
                    'Vue': """
âš ï¸ FRAMEWORK REQUIREMENT: VUE
This project MUST use Vue framework.

MANDATORY VUE REQUIREMENTS:
1. Create Vue components with proper structure
2. Use Vue's template syntax
3. Export components properly

""",
                    'Angular': """
âš ï¸ FRAMEWORK REQUIREMENT: ANGULAR
This project MUST use Angular framework.

MANDATORY ANGULAR REQUIREMENTS:
1. Create Angular components with TypeScript
2. Use Angular decorators (@Component, etc.)
3. Follow Angular module structure

""",
                    'Svelte': """
âš ï¸ FRAMEWORK REQUIREMENT: SVELTE
This project MUST use Svelte framework.

MANDATORY SVELTE REQUIREMENTS:
1. Create Svelte components with .svelte extension
2. Use Svelte's reactive syntax
3. Export components properly

"""
                }
                
                prompt = framework_prompts.get(detected_framework, '') + prompt
                
                # ==========================================
                # ã€Solution 4ã€‘Reactãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå‘ã‘è¿½åŠ æŒ‡ç¤º
                # ==========================================
                if detected_framework == 'React':
                    react_additional_prompt = """
âš ï¸ REACT PROJECT MANDATORY FILES:

1. package.json (REQUIRED):
   - MUST include React dependencies
   - MUST include scripts for start/build
   - Example structure:
     {
       "name": "react-app",
       "version": "0.1.0",
       "dependencies": {
         "react": "^18.0.0",
         "react-dom": "^18.0.0"
       },
       "scripts": {
         "start": "react-scripts start",
         "build": "react-scripts build"
       },
       "devDependencies": {
         "react-scripts": "^5.0.0"
       }
     }

2. Component Structure:
   - EACH component should have its own .jsx/.js file
   - If goal specifies "components: TodoList, TodoItem, AddTodo"
     â†’ Create separate files: TodoList.jsx, TodoItem.jsx, AddTodo.jsx
   - EACH component should have a corresponding .css file if styling is needed
   - If component imports CSS (e.g., "import './TodoItem.css'")
     â†’ MUST create that CSS file

3. File Organization:
   - Components in src/components/ or src/ directory
   - Styles in src/styles/ or alongside components
   - ALWAYS create ALL files that are imported

âš ï¸ CRITICAL CHECKLIST:
âœ“ package.json exists with React dependencies
âœ“ Each component has its own file
âœ“ Each imported CSS file is created
âœ“ No missing file imports
âœ“ All components are properly exported

"""
                    prompt = react_additional_prompt + prompt
                    logger.debug("[Prompt Construction] Added React project mandatory files instruction")
            
            
            # ==========================================
            # ã€æ”¹å–„ã€‘App Patterns ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¼·åŒ–ï¼ˆapp_patterns.jsonæ´»ç”¨ï¼‰
            # ==========================================
            # AppPatternDetectorã‚’ä½¿ã£ã¦goalã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
            app_pattern_detector = AppPatternDetector(logger=logger)
            detected_app_pattern = app_pattern_detector.detect_pattern(goal)
            
            if detected_app_pattern:
                logger.debug(f"[Prompt Construction] App pattern detected: {detected_app_pattern}")
                pattern_enhancement = app_pattern_detector.get_prompt_enhancement(detected_app_pattern)
                if pattern_enhancement:
                    prompt = pattern_enhancement + "\n\n" + prompt
                    logger.debug(f"[Prompt Construction] Added app pattern enhancement ({len(pattern_enhancement)} chars)")
            
            # detected_pattern ã«åŸºã¥ããƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¼·åŒ–ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            if not detected_app_pattern and multi_file_detection.get('detected_pattern'):
                pattern = multi_file_detection['detected_pattern']
                enhancement = pattern.get('prompt_enhancement')
                
                if enhancement:
                    logger.debug(f"[Prompt Construction] Adding app pattern enhancement")
                    
                    # prompt_enhancementãŒDictï¼ˆæ”¹å–„ç‰ˆï¼‰ã®å ´åˆ
                    if isinstance(enhancement, dict):
                        # Base instruction
                        base_instruction = enhancement.get('base', '')
                        if base_instruction:
                            prompt += f"\n\nâš ï¸ APPLICATION PATTERN REQUIREMENT:\n{base_instruction}\n"
                        
                        # File requirements
                        files_section = enhancement.get('files', {})
                        if files_section:
                            prompt += "\n### File Requirements:\n\n"
                            for file_type, file_spec in files_section.items():
                                if isinstance(file_spec, dict):
                                    prompt += f"**{file_spec.get('filename', file_type)}:**\n"
                                    prompt += f"Purpose: {file_spec.get('purpose', 'N/A')}\n"
                                    
                                    requirements = file_spec.get('requirements', [])
                                    if requirements:
                                        prompt += "Requirements:\n"
                                        for req in requirements:
                                            prompt += f"- {req}\n"
                                    
                                    # Anti-patterns
                                    anti_patterns = file_spec.get('anti_patterns', [])
                                    if anti_patterns:
                                        prompt += "\nâŒ AVOID:\n"
                                        for ap in anti_patterns:
                                            prompt += f"- {ap}\n"
                                    prompt += "\n"
                        
                        # Quality requirements
                        quality_reqs = enhancement.get('quality_requirements', {})
                        if quality_reqs:
                            prompt += "### Quality Requirements:\n"
                            for key, value in quality_reqs.items():
                                prompt += f"- {key}: {value}\n"
                    
                    # prompt_enhancementãŒæ–‡å­—åˆ—ï¼ˆå¾“æ¥ç‰ˆï¼‰ã®å ´åˆ
                    elif isinstance(enhancement, str):
                        prompt += f"\n\nâš ï¸ APPLICATION PATTERN REQUIREMENT:\n{enhancement}\n"

            # è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã®æŒ‡ç¤ºã‚’è¿½åŠ ï¼ˆãƒ«ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ï¼‰
            if force_multiple_files:
                logger.debug("[Prompt Construction] Adding multi-file instruction (rule-based)")
                
                # Multi-File Detection ã®çµæœã‹ã‚‰ expected_files ã‚’æ§‹ç¯‰
                if multi_file_detection['required']:
                    # goalã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æŠ½å‡º
                    common_extensions = [
                        'py', 'js', 'jsx', 'ts', 'tsx',
                        'html', 'css', 'scss', 'sass', 'less',
                        'json', 'xml', 'yaml', 'yml',
                        'md', 'txt', 'sh', 'bat',
                        'java', 'cpp', 'c', 'h', 'hpp',
                        'go', 'rs', 'rb', 'php', 'swift', 'kt',
                        'sql', 'db', 'ini', 'conf', 'cfg', 'env'
                    ]
                    extensions_pattern = '|'.join(common_extensions)
                    detected_filenames = re.findall(rf'([\w\-]+\.(?:{extensions_pattern}))\b', goal.lower())
                    
                    expected_files = []
                    for filename in detected_filenames:
                        # æ‹¡å¼µå­ã‚’æŠ½å‡º
                        ext = filename.split('.')[-1]
                        expected_files.append(FileRequirement(
                            filepath=filename,
                            file_type=ext,
                            reason=f"Explicitly mentioned in goal",
                            priority="required"
                        ))
                    
                    # â­ æ–°è¦è¿½åŠ : file_types ã‹ã‚‰ expected_files ã‚’è‡ªå‹•æ§‹ç¯‰
                    # Goal ã«æ˜ç¤ºçš„ãªãƒ•ã‚¡ã‚¤ãƒ«åãŒãªã„å ´åˆã€file_types ã‹ã‚‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
                    detected_file_types = multi_file_detection.get('file_types', [])
                    if detected_file_types and not expected_files:
                        logger.debug(f"[Prompt Construction] Building expected_files from file_types: {detected_file_types}")
                        
                        # file_types ã«åŸºã¥ã„ã¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ 
                        default_files_for_types = {
                            'html': ('index.html', 'html'),
                            'css': ('styles.css', 'css'),
                            'js': ('app.js', 'js'),
                            'jsx': ('src/App.jsx', 'jsx'),
                            'ts': ('src/main.ts', 'ts'),
                            'tsx': ('src/App.tsx', 'tsx'),
                            'python': ('main.py', 'py'),
                        }
                        
                        for file_type in detected_file_types:
                            if file_type in default_files_for_types:
                                default_file, ext = default_files_for_types[file_type]
                                expected_files.append(FileRequirement(
                                    filepath=default_file,
                                    file_type=ext,
                                    reason=f"Required for web project (detected file_type: {file_type})",
                                    priority="required"
                                ))
                        
                        logger.debug(f"[Prompt Construction] Auto-generated expected_files from file_types: {[f.filepath for f in expected_files]}")
                    
                    # HTML/Webãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆã€index.htmlã‚’è‡ªå‹•è¿½åŠ 
                    goal_lower = goal.lower()
                    web_keywords = ['landing page', 'website', 'web page', 'web app', 'webapp', 'home page', 'portfolio']
                    css_detected = any(f.file_type in ['css', 'scss', 'sass', 'less'] for f in expected_files)
                    html_detected = any(f.file_type == 'html' for f in expected_files)
                    is_web_project = any(keyword in goal_lower for keyword in web_keywords)
                    
                    if is_web_project and css_detected and not html_detected:
                        # CSSãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ã®ã«HTMLãŒãªã„ â†’ index.htmlã‚’è¿½åŠ 
                        expected_files.append(FileRequirement(
                            filepath='index.html',
                            file_type='html',
                            reason=f"Inferred from web project type and CSS files",
                            priority="required"
                        ))
                        logger.debug("[Prompt Construction] Auto-added index.html for web project")
                    
                    logger.debug(f"[Prompt Construction] Constructed expected_files: {[f.filepath for f in expected_files]}")
                else:
                    expected_files = None
                
                multi_file_prompt = self._build_multi_file_prompt(
                    expected_files=expected_files,
                    max_phase=3  # Phase 1-3ã®ã™ã¹ã¦ã®ãƒ«ãƒ¼ãƒ«ã‚’é©ç”¨
                )
                prompt += multi_file_prompt

            # Phase 1.1: å¤šæ®µéšç”Ÿæˆã®åˆ¤å®šã¯ execute_semi_auto_implementation ã§å®Ÿè¡Œæ¸ˆã¿
            # ã“ã“ã§ã¯ skip_multi_stage=True ã§å‘¼ã°ã‚Œã‚‹ãŸã‚ã€ã“ã®æ¡ä»¶ã¯å¸¸ã«False
            # ï¼ˆå¿µã®ãŸã‚æ¡ä»¶ã‚’æ®‹ã™ãŒã€å®Ÿéš›ã«ã¯é€šéã—ãªã„ï¼‰
            # ===== ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚° =====
                        # =======================
            
            if complexity == "complex" and multi_file_detection['required'] and not skip_multi_stage:
                # ã“ã®åˆ†å²ã¯åˆ°é”ã—ãªã„ã¯ãšï¼ˆexecute_semi_auto_implementationã§å‡¦ç†æ¸ˆã¿ï¼‰
                logger.warning("Unexpected multi-stage generation trigger in _generate_code_implementation")
                result = self._execute_multi_stage_generation(goal, complexity)
                # ã‚¿ãƒ—ãƒ«ã®æœ€åˆã®è¦ç´ ï¼ˆall_filesï¼‰ã®ã¿ã‚’è¿”ã™
                return result[0] if result else {}

            # â­ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—(repository_analyzerã®ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€)
            project_context = self._get_project_context()
            logger.debug(f"[LLM] Project context length: {len(project_context)} chars")
            logger.debug(f"[VERIFY] Project context length: {len(project_context)} chars")
            logger.debug(f"[VERIFY] Project context content:\n{project_context}")  #

            
            # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°: LLMå‘¼ã³å‡ºã—é–‹å§‹
            logger.debug(f"[VERIFY] Starting code generation for goal: {goal[:100]}...")  #
            logger.debug(f"[LLM] Starting code generation for goal: {goal[:100]}...")
            logger.debug(f"[VERIFY] Prompt length: {len(prompt)} chars")  #
            logger.debug(f"[LLM] Prompt length: {len(prompt)} chars")
        
            # LLMå‘¼ã³å‡ºã—(ã‚¹ãƒ”ãƒŠãƒ¼ä»˜ã)
            logger.debug("[LLM] Entering Spinner context...")
            
             # Phase 1.1: StepProgressã®ã‚¹ãƒ”ãƒŠãƒ¼ã®ã¿ã‚’ä½¿ç”¨ï¼ˆpause/resumeå‰Šé™¤ï¼‰
            try:
                logger.debug("[LLM] Calling llm_manager.generate_response()...")                
            
                def timeout_handler(signum, frame):
                    raise TimeoutError("LLM generation timed out after 60 seconds")
            
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š(Linuxã®ã¿)
                if hasattr(signal, 'SIGALRM'):
                    signal.signal(signal.SIGALRM, timeout_handler)
                    signal.alarm(60)  # 60ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            
                # æ”¹å–„ç­–A: è¤‡é›‘æ€§ã«å¿œã˜ã¦max_tokensã‚’è¨­å®š
                if complexity == "simple":
                    max_tokens = 2000
                elif complexity == "medium":
                    max_tokens = 4000
                elif complexity == "complex":
                    max_tokens = 8000
                else:
                    max_tokens = 4000
                
                # Multi-Fileç”Ÿæˆã®å ´åˆã€max_tokensã‚’å¢—ã‚„ã™ï¼ˆè¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆã«ã¯ååˆ†ãªãƒˆãƒ¼ã‚¯ãƒ³ãŒå¿…è¦ï¼‰
                # 
                # ä¿®æ­£: 2025-12-29, 2025-01-22
                # å•é¡Œ: styles.cssç­‰ãŒé€”ä¸­ã§é€”åˆ‡ã‚Œã‚‹ï¼ˆmax_tokensä¸è¶³ï¼‰
                # å¯¾ç­–: å„è¤‡é›‘åº¦ã®max_tokensã‚’å¢—åŠ 
                #   - simple: 8000 â†’ 16000 (2025-01-22: 12000â†’16000ã«å†å¢—åŠ )
                #   - medium: 12000 â†’ 16000
                #   - complex: 16000 â†’ å¤‰æ›´ãªã—
                if force_multiple_files and multi_file_detection['required']:
                    # äºˆæƒ³ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã«å¿œã˜ã¦å¢—ã‚„ã™
                    # æ³¨æ„: max_tokens > 16000 ã¯ Anthropic API ã§
                    # "Streaming is required for operations that may take longer than 10 minutes"
                    # ã‚¨ãƒ©ãƒ¼ã‚’å¼•ãèµ·ã“ã™å¯èƒ½æ€§ãŒã‚ã‚‹
                    if complexity == "simple":
                        max_tokens = 16000
                    elif complexity == "medium":
                        max_tokens = 16000
                    elif complexity == "complex":
                        max_tokens = 16000
                    logger.debug(f"[LLM] Multi-file detected, increased max_tokens to {max_tokens}")
                
                logger.debug(f"[LLM] Complexity: {complexity}, max_tokens: {max_tokens}")
                
                try:
                    response = self.llm_manager.generate_response(
                        prompt=prompt,
                        context=project_context,  # â­ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’LLMã«é€ä¿¡
                        system_prompt=self._get_code_generation_system_prompt(
                            goal_constraints=getattr(self, '_current_goal_constraints', None)
                        ),
                        max_tokens=max_tokens
                    )
                    logger.debug(f"[LLM] Response received. Type: {type(response)}")
                    
                    # â­ ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°: çœŸã®åŸå› ç‰¹å®šç”¨
                    # LLMResponse: finish_reason (not stop_reason), usage is Dict
                    if hasattr(response, 'finish_reason'):
                        logger.debug(f"[LLM] finish_reason: {response.finish_reason}")
                    if hasattr(response, 'usage') and isinstance(response.usage, dict):
                        usage = response.usage
                        input_tokens = usage.get('prompt_tokens', 'N/A')
                        output_tokens = usage.get('completion_tokens', 'N/A')
                        logger.debug(f"[LLM] usage: input_tokens={input_tokens}, output_tokens={output_tokens}, max_tokens={max_tokens}")
                    if hasattr(response, 'content') and response.content:
                        tail = response.content[-200:] if len(response.content) > 200 else response.content
                        logger.debug(f"[LLM] output_tail (last 200 chars): {repr(tail)}")
                    
                    # â­ ç¶šãç”Ÿæˆ: finish_reason == 'max_tokens' or 'length' ã®å ´åˆã€ç¶šãã‚’è‡ªå‹•ç”Ÿæˆ
                    # å‚è€ƒ: Azure OpenAI/ä¸€èˆ¬çš„ãªLLMãƒ„ãƒ¼ãƒ«ã®å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³
                    # æ³¨: OpenAI APIã¯'length'ã€Anthropic APIã¯'max_tokens'ã‚’è¿”ã™
                    accumulated_content = response.content if hasattr(response, 'content') else ""
                    continuation_count = 0
                    max_continuations = 5  # ç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢
                    
                    while (hasattr(response, 'finish_reason') and 
                           response.finish_reason in ('max_tokens', 'length') and 
                           continuation_count < max_continuations):
                        continuation_count += 1
                        logger.debug(f"[LLM] Continuation {continuation_count}/{max_continuations}: finish_reason was max_tokens, requesting continuation...")
                        
                        # ç¶šãç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: æœ«å°¾1000æ–‡å­—ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦æ¸¡ã™
                        context_tail = accumulated_content[-1000:] if len(accumulated_content) > 1000 else accumulated_content
                        continuation_prompt = f"""Continue generating code from where you left off.

The previous output ended with:
```
{context_tail}
```

IMPORTANT:
- Continue from EXACTLY where you stopped
- Do NOT repeat any code that was already generated
- Do NOT add any preamble or explanation, just continue the code
- Maintain the same file format (```filename markers)"""
                        
                        # â­ ä¿®æ­£: å…ƒã®Goalã¨LLMã®é€”ä¸­ã¾ã§ã®å›ç­”ã‚’conversation_historyã¨ã—ã¦æ¸¡ã™
                        # ã“ã‚Œã«ã‚ˆã‚Šã€LLMã¯ã€Œä½•ã‚’ç”Ÿæˆã—ã‚ˆã†ã¨ã—ã¦ã„ãŸã‹ã€ã‚’æŠŠæ¡ã—ãŸçŠ¶æ…‹ã§ç¶šãã‚’ç”Ÿæˆã§ãã‚‹
                        conversation_history_for_continuation = [
                            {"role": "user", "content": prompt},  # å…ƒã®Goal
                            {"role": "assistant", "content": accumulated_content}  # LLMã®é€”ä¸­ã¾ã§ã®å›ç­”
                        ]
                        
                        continuation_response = self.llm_manager.generate_response(
                            prompt=continuation_prompt,
                            context=project_context,
                            system_prompt=self._get_code_generation_system_prompt(
                                goal_constraints=getattr(self, '_current_goal_constraints', None)
                            ),
                            max_tokens=max_tokens,
                            conversation_history=conversation_history_for_continuation  # â­ è¿½åŠ 
                        )
                        
                        # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°
                        if hasattr(continuation_response, 'finish_reason'):
                            logger.debug(f"[LLM] Continuation {continuation_count}: finish_reason: {continuation_response.finish_reason}")
                        if hasattr(continuation_response, 'usage') and isinstance(continuation_response.usage, dict):
                            c_usage = continuation_response.usage
                            c_output = c_usage.get('completion_tokens', 'N/A')
                            logger.debug(f"[LLM] Continuation {continuation_count}: output_tokens={c_output}")
                        
                        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç´¯ç©
                        if hasattr(continuation_response, 'content') and continuation_response.content:
                            # â­ ä¿®æ­£: continuationå‡ºåŠ›ã®å…ˆé ­ã«ã‚ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ãƒãƒ¼ã‚«ãƒ¼ã‚’é™¤å»
                            # LLMãŒç¶šãã‚’ç”Ÿæˆã™ã‚‹éš›ã«æ–°ã—ã„ã€Œ## File: xxxã€ã‚„```ãƒãƒ¼ã‚«ãƒ¼ã‚’ä»˜ã‘ã¦ã—ã¾ã†å•é¡Œã‚’ä¿®æ­£
                            # ã“ã‚Œã‚’æ®‹ã™ã¨_extract_code_blocksãŒ2ã¤ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¨ã—ã¦è§£é‡ˆã—ã€
                            # å¾Œè€…ãŒå‰è€…ã‚’ä¸Šæ›¸ãã—ã¦HTMLã®å‰åŠãŒå¤±ã‚ã‚Œã‚‹
                            cont_content = continuation_response.content
                            # ãƒ‘ã‚¿ãƒ¼ãƒ³1: å…ˆé ­ã®ã€Œ## File: xxxã€ãƒ˜ãƒƒãƒ€ãƒ¼ + ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ãƒãƒ¼ã‚«ãƒ¼ã‚’é™¤å»
                            cont_content = re.sub(r'^(?:##\s*File:\s*[^\n]*\r?\n)?```\w*\r?\n?', '', cont_content)
                            # ãƒ‘ã‚¿ãƒ¼ãƒ³2: å…ˆé ­ãŒã€Œ## File: xxxã€ã®ã¿ï¼ˆ```ãªã—ï¼‰ã®å ´åˆã‚‚é™¤å»
                            cont_content = re.sub(r'^##\s*File:\s*[^\n]*\r?\n', '', cont_content)
                            accumulated_content += cont_content
                            logger.debug(f"[LLM] Continuation {continuation_count}: Added {len(cont_content)} chars, total: {len(accumulated_content)}")
                        
                        # æ¬¡ã®ãƒ«ãƒ¼ãƒ—åˆ¤å®šç”¨ã«responseã‚’æ›´æ–°
                        response = continuation_response
                    
                    # ç´¯ç©ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã§responseã‚’æ›´æ–°ï¼ˆç¶šãç”ŸæˆãŒã‚ã£ãŸå ´åˆï¼‰
                    if continuation_count > 0:
                        if hasattr(response, 'content'):
                            response.content = accumulated_content
                        logger.debug(f"[LLM] Continuation completed: {continuation_count} additional call(s), final content length: {len(accumulated_content)} chars")
                    
                finally:
                    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè§£é™¤
                    if hasattr(signal, 'SIGALRM'):
                        signal.alarm(0)
            except Exception as e:
                logger.error(f"[LLM] Generation failed: {e}")
                raise

            logger.debug("[LLM] LLM call completed successfully")
        
            # â­â­â­ ã“ã“ãŒæœ€é‡è¦: LLMResponse â†’ æ–‡å­—åˆ—å¤‰æ› â­â­â­
            if hasattr(response, 'content'):
                # LLMResponse.content å±æ€§ã‚’å–å¾—
                response_text = response.content
                logger.debug(f"[LLM] Extracted content from response.content (length: {len(response_text)})")
            elif isinstance(response, str):
                # æ—¢ã«æ–‡å­—åˆ—ã®å ´åˆ
                response_text = response
                logger.debug(f"[LLM] Response is already string (length: {len(response_text)})")
            else:
                # ãã®ä»–ã®å ´åˆã¯ str() ã§å¤‰æ›
                response_text = str(response)
                logger.debug(f"[LLM] Converted response to string (length: {len(response_text)})")
        
        except TimeoutError as e:
            logger.error(f"[LLM] Timeout error: {e}")
            logger.debug(f"   âš  LLM generation timed out after 60 seconds")
            logger.debug(f"   â„¹ï¸ Falling back to template code generation")
            return self._generate_fallback_code(goal)
        except Exception as e:
            logger.error(f"[LLM] Generation failed with exception: {type(e).__name__}: {e}")
            logger.debug(f"[LLM] Full traceback: {traceback.format_exc()}")
            logger.debug(f"   âš  LLM generation failed: {e}")
            logger.debug(f"   â„¹ï¸ Falling back to template code generation")
            return self._generate_fallback_code(goal)
        
        
        # ========================================
        # â­ ã‚³ãƒ¼ãƒ‰ã®æŠ½å‡ºã¨æ•´å½¢(response_text ã‚’ä½¿ç”¨)
        # ========================================
        cleaned_files = self._extract_code_blocks(response_text)
        
        # â­ ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°: _extract_code_blocks ã®çµæœ
        logger.debug(f"[Code Extraction] _extract_code_blocks returned {len(cleaned_files)} file(s)")
        if cleaned_files:
            for fn, code in cleaned_files.items():
                logger.debug(f"[Code Extraction]   - {fn}: {len(code)} chars")
        else:
            logger.debug(f"[Code Extraction]   (empty result)")
        
        
        # ========================================
        # â­ force_filename ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã®å¼·åˆ¶ä¿®æ­£
        # ========================================
        if force_filename:
            logger.debug(f"[Code Extraction] force_filename specified: {force_filename}")
            
            # cleaned_files ã«ã‚³ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆ
            if cleaned_files:
                logger.debug(f"[Code Extraction] Branch: cleaned_files has content")
                # LLMãŒè¿”ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«åã«é–¢ã‚ã‚‰ãšã€force_filename ã§çµ±ä¸€
                forced_files = {}
                first_code = None
                original_filename = None
                
                # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸã‚³ãƒ¼ãƒ‰ã‚’å–å¾—
                for filename, code in cleaned_files.items():
                    first_code = code
                    original_filename = filename
                    break
                
                if first_code:
                    # ãƒ•ã‚¡ã‚¤ãƒ«åãŒä¸€è‡´ã—ãªã„å ´åˆã¯è­¦å‘Šã‚’å‡ºåŠ›
                    if original_filename != force_filename:
                        logger.debug(f"[Auto-completion] LLM returned incorrect filename: {original_filename}")
                        logger.debug(f"[Auto-completion] Correcting to specified filename: {force_filename}")
                    
                    logger.debug(f"[Code Extraction] Returning {force_filename}: {len(first_code)} chars")
                    forced_files[force_filename] = first_code
                    return forced_files
                else:
                    logger.debug(f"[Code Extraction] WARNING: first_code is empty/None")
            
            # ã‚³ãƒ¼ãƒ‰ãŒæŠ½å‡ºã§ããªã‹ã£ãŸå ´åˆ
            # (LLMãŒ ## File: å½¢å¼ã‚’è¿”ã•ãªã‹ã£ãŸå ´åˆ)
            if response_text.strip():
                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å…¨ä½“ã‚’ã‚³ãƒ¼ãƒ‰ã¨ã—ã¦æ‰±ã†
                logger.debug("[Auto-completion] No ## File: format detected in LLM response")
                logger.debug(f"[Auto-completion] Using entire response as code for: {force_filename}")
                logger.debug(f"[Code Extraction] Branch: Fallback (response_text as code)")
                logger.debug(f"[Code Extraction] response_text length: {len(response_text)}")
                
                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ä½™è¨ˆãªèª¬æ˜æ–‡ã‚’é™¤å»
                code_content = response_text.strip()
                
                # "Based on..." ãªã©ã®èª¬æ˜æ–‡ã‚’é™¤å»
                if code_content.startswith("Based on"):
                    lines = code_content.split('\n')
                    # æœ€åˆã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã¾ãŸã¯å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰ã‚’æ¢ã™
                    code_start = 0
                    for i, line in enumerate(lines):
                        if line.strip().startswith('const ') or \
                        line.strip().startswith('import ') or \
                        line.strip().startswith('module.') or \
                        line.strip().startswith('function ') or \
                        line.strip().startswith('class ') or \
                        line.strip().startswith('def ') or \
                        line.strip().startswith('package ') or \
                        line.strip().startswith('<?php') or \
                        line.strip().startswith('<!DOCTYPE'):
                            code_start = i
                            break
                    
                    if code_start > 0:
                        code_content = '\n'.join(lines[code_start:])
                
                # â­ ä¿®æ­£: ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³æ§‹æ–‡ã‚’é™¤å»ï¼ˆæ”¹å–„4ï¼‰
                before_clean = len(code_content)
                code_content = self._clean_generated_code(code_content)
                logger.debug(f"[Code Extraction] After _clean_generated_code: {before_clean} -> {len(code_content)} chars")
                
                if code_content.strip():
                    logger.debug(f"[Code Extraction] Returning fallback code for {force_filename}")
                    return {force_filename: code_content}
                else:
                    logger.debug(f"[Code Extraction] WARNING: code_content became empty after cleaning!")
            else:
                logger.debug(f"[Code Extraction] Branch: Empty response, using template")
                # å®Œå…¨ã«ç©ºã®å ´åˆã¯ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’è¿”ã™
                logger.debug(f"   âš  LLM returned empty response")
                logger.debug(f"   âœ… Creating template file: {force_filename}")
                
                # ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã‹ã‚‰è¨€èªã‚’åˆ¤å®š
                ext = force_filename.split('.')[-1] if '.' in force_filename else ''
                
                if ext == 'js':
                    template = """// TODO: Implement this file

    module.exports = {};
    """
                elif ext == 'jsx':
                    template = """import React from 'react';

    const Component = () => {
    return (
        <div>
        {/* TODO: Implement this component */}
        </div>
    );
    };

    export default Component;
    """
                elif ext == 'py':
                    template = """# TODO: Implement this file

    def main():
        pass

    if __name__ == "__main__":
        main()
    """
                elif ext == 'html':
                    template = """<!DOCTYPE html>
    <html lang="en">
    <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TODO</title>
    </head>
    <body>
    <!-- TODO: Implement this page -->
    </body>
    </html>
    """
                elif ext == 'css':
                    template = """/* TODO: Implement styles */

    body {
    margin: 0;
    padding: 0;
    }
    """
                elif ext == 'json':
                    template = """{
    "todo": "Implement this configuration"
    }
    """
                else:
                    template = "// TODO: Implement this file\n"
                
                return {force_filename: template}
        
        
        # ========================================
        # force_filename ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆ(æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯)
        # ========================================
        if not cleaned_files:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚¡ã‚¤ãƒ«åã®ç”Ÿæˆ
            # ğŸ†• ä¿®æ­£: response_textã®å®Ÿéš›ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ç¨®åˆ¥ã‚’å„ªå…ˆåˆ¤å®šã™ã‚‹ã€‚
            # å•é¡Œ: LLMãŒã€Œ## File: neon_tetris.htmlã€å½¢å¼ã§HTMLã‚’è¿”ã—ãŸãŒ
            # _extract_code_blocksãŒæŠ½å‡ºã«å¤±æ•—ã—ãŸå ´åˆï¼ˆfinish_reason: lengthç­‰ï¼‰ã€
            # goalã«ã€Œjavascriptã€ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã¨script.jsã¨èª¤åˆ¤å®šã•ã‚Œã€
            # auto-completionå¾Œã‚‚script.jsãŒgenerated_codeã«æ®‹ã‚Šç¶šã‘ã‚‹ã€‚
            code_content = response_text.strip()
            
            # Step 1: response_textã‹ã‚‰ã€Œ## File: xxxã€ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æŠ½å‡ºã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—
            file_header_match = re.match(r'^##\s*File:\s*(\S+)', code_content)
            if file_header_match:
                primary_file = file_header_match.group(1)
                logger.debug(f"[Code Extraction] Detected file header in response: {primary_file}")
                # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã¨ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ãƒãƒ¼ã‚«ãƒ¼ã‚’é™¤å»
                lines = code_content.split('\n')
                clean_start = 0
                for i, line in enumerate(lines):
                    stripped = line.strip()
                    if stripped.startswith('## File:') or stripped.startswith('```'):
                        clean_start = i + 1
                    else:
                        break
                code_content = '\n'.join(lines[clean_start:])
                # æœ«å°¾ã®```ã‚‚é™¤å»
                if code_content.rstrip().endswith('```'):
                    code_content = code_content.rstrip()[:-3].rstrip()
            else:
                # Step 2: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å®Ÿéš›ã®å†…å®¹ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ç¨®åˆ¥ã‚’åˆ¤å®š
                content_lower = code_content[:2000].lower()
                
                if '<!doctype html' in content_lower or '<html' in content_lower:
                    primary_file = "index.html"
                    logger.debug("[Code Extraction] Content-based detection: HTML document")
                else:
                    # Step 3: goalã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ã‚ˆã‚‹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆå¾“æ¥ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
                    goal_lower = goal.lower()
                    
                    if "python" in goal_lower or "py " in goal_lower:
                        primary_file = "main.py"
                    elif "javascript" in goal_lower or "js " in goal_lower:
                        primary_file = "script.js"
                    elif "html" in goal_lower:
                        primary_file = "index.html"
                    elif "react" in goal_lower:
                        primary_file = "App.jsx"
                    elif "css" in goal_lower:
                        primary_file = "style.css"
                    elif "typescript" in goal_lower or "ts " in goal_lower:
                        primary_file = "main.ts"
                    else:
                        primary_file = "code.txt"
            
            return {primary_file: code_content}
        

        # ========================================
        # Layer 3: äº‹å¾Œæ¤œè¨¼ + è‡ªå‹•è£œå®Œ
        # ========================================
        if not skip_validation and 'multi_file_detection' in locals() and multi_file_detection and multi_file_detection.get('required', False):
            logger.debug("[Code Generation] Starting post-generation validation...")
            logger.debug(f"[Post-Validation] Checking for incomplete multi-file generation...")
            cleaned_files = self.post_validator.validate_and_complete(
                generated_files=cleaned_files,
                goal=goal,
                multi_file_detection=multi_file_detection
            )
            logger.debug(f"[Code Generation] After validation: {len(cleaned_files)} files")
            logger.debug(f"[Post-Validation] Final file count: {len(cleaned_files)}")


        # è¿½åŠ : ãƒ•ã‚¡ã‚¤ãƒ«é–“æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        consistency_issues = self._validate_file_consistency(cleaned_files)
        if consistency_issues:
            logger.debug("\nâš   File Consistency Issues Detected:")
            for issue in consistency_issues:
                logger.debug(f"   â€¢ {issue}")
            logger.debug("\nâš™ These issues may cause runtime errors and should be fixed.")

        return cleaned_files


    def _validate_file_consistency(self, files: Dict[str, str]) -> List[str]:
        """
        ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«é–“ã®æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯
        
        Args:
            files: {filename: code}
        
        Returns:
            æ•´åˆæ€§ã®å•é¡Œãƒªã‚¹ãƒˆ
        """
        issues = []
        
        # â­ ä¿®æ­£B: æœªå®Ÿè£…ãƒ¡ã‚½ãƒƒãƒ‰æ¤œå‡ºï¼ˆpass onlyï¼‰
        issues.extend(self._check_unimplemented_methods(files))
        
        # HTMLã¨JavaScriptã®ãƒšã‚¢ã‚’æ¢ã™
        html_files = {f: c for f, c in files.items() if f.endswith('.html')}
        js_files = {f: c for f, c in files.items() if f.endswith('.js')}
        
        for html_file, html_code in html_files.items():
            for js_file, js_code in js_files.items():
                # HTMLã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã¦ã„ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æŠ½å‡º
                method_calls = re.findall(r'calculator\.(\w+)\(', html_code)
                
                # JavaScriptã§å®šç¾©ã•ã‚Œã¦ã„ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æŠ½å‡º
                method_definitions = re.findall(r'^\s*(\w+)\s*\([^)]*\)\s*\{', js_code, re.MULTILINE)
                
                # å‘¼ã³å‡ºã•ã‚Œã¦ã„ã‚‹ãŒå®šç¾©ã•ã‚Œã¦ã„ãªã„ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æ¤œå‡º
                undefined_methods = set(method_calls) - set(method_definitions)
                for method in undefined_methods:
                    issues.append(
                        f"{html_file}: Method 'calculator.{method}()' is called but not defined in {js_file}"
                    )
                
                # dataå±æ€§ã®å€¤ãƒã‚§ãƒƒã‚¯ï¼ˆdata-number ãªã©ãŒç©ºã§ãªã„ã‹ï¼‰
                empty_data_attrs = re.findall(r'data-(\w+)>(?!\w)', html_code)
                if empty_data_attrs:
                    for attr in set(empty_data_attrs):
                        issues.append(
                            f"{html_file}: data-{attr} attribute has no value (should be data-{attr}=\"value\")"
                        )
        
        return issues
    
    def _check_unimplemented_methods(self, files: Dict[str, str]) -> List[str]:
        """
        â­ ä¿®æ­£B: æœªå®Ÿè£…ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆpass onlyï¼‰ã‚’æ¤œå‡º
        
        def method_name(...):
            pass
        
        ã¾ãŸã¯
        
        def method_name(...):
            '''docstring'''
            pass
        
        ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºã—ã€è­¦å‘Šã‚’è¿”ã™ã€‚
        
        Args:
            files: {filename: code}
        
        Returns:
            æœªå®Ÿè£…ãƒ¡ã‚½ãƒƒãƒ‰ã®è­¦å‘Šãƒªã‚¹ãƒˆ
        """
        issues = []
        
        for filename, code in files.items():
            if not filename.endswith('.py'):
                continue
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³: def xxx(...): ã®å¾Œã« pass ã®ã¿ï¼ˆdocstringã‚ã‚Š/ãªã—ï¼‰
            pattern = re.compile(
                r'def\s+(\w+)\s*\([^)]*\)\s*(?:->\s*[^:]+)?\s*:\s*\n'  # def xxx(...) -> type:
                r'(?:\s*(?:"""[\s\S]*?"""|\'\'\'[\s\S]*?\'\'\')\s*\n)?'  # optional docstring
                r'\s*pass\s*(?:\n|$)',  # pass
                re.MULTILINE
            )
            
            matches = pattern.findall(code)
            
            for method_name in matches:
                # __init__ ã‚„ __repr__ ãªã©ã®ç‰¹æ®Šãƒ¡ã‚½ãƒƒãƒ‰ã§ pass ã®ã¿ã¯è¨±å®¹
                if method_name.startswith('__') and method_name.endswith('__'):
                    continue
                
                issues.append(
                    f"{filename}: Method '{method_name}()' is not implemented (pass only)"
                )
                logger.debug(f"[Consistency] Unimplemented method detected: {filename}:{method_name}()")
        
        return issues


    def _build_multi_file_instruction(self, force: bool, file_types: List[str], attempt: int, detection_result: dict) -> str:
        """
        è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«æŒ‡ç¤ºã‚’æ§‹ç¯‰(ãƒªãƒˆãƒ©ã‚¤æ™‚ã«å¼·åº¦ã‚’ä¸Šã’ã‚‹)
        
        Args:
            force: å¼·åˆ¶ãƒ•ãƒ©ã‚°
            file_types: æ¤œå‡ºã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ç¨®åˆ¥
            attempt: ãƒªãƒˆãƒ©ã‚¤å›æ•° (0-indexed)
            detection_result: æ¤œå‡ºçµæœ
            
        Returns:
            ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæŒ‡ç¤ºæ–‡å­—åˆ—
        """
        if not force:
            # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰
            return """
    If this implementation would benefit from multiple files (e.g., separation of concerns,
    modularity, or maintainability), you may generate multiple files using this format:

    ## File: filename1.ext
    ```language
    [code]
    ```

    ## File: filename2.ext
    ```language
    [code]
    ```

    Otherwise, a single file is acceptable.
    """
        
        # å¼·åˆ¶ãƒ¢ãƒ¼ãƒ‰ - ãƒªãƒˆãƒ©ã‚¤å›æ•°ã«å¿œã˜ã¦å¼·åº¦ã‚’ä¸Šã’ã‚‹
        if attempt == 0:
            # 1å›ç›®: é€šå¸¸ã®å¼·åˆ¶æŒ‡ç¤º
            emphasis = "CRITICAL REQUIREMENT"
            you_must = "You MUST"
        elif attempt == 1:
            # 2å›ç›®: å¼·èª¿
            emphasis = "âš  CRITICAL REQUIREMENT - SECOND ATTEMPT âš "
            you_must = "You are REQUIRED to"
        else:
            # 3å›ç›®: æœ€å¤§å¼·èª¿
            emphasis = "ğŸš¨ FINAL CRITICAL REQUIREMENT - LAST ATTEMPT ğŸš¨"
            you_must = "It is MANDATORY that you"
        
        # æ¤œå‡ºã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ç¨®åˆ¥ã«å¿œã˜ãŸå…·ä½“ä¾‹ã‚’ç”Ÿæˆ
        if 'html' in file_types and 'css' in file_types and 'js' in file_types:
            file_examples = """
    ## File: calculator.html
    ```html
    <!DOCTYPE html>
    <html>
    <head>
        <link rel="stylesheet" href="styles.css">
    </head>
    <body>
        <div id="calculator"></div>
        <script src="script.js"></script>
    </body>
    </html>
    ```

    ## File: styles.css
    ```css
    body {
        font-family: Arial, sans-serif;
    }
    #calculator {
        margin: 50px auto;
    }
    ```

    ## File: script.js
    ```javascript
    function calculate() {
        // Calculator logic
    }
    document.addEventListener('DOMContentLoaded', calculate);
    ```
    """
        elif 'python' in file_types:
            file_examples = """
    ## File: main.py
    ```python
    from models import Calculator
    from utils import validate_input

    def main():
        calc = Calculator()
        # Main logic
    ```

    ## File: models.py
    ```python
    class Calculator:
        def __init__(self):
            pass
    ```

    ## File: utils.py
    ```python
    def validate_input(value):
        # Validation logic
        pass
    ```
    """
        else:
            file_examples = """
    ## File: filename1.ext
    ```language
    [code for file 1]
    ```

    ## File: filename2.ext
    ```language
    [code for file 2]
    ```

    ## File: filename3.ext
    ```language
    [code for file 3]
    ```
    """
        
        instruction = f"""
    {'='*80}
    {emphasis}: MULTIPLE SEPARATE FILES
    {'='*80}

    USER EXPLICITLY REQUESTED: {detection_result['reason']}

    {you_must} generate SEPARATE files. This is NOT optional.

    Required file types: {', '.join(file_types) if file_types else 'separate modules'}

    MANDATORY FORMAT for each file:
    {file_examples}

    IMPORTANT RULES:
    1. Each file MUST start with "## File: filename.ext"
    2. Each file MUST be in a separate code block with ``` markers
    3. DO NOT combine files into one
    4. DO NOT embed CSS/JS inside HTML
    5. DO NOT create a single monolithic file

    {you_must} generate ALL necessary separate files now.
    {'='*80}
    """
        
        # ãƒªãƒˆãƒ©ã‚¤æ™‚ã®è¿½åŠ è­¦å‘Š
        if attempt > 0:
            instruction += f"""

    âš  ATTENTION: This is retry attempt #{attempt + 1}
    The previous attempt failed to generate multiple files.
    You MUST follow the format exactly as shown above.

    """
        
        return instruction

    def _build_system_prompt_enforcement(self, attempt: int, reason: str) -> str:
        """
        System Promptã®å¼·åˆ¶æŒ‡ç¤ºã‚’æ§‹ç¯‰
        
        Args:
            attempt: ãƒªãƒˆãƒ©ã‚¤å›æ•°
            reason: è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«å¿…é ˆã®ç†ç”±
            
        Returns:
            System Promptè¿½åŠ æ–‡å­—åˆ—
        """
        if attempt == 0:
            return f"\n\nCRITICAL: User requested separate files. Generate multiple files using ## File: format. Reason: {reason}"
        elif attempt == 1:
            return f"\n\nâš  CRITICAL - SECOND ATTEMPT: You FAILED to generate multiple files in the previous attempt. User EXPLICITLY requested separate files. You MUST use ## File: format. Reason: {reason}"
        else:
            return f"\n\nğŸš¨ FINAL CRITICAL WARNING: This is your LAST ATTEMPT. You have FAILED {attempt} times to generate multiple files. User DEMANDS separate files. USE ## File: format NOW. Reason: {reason}"

    def _clean_generated_code(self, code_content: str) -> str:
        """
        Remove all markdown formatting from generated code
        â­ æ”¹å–„ç‰ˆ: shebangå¯¾å¿œã€å³æ ¼ãªMarkdownãƒ˜ãƒƒãƒ€ãƒ¼é™¤å¤–
        
        å¯¾å¿œã™ã‚‹ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼:
        - ```language ... ```
        - ## File: filename.ext
        - ãã®ä»–ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼(###, ####ãªã©)
        """
        import re
        
        lines = code_content.split('\n')
        cleaned_lines = []
        in_code_block = False
        skip_next_empty = False
        
        for i, line in enumerate(lines):
            stripped = line.strip()
            
            # â­ shebang(#!/usr/bin/env python)ã¯å¿…ãšä¿æŒ
            if stripped.startswith('#!'):
                cleaned_lines.append(line)
                continue
            
            # "## File:" å½¢å¼ã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—
            if stripped.startswith('## File:'):
                skip_next_empty = True
                continue
            
            # â­ Markdownãƒ˜ãƒƒãƒ€ãƒ¼å…¨ä½“ã‚’é™¤å¤–(##, ###, #### ã™ã¹ã¦)
            if re.match(r'^#{2,6}\s', stripped):
                # ##, ###, ####, #####, ###### ã‚’é™¤å¤–
                skip_next_empty = True
                continue
            
            # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯é–‹å§‹/çµ‚äº†ãƒãƒ¼ã‚«ãƒ¼
            if stripped.startswith('```'):
                in_code_block = not in_code_block
                skip_next_empty = True
                continue
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼ç›´å¾Œã®ç©ºè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
            if skip_next_empty and stripped == '':
                skip_next_empty = False
                continue
            
            skip_next_empty = False
            
            # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å†…ã€ã¾ãŸã¯é€šå¸¸ã®ã‚³ãƒ¼ãƒ‰è¡Œã‚’ä¿æŒ
            if in_code_block or not stripped.startswith('#'):
                cleaned_lines.append(line)
            # â­ Pythonã‚³ãƒ¡ãƒ³ãƒˆ(# ã§å§‹ã¾ã‚Šã€æ¬¡ãŒ# ã§ãªã„)ã¯ä¿æŒ
            elif stripped.startswith('#') and len(stripped) > 1:
                if stripped[1] not in ['#', '!']:  # ## ã‚„ #! ä»¥å¤–
                    cleaned_lines.append(line)
        
        # å‰å¾Œã®ç©ºç™½ã‚’é™¤å»
        result = '\n'.join(cleaned_lines).strip()
        
        # æœ€çµ‚ç¢ºèª: ã¾ã ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãŒæ®‹ã£ã¦ã„ã‚‹ã‹
        if '```' in result or result.startswith('## File:'):
            # ã‚ˆã‚Šç©æ¥µçš„ãªé™¤å»
            result = self._aggressive_markdown_removal(result)

        # â­ è¿½åŠ ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°: ã‚ˆã‚Šç©æ¥µçš„ãªãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³é™¤å»
        # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯é–‹å§‹ãƒãƒ¼ã‚«ãƒ¼ã®å®Œå…¨é™¤å»
        result = re.sub(r'^```[a-zA-Z0-9]*\s*$', '', result, flags=re.MULTILINE)
        
        # ## File: è¡Œã®å®Œå…¨é™¤å» (æ®‹ã£ã¦ã„ã‚‹å ´åˆ)
        result = re.sub(r'^##\s*File:\s*[^\n]+\s*$', '', result, flags=re.MULTILINE)
        
        # é€£ç¶šã™ã‚‹ç©ºè¡Œã‚’1ã¤ã«ã¾ã¨ã‚ã‚‹
        result = re.sub(r'\n\n\n+', '\n\n', result)
        
        # å†åº¦å‰å¾Œã®ç©ºç™½ã‚’é™¤å»
        result = result.strip()
        
        return result

    def _extract_multiple_files_from_response(self, response_content: str) -> Dict[str, str]:
        """
        LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŠ½å‡º(å¼·åŒ–ç‰ˆ)
        
        å¯¾å¿œãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:
        1. ## File: filename.ext (â­ ä½ç½®ãƒ™ãƒ¼ã‚¹æŠ½å‡º - ãƒã‚¹ãƒˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å¯¾å¿œ)
        2. ### filename.ext
        3. **filename.ext**
        4. filename.ext (ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿ã®è¡Œ + ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯)
        5. File: filename.ext (æ–°è¦è¿½åŠ )
        6. ğŸ“„ filename (æ–°è¦è¿½åŠ )
        """
        import re
        files = {}
        
        # ==========================================
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ## File: filename (â­ å¯¾ç­–A: ä½ç½®ãƒ™ãƒ¼ã‚¹æŠ½å‡º)
        # ==========================================
        # å•é¡Œ: (.*?)```ã¯éè²ªæ¬²ãªã®ã§ã€ãƒã‚¹ãƒˆã•ã‚ŒãŸ```ã§çµ‚äº†ã—ã¦ã—ã¾ã†
        # è§£æ±º: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ˜ãƒƒãƒ€ãƒ¼ã®ä½ç½®ã§åŒºåˆ‡ã‚Šã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã®æœ€åˆã€œæœ€å¾Œã®```ã‚’å–å¾—
        
        # 1. ã™ã¹ã¦ã® "## File:" ãƒ˜ãƒƒãƒ€ãƒ¼ã®ä½ç½®ã‚’ç‰¹å®š
        file_headers = list(re.finditer(r'##\s*File:\s*([^\n]+)', response_content, re.IGNORECASE))
        
        for i, match in enumerate(file_headers):
            filename = match.group(1).strip()
            start_pos = match.end()
            
            # 2. æ¬¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ˜ãƒƒãƒ€ãƒ¼ã¾ã§ã€ã¾ãŸã¯æ–‡å­—åˆ—ã®çµ‚ç«¯ã¾ã§
            if i + 1 < len(file_headers):
                end_pos = file_headers[i + 1].start()
            else:
                end_pos = len(response_content)
            
            # 3. ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã§ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º
            section = response_content[start_pos:end_pos]
            
            # 4. æœ€åˆã® ``` ã®é–‹å§‹ä½ç½®ã‚’è¦‹ã¤ã‘ã‚‹
            first_fence_match = re.search(r'```(?:[a-zA-Z0-9]+)?\s*\n', section)
            if not first_fence_match:
                continue
            
            code_start = first_fence_match.end()
            
            # 5. æœ€å¾Œã® ``` ã®ä½ç½®ã‚’è¦‹ã¤ã‘ã‚‹ï¼ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã§é€†æ–¹å‘æ¤œç´¢ï¼‰
            last_fence_pos = section.rfind('\n```')
            if last_fence_pos == -1:
                last_fence_pos = section.rfind('```')
            
            if last_fence_pos <= code_start:
                continue
            
            # 6. ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡ºï¼ˆæœ€åˆã®```ã®å¾Œã‹ã‚‰æœ€å¾Œã®```ã®å‰ã¾ã§ï¼‰
            code = section[code_start:last_fence_pos].strip()
            
            if code and len(code) >= 10:
                files[filename] = code
                logger.debug(f"[Extract:Pattern1-SectionBased] Extracted {filename}: {len(code)} chars")
        
        # ==========================================
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ### filename (ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼)
        # ==========================================
        pattern2 = r'###\s*([^\n]+\.(?:py|js|html|css|java|cpp|rs|go|rb|php|sh))\n```(?:\w+)?\n(.*?)```'
        matches2 = re.finditer(pattern2, response_content, re.DOTALL | re.IGNORECASE)
        
        for match in matches2:
            filename = match.group(1).strip()
            code = match.group(2).strip()
            if filename not in files:
                files[filename] = code
        
        # ==========================================
        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: **filename** (å¤ªå­—)
        # ==========================================
        pattern3 = r'\*\*([^\*]+\.(?:py|js|html|css|java|cpp|rs|go|rb|php|sh))\*\*\n```(?:\w+)?\n(.*?)```'
        matches3 = re.finditer(pattern3, response_content, re.DOTALL | re.IGNORECASE)
        
        for match in matches3:
            filename = match.group(1).strip()
            code = match.group(2).strip()
            if filename not in files:
                files[filename] = code
        
        # ==========================================
        # ãƒ‘ã‚¿ãƒ¼ãƒ³4: ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿ã®è¡Œ + ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯
        # ==========================================
        pattern4 = r'^([a-zA-Z0-9_\-\.\/]+\.(?:py|js|html|css|java|cpp|rs|go|rb|php|sh))$\n```(?:\w+)?\n(.*?)```'
        matches4 = re.finditer(pattern4, response_content, re.MULTILINE | re.DOTALL)
        
        for match in matches4:
            filename = match.group(1).strip()
            code = match.group(2).strip()
            if filename not in files:
                files[filename] = code
        
        # ==========================================
        # ãƒ‘ã‚¿ãƒ¼ãƒ³5: File: filename (ã‚·ãƒ³ãƒ—ãƒ«å½¢å¼) - æ–°è¦è¿½åŠ 
        # ==========================================
        pattern5 = r'File:\s*([^\n]+)\n```(?:\w+)?\n(.*?)```'
        matches5 = re.finditer(pattern5, response_content, re.DOTALL | re.IGNORECASE)
        
        for match in matches5:
            filename = match.group(1).strip()
            code = match.group(2).strip()
            if filename not in files:
                files[filename] = code
        
        # ==========================================
        # ãƒ‘ã‚¿ãƒ¼ãƒ³6: ğŸ“„ filename (çµµæ–‡å­—å½¢å¼) - æ–°è¦è¿½åŠ 
        # ==========================================
        pattern6 = r'ğŸ“„\s*([^\n]+)\s*\n[-=]+\s*\n```(?:\w+)?\n(.*?)```'
        matches6 = re.finditer(pattern6, response_content, re.DOTALL)
        
        for match in matches6:
            filename = match.group(1).strip()
            code = match.group(2).strip()
            if filename not in files:
                files[filename] = code
        
        # ==========================================
        # ã€æ–°è¦è¿½åŠ ã€‘ãƒ‘ã‚¿ãƒ¼ãƒ³7: ã‚³ãƒ¡ãƒ³ãƒˆå½¢å¼ã®ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆå¯¾ç­–2ï¼‰
        # ==========================================
        # // src/App.js ã¾ãŸã¯ /* src/App.js */
        pattern7 = r'(?://|/\*)\s*([^\n]+\.(?:js|jsx|ts|tsx|py|html|css|json))\s*(?:\*/)?\s*\n```(?:\w+)?\n(.*?)```'
        matches7 = re.finditer(pattern7, response_content, re.DOTALL)
        
        for match in matches7:
            filename = match.group(1).strip()
            code = match.group(2).strip()
            if filename not in files:
                files[filename] = code
                logger.debug(f"[Extract:Pattern7] Found file with comment marker: {filename}")
        
        # ==========================================
        # ã€æ–°è¦è¿½åŠ ã€‘ãƒ‘ã‚¿ãƒ¼ãƒ³8: ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆå¯¾ç­–2ï¼‰
        # Reactã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆè‡ªå‹•æ¤œå‡º
        # ==========================================
        if not files:
            logger.debug("[Extract:Fallback] No files found with standard patterns, trying intelligent fallback...")
            
            # ã™ã¹ã¦ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡º
            code_blocks = re.findall(r'```(?:\w+)?\n(.*?)```', response_content, re.DOTALL)
            
            for i, block in enumerate(code_blocks):
                block_stripped = block.strip()
                
                # Reactã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®æ¤œå‡º
                # ğŸ†• no_framework_modeã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                if not getattr(self, '_no_framework_mode', False):
                    if 'export default' in block_stripped and ('function' in block_stripped or 'const' in block_stripped):
                        # function App() ã¾ãŸã¯ const App = () ã®æ¤œå‡º
                        component_match = re.search(r'(?:function|const)\s+(\w+)', block_stripped)
                        if component_match:
                            component_name = component_match.group(1)
                            filename = f'src/{component_name}.js'
                            files[filename] = block_stripped
                            logger.debug(f"[Extract:Fallback] Auto-detected React component: {filename}")
                            continue
                
                # package.jsonã®æ¤œå‡º
                if '"name"' in block_stripped and '"version"' in block_stripped and '"dependencies"' in block_stripped:
                    files['package.json'] = block_stripped
                    logger.debug("[Extract:Fallback] Auto-detected package.json")
                    continue
                
                # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œå‡º
                if '<!DOCTYPE html>' in block_stripped or '<html' in block_stripped:
                    files['public/index.html'] = block_stripped
                    logger.debug("[Extract:Fallback] Auto-detected HTML file")
                    continue
        
        # ==========================================
        # Aiderã‚¹ã‚¿ã‚¤ãƒ«æŠ½å‡º(æ—¢å­˜ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)
        # ==========================================
        if not files and self.extractor:
            try:
                modifications = self.extractor.extract_modifications(response_content)
                for mod in modifications:
                    if mod['type'] == 'create':
                        filename = mod['filename']
                        if filename not in files:
                            files[filename] = mod['content']
            except Exception:
                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç„¡è¦–
                pass
        
        # ==========================================
        # ã‚³ãƒ¼ãƒ‰å“è³ªãƒã‚§ãƒƒã‚¯: ç©ºãƒ•ã‚¡ã‚¤ãƒ«ãƒ»ç„¡åŠ¹ãƒ•ã‚¡ã‚¤ãƒ«åã‚’é™¤å¤–
        # ==========================================
        # Windowsã§ä½¿ç”¨ã§ããªã„æ–‡å­—: * : ? " < > |
        # ã¾ãŸã€Markdownè¨˜æ³•ã®èª¤æ¤œå‡ºï¼ˆ**xxx**ï¼‰ã‚‚é™¤å¤–
        INVALID_FILENAME_CHARS = set('*:?"<>|')
        
        cleaned_files = {}
        for filename, content in files.items():
            # æœ€ä½é™ã®ã‚³ãƒ¼ãƒ‰é‡ãƒã‚§ãƒƒã‚¯(10æ–‡å­—ä»¥ä¸Š)
            if len(content.strip()) < 10:
                continue
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³: ç„¡åŠ¹ãªæ–‡å­—ã‚’å«ã‚€ãƒ•ã‚¡ã‚¤ãƒ«åã‚’é™¤å¤–
            if any(char in filename for char in INVALID_FILENAME_CHARS):
                logger.debug(f"[Extract:Validation] Skipping invalid filename (contains forbidden chars): {filename}")
                continue
            
            # Markdownå¤ªå­—è¨˜æ³•ã®èª¤æ¤œå‡ºã‚’é™¤å¤–ï¼ˆ**xxx** ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
            if filename.startswith('**') or filename.endswith('**'):
                logger.debug(f"[Extract:Validation] Skipping markdown bold pattern: {filename}")
                continue
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åãŒç©ºã€ã¾ãŸã¯ç©ºç™½ã®ã¿ã®å ´åˆã‚’é™¤å¤–
            if not filename.strip():
                logger.debug("[Extract:Validation] Skipping empty filename")
                continue
            
            cleaned_files[filename] = content
        
        # ==========================================
        # Phase 1: å¤–éƒ¨Linterçµ±åˆ
        # ==========================================
        if self.linter_integration and cleaned_files:
            try:
                logger.debug("[Linter] Running external linters...")
                lint_result = self.linter_integration.lint_generated_code(cleaned_files)
                
                if lint_result['has_errors']:
                    # Phase 5: Linter Errorã®TableåŒ–ï¼ˆloggerç‰ˆï¼‰
                    # æ³¨: é‡è¤‡è¡¨ç¤ºã‚’é¿ã‘ã‚‹ãŸã‚ã€_clean_and_validate_fileså´ã§è¡¨ç¤º
                    error_count = len(lint_result['errors'])
                    # logger.warning(f"Linter detected {error_count} error(s)") - å‰Šé™¤
                    
                    # Tableã¯_clean_and_validate_fileså´ã§è¡¨ç¤ºã•ã‚Œã‚‹ãŸã‚ã€ã“ã“ã§ã¯è¡¨ç¤ºã—ãªã„
                    # ï¼ˆé‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚ï¼‰
                    
                    # è‡ªå‹•ä¿®æ­£å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
                    if lint_result.get('can_auto_fix', False):
                        # logger.normal("âš™ Linter errors detected - these may need manual review") - å‰Šé™¤
                        # æ³¨: è‡ªå‹•ä¿®æ­£æ©Ÿèƒ½ã¯å°†æ¥å®Ÿè£…äºˆå®š
                        pass  # ifãƒ–ãƒ­ãƒƒã‚¯ãŒç©ºã«ãªã‚‰ãªã„ã‚ˆã†ã«passæ–‡ã‚’è¿½åŠ 
                    else:
                        logger.warning("âš  Critical linter errors - manual review required")
                
                elif lint_result.get('has_warnings', False):
                    logger.debug(f"[Linter] {len(lint_result['warnings'])} warning(s) detected")
                
                else:
                    logger.debug("[Linter] No issues detected")
                
                # Linterä½¿ç”¨çŠ¶æ³ã‚’ãƒ­ã‚°
                if lint_result.get('linters_used'):
                    logger.debug(f"[Linter] Used: {', '.join(lint_result['linters_used'])}")
            
            except Exception as e:
                logger.debug(f"[Linter] Error during lint execution: {e}")
                # Linterã‚¨ãƒ©ãƒ¼ã¯è­¦å‘Šã®ã¿ã§å‡¦ç†ã‚’ç¶™ç¶š
        
        return cleaned_files

    def _extract_code_blocks(self, response: str) -> Dict[str, str]:
        """
        LLMã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡º
        
        â­ ä¿®æ­£ç‰ˆ:
        1. _extract_multiple_files_from_response ã‚’ä½¿ç”¨ã—ã¦è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾å¿œ
        2. _clean_generated_code ã‚’å‘¼ã³å‡ºã—ã¦ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³æ§‹æ–‡ã‚’é™¤å»
        3. ã‚³ãƒ¼ãƒ‰å“è³ªãƒã‚§ãƒƒã‚¯ã‚’å¼·åŒ–
        
        Args:
            response: LLMã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚­ã‚¹ãƒˆ
        
        Returns:
            {filename: code} ã®è¾æ›¸
        """
        import re
        
        logger.debug(f"[_extract_code_blocks] Input response length: {len(response)}")
        
        # ==========================================
        # ã‚¹ãƒ†ãƒƒãƒ—1: é«˜æ©Ÿèƒ½æŠ½å‡ºé–¢æ•°ã‚’ä½¿ç”¨
        # ==========================================
        files = self._extract_multiple_files_from_response(response)  # â­ å¤‰æ›´: 6ç¨®é¡ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾å¿œ
        
        logger.debug(f"[_extract_code_blocks] Step1: _extract_multiple_files_from_response returned {len(files)} file(s)")
        if files:
            for fn, code in files.items():
                logger.debug(f"[_extract_code_blocks]   Raw: {fn} = {len(code)} chars")
        
        if files:
            # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚³ãƒ¼ãƒ‰ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            cleaned_files = {}
            for filename, code in files.items():
                before_clean = len(code)
                
                # â­ ä¿®æ­£: Markdownãƒ•ã‚¡ã‚¤ãƒ«(.md)ã¯ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—
                # README.mdãªã©ã®Markdownãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯##ãƒ˜ãƒƒãƒ€ãƒ¼ã‚„```ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ãŒå¿…è¦
                if filename.endswith('.md'):
                    cleaned_code = code  # Markdownã¯ãã®ã¾ã¾ä¿æŒ
                    logger.debug(f"[_extract_code_blocks]   Skipping clean for Markdown: {filename}")
                else:
                    cleaned_code = self._clean_generated_code(code)
                after_clean = len(cleaned_code)
                
                # â­ æ”¹å–„4: ãƒãƒ¼ã‚«ãƒ¼æ®‹å­˜ãƒã‚§ãƒƒã‚¯ï¼ˆMarkdownä»¥å¤–ã®ã¿ï¼‰
                if not filename.endswith('.md'):
                    cleaned_code = self._verify_and_fix_markdown_markers(cleaned_code, filename)
                after_verify = len(cleaned_code)
                
                logger.debug(f"[_extract_code_blocks]   Cleaning {filename}: {before_clean} -> {after_clean} -> {after_verify} chars")
                
                # ã‚³ãƒ¼ãƒ‰å“è³ªãƒã‚§ãƒƒã‚¯: ç©ºã§ãªã„ã‹ç¢ºèª
                if cleaned_code.strip():
                    cleaned_files[filename] = cleaned_code
                else:
                    # ç©ºã®å ´åˆã¯è­¦å‘Šã‚’å‡ºåŠ›
                    logger.warning(f"[_extract_code_blocks] Empty code after cleaning for file: {filename}")
                    logger.warning(f"[_extract_code_blocks]   Original code preview: {code[:200]}...")
            
            logger.debug(f"[_extract_code_blocks] Step1 result: {len(cleaned_files)} file(s) after cleaning")
            return cleaned_files
        
        # ==========================================
        # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ - å˜ä¸€ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯
        # ==========================================
        logger.debug(f"[_extract_code_blocks] Step2: Trying fallback (single code block)")
        code_block_pattern = r'```(?:[a-zA-Z0-9]+)?\s*\n(.*?)```'  # â­ æ”¹å–„: [a-z] â†’ [a-zA-Z0-9]
        code_matches = re.findall(code_block_pattern, response, re.DOTALL)
        
        logger.debug(f"[_extract_code_blocks] Step2: Found {len(code_matches)} code block(s)")
        
        if code_matches:
            # æœ€åˆã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            before_clean = len(code_matches[0])
            cleaned_code = self._clean_generated_code(code_matches[0])  # â­ è¿½åŠ : ã“ã“ã§ã‚‚ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            logger.debug(f"[_extract_code_blocks] Step2: Cleaned {before_clean} -> {len(cleaned_code)} chars")
            
            if cleaned_code.strip():
                logger.debug(f"[_extract_code_blocks] Step2: Returning __code__ with {len(cleaned_code)} chars")
                return {'__code__': cleaned_code.strip()}
            else:
                logger.debug(f"[_extract_code_blocks] Step2: Code became empty after cleaning")
        
        # ==========================================
        # ã‚¹ãƒ†ãƒƒãƒ—3: ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ãŒãªã„å ´åˆ
        # ==========================================
        logger.debug("[_extract_code_blocks] Step3: No code blocks found, returning empty dict")
        logger.debug(f"[_extract_code_blocks] Response preview (first 500 chars): {response[:500]}")
        return {}

    def _aggressive_markdown_removal(self, code_content: str) -> str:
        """
        Aggressive markdown removal as fallback
        æœ€å¾Œã®æ‰‹æ®µã¨ã—ã¦ä½¿ç”¨
        """
        lines = code_content.split('\n')
        cleaned = []
        
        for line in lines:
            # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³é–¢é€£ã®è¡Œã‚’å®Œå…¨é™¤å¤–
            if any([
                line.strip().startswith('```'),
                line.strip().startswith('## File:'),
                line.strip().startswith('## '),
                line.strip() == '##'
            ]):
                continue
            cleaned.append(line)
        
        return '\n'.join(cleaned).strip()

    def _verify_and_fix_markdown_markers(self, code_content: str, filename: str) -> str:
        """
        ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã«ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒãƒ¼ã‚«ãƒ¼ãŒæ®‹ã£ã¦ã„ãªã„ã‹ã‚’æ¤œè¨¼ã—ã€
        æ®‹ã£ã¦ã„ã‚‹å ´åˆã¯é™¤å»ã™ã‚‹ï¼ˆæ”¹å–„4: ãƒãƒ¼ã‚«ãƒ¼æ®‹å­˜ãƒã‚§ãƒƒã‚¯ï¼‰
        
        Args:
            code_content: ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ã®ã‚³ãƒ¼ãƒ‰
            filename: ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆãƒ­ã‚°ç”¨ï¼‰
        
        Returns:
            ãƒãƒ¼ã‚«ãƒ¼ãŒå®Œå…¨ã«é™¤å»ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰
        """
        import re
        
        # ãƒãƒ¼ã‚«ãƒ¼æ®‹å­˜ãƒ‘ã‚¿ãƒ¼ãƒ³
        marker_patterns = [
            r'^## File:.*$',           # ## File: header
            r'^```[a-zA-Z0-9]*\s*$',   # ``` or ```language
            r'^```$',                   # closing ```
        ]
        
        lines = code_content.split('\n')
        has_markers = False
        
        # å…ˆé ­5è¡Œã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆãƒãƒ¼ã‚«ãƒ¼ã¯é€šå¸¸å…ˆé ­ã«ã‚ã‚‹ï¼‰
        for i, line in enumerate(lines[:5]):
            for pattern in marker_patterns:
                if re.match(pattern, line.strip()):
                    has_markers = True
                    logger.warning(f"[Marker Check] Found residual marker in {filename} line {i+1}: {line.strip()[:50]}")
                    break
            if has_markers:
                break
        
        if has_markers:
            # ãƒãƒ¼ã‚«ãƒ¼ãŒæ®‹ã£ã¦ã„ã‚‹å ´åˆã€ç©æ¥µçš„ãªé™¤å»ã‚’å®Ÿè¡Œ
            logger.debug(f"[Marker Check] Applying aggressive removal for: {filename}")
            code_content = self._aggressive_markdown_removal(code_content)
            
            # å†ãƒã‚§ãƒƒã‚¯: ã¾ã æ®‹ã£ã¦ã„ã‚‹å ´åˆã¯è¡Œå˜ä½ã§é™¤å»
            lines = code_content.split('\n')
            cleaned_lines = []
            for line in lines:
                skip = False
                for pattern in marker_patterns:
                    if re.match(pattern, line.strip()):
                        skip = True
                        break
                if not skip:
                    cleaned_lines.append(line)
            
            code_content = '\n'.join(cleaned_lines).strip()
        
        return code_content

    # ==========================================
    # å¯¾ç­–C: ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ
    # ==========================================
    def _validate_runtime_import(
        self,
        generated_files: Dict[str, str]
    ) -> Tuple[bool, Optional[str], Optional[str]]:
        """
        ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
        
        ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã‚’ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«å±•é–‹ã—ã€ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
        (main.py, backend/main.pyç­‰)ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦å®Ÿè¡Œæ™‚ã‚¨ãƒ©ãƒ¼ã‚’æ¤œå‡ºã™ã‚‹ã€‚
        ã“ã‚Œã«ã‚ˆã‚Šã€é™çš„è§£æã§ã¯æ¤œå‡ºã§ããªã„ã‚¯ãƒ©ã‚¹å®šç¾©æ™‚ã®ã‚¨ãƒ©ãƒ¼ãªã©ã‚’
        äº‹å‰ã«ç™ºè¦‹ã§ãã‚‹ã€‚
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
            
        Returns:
            Tuple[bool, Optional[str], Optional[str]]:
                - success: ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸã‹ã©ã†ã‹
                - error_message: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆæˆåŠŸæ™‚ã¯Noneï¼‰
                - failing_file: ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆæˆåŠŸæ™‚ã¯Noneï¼‰
        """
        import tempfile
        import importlib.util
        import traceback
        
        # ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆã‚’ç‰¹å®šï¼ˆå„ªå…ˆåº¦é †ï¼‰
        ENTRY_POINT_CANDIDATES = [
            # ãƒ«ãƒ¼ãƒˆãƒ¬ãƒ™ãƒ«ï¼ˆå„ªå…ˆåº¦é«˜ï¼‰
            'main.py', 'app.py', 'server.py', 'run.py',
            # ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆFastAPI/Djangoç­‰ï¼‰
            'backend/main.py', 'backend/app.py',
            'src/main.py', 'src/app.py',
            'app/main.py', 'app/app.py',
            'api/main.py', 'api/app.py',
            # æ·±ã„ãƒã‚¹ãƒˆ
            'src/backend/main.py', 'src/api/main.py',
        ]
        
        entry_point = None
        for candidate in ENTRY_POINT_CANDIDATES:
            if candidate in generated_files:
                entry_point = candidate
                break
        
        if not entry_point:
            logger.debug(f"[Runtime Import] No entry point found in {len(generated_files)} files")
            logger.debug(f"[Runtime Import] Searched patterns: {ENTRY_POINT_CANDIDATES[:4]}...")
            return (True, None, None)  # ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆãªã— = ã‚¹ã‚­ãƒƒãƒ—
        
        logger.debug(f"[Runtime Import] Testing entry point: {entry_point}")
        
        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å±•é–‹
        with tempfile.TemporaryDirectory() as tmpdir:
            try:
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å±•é–‹
                for filepath, content in generated_files.items():
                    full_path = os.path.join(tmpdir, filepath)
                    dir_path = os.path.dirname(full_path)
                    
                    if dir_path:
                        os.makedirs(dir_path, exist_ok=True)
                    
                    with open(full_path, 'w', encoding='utf-8') as f:
                        f.write(content)
                    
                    # __init__.pyãŒå¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã¯è‡ªå‹•ç”Ÿæˆ
                    if '/' in filepath:
                        parts = filepath.split('/')
                        for i in range(len(parts) - 1):
                            init_dir = os.path.join(tmpdir, *parts[:i+1])
                            init_file = os.path.join(init_dir, '__init__.py')
                            if os.path.isdir(init_dir) and not os.path.exists(init_file):
                                with open(init_file, 'w', encoding='utf-8') as f:
                                    f.write('# Auto-generated __init__.py\n')
                
                # sys.pathã«ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¿½åŠ 
                original_sys_path = sys.path.copy()
                sys.path.insert(0, tmpdir)
                
                # æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ï¼ˆæ±šæŸ“é˜²æ­¢ï¼‰
                modules_to_remove = [m for m in sys.modules.keys() 
                                    if m.startswith(('config', 'models', 'views', 'schemas', 'database',
                                                     'backend', 'src', 'app', 'api', 'routes', 'services'))]
                for m in modules_to_remove:
                    del sys.modules[m]
                
                try:
                    # ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ
                    # backend/main.py -> backend.main (ãƒ‰ãƒƒãƒˆåŒºåˆ‡ã‚Šã«å¤‰æ›)
                    module_name = entry_point.replace('.py', '').replace('/', '.')
                    spec = importlib.util.spec_from_file_location(
                        module_name,
                        os.path.join(tmpdir, entry_point)
                    )
                    
                    if spec and spec.loader:
                        module = importlib.util.module_from_spec(spec)
                        spec.loader.exec_module(module)
                        
                        logger.debug(f"[Runtime Import] âœ… Successfully imported {entry_point}")
                        return (True, None, None)
                    else:
                        return (False, f"Could not create module spec for {entry_point}", entry_point)
                
                except ModuleNotFoundError as e:
                    # â­ å¯¾ç­–A: å¤–éƒ¨ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ä¸è¶³ã¯ã‚³ãƒ¼ãƒ‰ã®ãƒã‚°ã§ã¯ãªã„ã®ã§ã‚¹ã‚­ãƒƒãƒ—
                    # LLMã§ä¿®æ­£ã™ã‚‹ã“ã¨ã¯ä¸å¯èƒ½ã§ã‚ã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ç’°å¢ƒã§pip installå¾Œã«æ­£å¸¸å‹•ä½œã™ã‚‹
                    module_name = str(e).replace("No module named ", "").strip("'\"")
                    logger.debug(f"[Runtime Import] âš  External package not found: {module_name}")
                    logger.debug(f"[Runtime Import] This is not a code bug. Skipping test.")
                    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚’è¿”ã™ï¼ˆã‚¨ãƒ©ãƒ¼ã§ã¯ãªãæƒ…å ±ã¨ã—ã¦ï¼‰
                    return (True, f"EXTERNAL_PACKAGE:{module_name}", None)
                        
                except Exception as e:
                    # ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ - è©³ç´°æƒ…å ±ã‚’æŠ½å‡º
                    tb = traceback.format_exc()
                    error_message = str(e)
                    
                    # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç‰¹å®š
                    failing_file = self._extract_failing_file_from_traceback(tb, tmpdir, generated_files)
                    
                    logger.debug(f"[Runtime Import] âŒ Import failed: {error_message}")
                    logger.debug(f"[Runtime Import]    Failing file: {failing_file}")
                    logger.debug(f"[Runtime Import]    Traceback:\n{tb}")
                    
                    return (False, error_message, failing_file)
                    
                finally:
                    # sys.pathã‚’å¾©å…ƒ
                    sys.path = original_sys_path
                    
                    # ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                    modules_to_remove = [m for m in sys.modules.keys() 
                                        if m.startswith(('config', 'models', 'views', 'schemas', 'database',
                                                         'backend', 'src', 'app', 'api', 'routes', 'services'))]
                    for m in modules_to_remove:
                        if m in sys.modules:
                            del sys.modules[m]
                            
            except Exception as e:
                logger.debug(f"[Runtime Import] Error during test setup: {e}")
                return (False, str(e), None)
    
    def _extract_failing_file_from_traceback(
        self,
        tb: str,
        tmpdir: str,
        generated_files: Dict[str, str]
    ) -> Optional[str]:
        """
        ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‹ã‚‰ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç‰¹å®š
        
        Args:
            tb: ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯æ–‡å­—åˆ—
            tmpdir: ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«
            
        Returns:
            ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆç‰¹å®šã§ããªã„å ´åˆã¯Noneï¼‰
        """
        import re
        
        # ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŠ½å‡º
        # ãƒ‘ã‚¿ãƒ¼ãƒ³: File "path/to/file.py", line X
        pattern = r'File "([^"]+)", line (\d+)'
        matches = re.findall(pattern, tb)
        
        for filepath, line_no in reversed(matches):  # æœ€å¾Œï¼ˆæœ€ã‚‚æ·±ã„ï¼‰ã‹ã‚‰æ¤œç´¢
            # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã‚’é™¤å»
            if tmpdir in filepath:
                relative_path = filepath.replace(tmpdir + os.sep, '').replace(tmpdir + '/', '')
                # Windowsãƒ‘ã‚¹å¯¾å¿œ
                relative_path = relative_path.replace('\\', '/')
                
                if relative_path in generated_files:
                    return relative_path
        
        return None
    
    def _fix_import_error_with_llm(
        self,
        generated_files: Dict[str, str],
        error_message: str,
        failing_file: Optional[str]
    ) -> Optional[Dict[str, str]]:
        """
        LLMã‚’ä½¿ç”¨ã—ã¦ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ã‚’è‡ªå‹•ä¿®æ­£
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«
            error_message: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            failing_file: ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãƒ•ã‚¡ã‚¤ãƒ«
            
        Returns:
            ä¿®æ­£ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä¿®æ­£ã§ããªã„å ´åˆã¯Noneï¼‰
        """
        if not failing_file or failing_file not in generated_files:
            logger.debug(f"[Import Fix] Cannot fix: failing_file not identified or not in generated_files")
            return None
        
        logger.debug(f"[Import Fix] Attempting to fix: {failing_file}")
        
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚³ãƒ¼ãƒ‰
        failing_code = generated_files[failing_file]
        
        # é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆã‚¤ãƒ³ãƒãƒ¼ãƒˆå…ˆãªã©ï¼‰ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        related_files_context = []
        for filepath, content in generated_files.items():
            if filepath != failing_file and filepath.endswith('.py'):
                # æœ€å¤§3ãƒ•ã‚¡ã‚¤ãƒ«ã¾ã§
                if len(related_files_context) < 3:
                    related_files_context.append(f"## File: {filepath}\n```python\n{content}\n```")
        
        related_context = "\n\n".join(related_files_context) if related_files_context else "No related files"
        
        # ä¿®æ­£ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        # Note: ã‚³ãƒ¼ãƒ‰ãƒ•ã‚§ãƒ³ã‚¹ã‚’å¤‰æ•°åŒ–ã—ã¦f-stringå†…ã§ã®å•é¡Œã‚’å›é¿
        code_fence = "```"
        prompt = f"""Fix the Python import/runtime error in the following code.

## Error Information
- Error Message: {error_message}
- Failing File: {failing_file}

## Common Causes
1. Class-level code that executes at import time (e.g., validation in class body)
2. Missing __init__.py files
3. Circular imports
4. Undefined variables at module level
5. Missing dependencies

## CRITICAL FIX PATTERNS

### Pattern 1: Class-level validation (WRONG)
{code_fence}python
class SomeConfig(BaseConfig):
    SECRET_KEY = os.environ.get('SECRET_KEY')
    if not SECRET_KEY:  # This executes at import time!
        raise ValueError("...")
{code_fence}

### Pattern 1: Class-level validation (CORRECT)
{code_fence}python
class SomeConfig(BaseConfig):
    SECRET_KEY = os.environ.get('SECRET_KEY') or 'default-dev-key'
    # Or use @property for runtime validation
{code_fence}

### Pattern 2: Missing fallback (WRONG)
{code_fence}python
DATABASE_URL = os.environ['DATABASE_URL']  # Raises KeyError if not set
{code_fence}

### Pattern 2: Missing fallback (CORRECT)
{code_fence}python
DATABASE_URL = os.environ.get('DATABASE_URL', 'sqlite:///default.db')
{code_fence}

## File with Error: {failing_file}
{code_fence}python
{failing_code}
{code_fence}

## Related Files (for context)
{related_context}

## Instructions
1. Identify the exact cause of the import error
2. Fix the issue while preserving all functionality
3. Return the COMPLETE fixed file

Return ONLY the fixed code in this format:
## File: {failing_file}
{code_fence}python
<complete fixed code here>
{code_fence}
"""
        
        try:
            response = self.llm_manager.generate_response(
                prompt=prompt,
                system_prompt="You are a Python expert. Fix the import/runtime error. Return only the fixed code.",
                max_tokens=8000
            )
            
            if hasattr(response, 'content'):
                response_text = response.content
            elif isinstance(response, str):
                response_text = response
            else:
                logger.debug("[Import Fix] Unexpected response type")
                return None
            
            # ä¿®æ­£ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º
            fixed_files = self._extract_code_blocks(response_text)
            
            if fixed_files:
                # ä¿®æ­£ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã§æ›´æ–°
                result = generated_files.copy()
                for filename, code in fixed_files.items():
                    # ãƒ•ã‚¡ã‚¤ãƒ«åã®æ­£è¦åŒ–
                    normalized_name = filename.replace('\\', '/')
                    if normalized_name == failing_file or filename == failing_file:
                        result[failing_file] = code
                        logger.debug(f"[Import Fix] âœ… Fixed {failing_file} ({len(code)} chars)")
                        return result
                
                # ãƒ•ã‚¡ã‚¤ãƒ«åãŒä¸€è‡´ã—ãªã„å ´åˆã€æœ€åˆã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
                first_code = list(fixed_files.values())[0]
                result[failing_file] = first_code
                logger.debug(f"[Import Fix] âœ… Fixed {failing_file} (name mismatch, used first file)")
                return result
            
            logger.debug("[Import Fix] No code blocks extracted from LLM response")
            return None
            
        except Exception as e:
            logger.debug(f"[Import Fix] Error during LLM fix: {e}")
            return None

    # è¡Œ3258-3263: é–¢æ•°å®šç¾©ï¼ˆã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆ: 4ã‚¹ãƒšãƒ¼ã‚¹ï¼‰- å¤‰æ›´ãªã—
    def _fix_linter_errors_with_llm(
        self,
        generated_files: Dict[str, str],
        errors: List[Dict[str, Any]],
        file_languages: Dict[str, str] = None
    ) -> Optional[Dict[str, str]]:
        """
        LLMã‚’ä½¿ç”¨ã—ã¦Lintã‚¨ãƒ©ãƒ¼ã‚’è‡ªå‹•ä¿®æ­£
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰
            errors: Lintã‚¨ãƒ©ãƒ¼ã®ãƒªã‚¹ãƒˆ
            file_languages: ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥è¨€èªæƒ…å ±
            
        Returns:
            ä¿®æ­£ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰
        """
        # ä¿®æ­£: 2025-12-29
        # å•é¡Œ: ã‚¨ãƒ©ãƒ¼æ•°ãŒå¤šã™ãã‚‹ï¼ˆ187ä»¶ç­‰ï¼‰ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒå·¨å¤§åŒ–ã—ã€
        #       LLMã®å‡ºåŠ›ãŒé€”åˆ‡ã‚ŒãŸã‚Šã€æŠ½å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³ã«ãƒãƒƒãƒã—ãªããªã‚‹
        # å¯¾ç­–: 
        #   1. ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«æœ€å¤§20ä»¶ã®ã‚¨ãƒ©ãƒ¼ã‚’é¸æŠ
        #   2. åˆè¨ˆã§æœ€å¤§60ä»¶ã«åˆ¶é™
        #   3. é‡è¦åº¦é †ã«ã‚½ãƒ¼ãƒˆï¼ˆCritical > High > Medium > Lowï¼‰
        
        MAX_ERRORS_PER_FILE = 20
        MAX_TOTAL_ERRORS = 60
        
        # é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆ
        severity_order = {'critical': 0, 'high': 1, 'medium': 2, 'low': 3, 'warning': 4}
        
        def get_severity(err):
            severity = err.get('severity', 'low').lower()
            return severity_order.get(severity, 3)
        
        sorted_errors = sorted(errors, key=get_severity)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ã‚¨ãƒ©ãƒ¼ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã€åˆ¶é™
        from collections import defaultdict
        errors_by_file = defaultdict(list)
        for err in sorted_errors:
            # ğŸ†• å­˜åœ¨ã—ãªã„ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ã‚¨ãƒ©ãƒ¼ã‚’é™¤å¤–ï¼ˆtscãŒæœªç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã™ã‚‹å•é¡Œå¯¾ç­–ï¼‰
            if err['file'] not in generated_files:
                logger.debug(f"[LLM Fix] Skipping error for non-existent file: {err['file']}")
                continue
            errors_by_file[err['file']].append(err)
        
        # ã‚¨ãƒ©ãƒ¼ãŒå…¨ã¦é™¤å¤–ã•ã‚ŒãŸå ´åˆã¯ä¿®æ­£ä¸è¦
        if not errors_by_file:
            logger.debug("[LLM Fix] All errors refer to non-existent files, skipping fix")
            return None
        
        limited_errors = []
        for filename, file_errors in errors_by_file.items():
            limited_errors.extend(file_errors[:MAX_ERRORS_PER_FILE])
        
        # åˆè¨ˆåˆ¶é™
        limited_errors = limited_errors[:MAX_TOTAL_ERRORS]
        
        if len(errors) > len(limited_errors):
            logger.debug(f"[LLM Fix] Limited errors from {len(errors)} to {len(limited_errors)} (max {MAX_TOTAL_ERRORS})")
        
        # ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’æ•´å½¢ï¼ˆè©³ç´°ç‰ˆ - è¡Œç•ªå·ã€ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰ã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å«ã‚€ï¼‰
        error_details = []
        for i, err in enumerate(limited_errors, 1):
            error_info = [
                f"Error #{i}:",
                f"  File: {err['file']}",
                f"  Line: {err.get('line', 'N/A')}, Column: {err.get('column', 'N/A')}",
                f"  Code: {err.get('code', 'N/A')}",
                f"  Message: {err.get('message', 'No message')}",
                f"  Linter: {err.get('linter', 'unknown')}",
            ]
            error_details.append('\n'.join(error_info))
        
        error_text = '\n\n'.join(error_details)
        

            
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        error_files = set(err['file'] for err in errors)
        files_context = []
        for filename in error_files:
            if filename in generated_files:
                code = generated_files[filename]
                files_context.append(f"## File: {filename}\n```\n{code}\n```")
            
        files_text = "\n\n".join(files_context)
        
        # ãƒªãƒã‚¸ãƒˆãƒªãƒãƒƒãƒ—ã‚’ç”Ÿæˆï¼ˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«é–“ã®é–¢ä¿‚æ€§ã‚’æä¾›ï¼‰
        repository_map = self._generate_repository_map()
            
        # ä¿®æ­£ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        # âœ… è¨€èªç¨®åˆ¥ã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
        # language_requirementsã‚’ç”Ÿæˆ
        if file_languages:
            lang_reqs = []
            for filename, lang in file_languages.items():
                if lang == 'html':
                    lang_reqs.append(f"- {filename}: HTML5 æ¨™æº–æº–æ‹ ã€altå±æ€§å¿…é ˆ")
                elif lang == 'css':
                    lang_reqs.append(f"- {filename}: CSS3 æ¨™æº–æº–æ‹ ")
                elif lang == 'javascript':
                    lang_reqs.append(f"- {filename}: Pure JavaScript (ES6+), NO TypeScript")
                elif lang == 'python':
                    lang_reqs.append(f"- {filename}: PEP8 æº–æ‹ ")
                else:
                    lang_reqs.append(f"- {filename}: {lang} best practices")
            language_requirements = "\n    ".join(lang_reqs) if lang_reqs else "No specific requirements"
        else:
            language_requirements = "No specific requirements"
        
        prompt = f"""Fix the following linter errors in the code.

    âš ï¸ CRITICAL CONSTRAINTS:
    - Do NOT create any new files. Only modify the existing files listed below.
    - If an error refers to a non-existent file, IGNORE that error entirely.
    - You must ONLY output files that already exist in "CURRENT CODE" section.

    REPOSITORY CONTEXT (available functions and classes):
    {repository_map if repository_map else 'No repository context available'}

    LINTER ERRORS:
    {error_text}

    CURRENT CODE:
    {files_text}

    LANGUAGE-SPECIFIC REQUIREMENTS:
    {language_requirements}  # âœ… è¨€èªåˆ¥ã®è¦ä»¶ã‚’è¿½åŠ 

    INSTRUCTIONS:
    1. Fix ONLY the reported errors at the specified lines and columns
    2. For each error, address the specific issue mentioned in the error message
    3. Do NOT change any other code
    4. Preserve all existing functionality
    5. Keep the code in its original language (JavaScript, Python, etc.)
    6. Do NOT create new files - only fix files that exist in CURRENT CODE above
    7. Return the fixed files using this format:

    ## File: filename
    ```
    fixed code here
    ```

    Generate the fixed code now."""
            
        try:
            logger.debug("[LLM Fix] Requesting error fixes from LLM...")
            
            # ä¿®æ­£: 2025-12-29
            # å•é¡Œ: max_tokensæœªæŒ‡å®šã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆ4096ï¼‰ãŒä½¿ç”¨ã•ã‚Œã€
            #       å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿®æ­£çµæœãŒé€”åˆ‡ã‚Œã‚‹
            # å¯¾ç­–: max_tokens=16000 ã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
            #
            # ã¾ãŸã€ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã«å¿œã˜ã¦max_tokensã‚’èª¿æ•´
            error_file_count = len(set(err['file'] for err in errors))
            if error_file_count >= 3:
                fix_max_tokens = 16000
            elif error_file_count == 2:
                fix_max_tokens = 16000
            else:
                fix_max_tokens = 8000
            
            logger.debug(f"[LLM Fix] Using max_tokens={fix_max_tokens} for {error_file_count} file(s)")
            
            response = self.llm_manager.generate_response(
                prompt=prompt,
                system_prompt="You are a code quality expert. Fix linter errors without changing functionality.",
                max_tokens=fix_max_tokens
            )
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
            if hasattr(response, 'content'):
                response_text = response.content
            elif isinstance(response, str):
                response_text = response
            else:
                response_text = str(response)
            
            logger.debug(f"[LLM Fix] Response received, length: {len(response_text)}")
            
            # ä¿®æ­£ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º
            fixed_files = self._extract_code_blocks(response_text)
            
            if fixed_files:
                # ğŸ†• æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«é™¤å¤–: å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚»ãƒƒãƒˆã«ãªã„ãƒ•ã‚¡ã‚¤ãƒ«ã¯æ¨ã¦ã‚‹
                original_filenames = set(generated_files.keys())
                
                # ãƒ•ã‚¡ã‚¤ãƒ«åæ­£è¦åŒ–ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆLLMãŒå¾®å¦™ã«ç•°ãªã‚‹ãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¿”ã™å¯¾ç­–ï¼‰
                # ä¾‹: "./neon_tetris.html" â†’ "neon_tetris.html"
                normalized_originals = {}
                for orig in original_filenames:
                    # æ­£è¦åŒ–ã‚­ãƒ¼: å…ˆé ­ã®./ ã‚’é™¤å» + å°æ–‡å­—åŒ–
                    norm_key = orig
                    while norm_key.startswith('./'):
                        norm_key = norm_key[2:]
                    norm_key = norm_key.lower()
                    normalized_originals[norm_key] = orig
                    # basenameå˜ä½“ã‚‚ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆãƒ‘ã‚¹ä¸ä¸€è‡´å¯¾ç­–ï¼‰
                    base_key = os.path.basename(orig).lower()
                    if base_key not in normalized_originals:
                        normalized_originals[base_key] = orig
                
                filtered_files = {}
                for k, v in fixed_files.items():
                    if k in original_filenames:
                        # å®Œå…¨ä¸€è‡´
                        filtered_files[k] = v
                    else:
                        # æ­£è¦åŒ–ä¸€è‡´ã‚’è©¦è¡Œ
                        norm_k = k
                        while norm_k.startswith('./'):
                            norm_k = norm_k[2:]
                        norm_k = norm_k.lower()
                        base_k = os.path.basename(k).lower()
                        
                        matched_original = None
                        if norm_k in normalized_originals:
                            matched_original = normalized_originals[norm_k]
                        elif base_k in normalized_originals:
                            matched_original = normalized_originals[base_k]
                        
                        if matched_original:
                            # æ­£è¦åŒ–ä¸€è‡´: å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ã‚­ãƒ¼ã¨ã—ã¦ä½¿ç”¨
                            filtered_files[matched_original] = v
                            logger.debug(f"[LLM Fix] Normalized filename: '{k}' -> '{matched_original}'")
                        else:
                            logger.debug(f"[LLM Fix] Removed unexpected new file: '{k}'")
                
                if len(filtered_files) < len(fixed_files):
                    removed_count = len(fixed_files) - len(filtered_files)
                    logger.debug(f"[LLM Fix] Filtered out {removed_count} unexpected file(s)")
                
                fixed_files = filtered_files
                
                if not fixed_files:
                    logger.debug("[LLM Fix] No valid fixes after filtering out new files")
                    return None
                
                # å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ã¨ãƒãƒ¼ã‚¸ (ä¿®æ­£ã•ã‚Œãªã‹ã£ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚‚å«ã‚ã‚‹)
                result = generated_files.copy()
                result.update(fixed_files)
                
                logger.debug(f"[LLM Fix] Successfully fixed {len(fixed_files)} file(s)")
                return result
            else:
                logger.debug("[LLM Fix] Failed to extract fixed code from LLM response")
                return None
                
        except Exception as e:
            logger.debug(f"[LLM Fix] Error during auto-fix: {e}")
            return None

    # ==========================================
    # Cross-File Consistency Check Methods
    # ==========================================
    
    def _check_cross_file_consistency(
        self, 
        generated_code: Dict[str, str]
    ) -> Dict[str, Any]:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«é–“æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ï¼ˆPhase 1: ãƒ¢ãƒ‡ãƒ«â†”åˆæœŸãƒ‡ãƒ¼ã‚¿ï¼‰
        
        SQLAlchemyãƒ¢ãƒ‡ãƒ«ã®å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¨åˆæœŸãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ã‚’æ¤œè¨¼ã—ã€
        æ¬ è½ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æ¤œå‡ºã™ã‚‹ã€‚
        
        Args:
            generated_code: ãƒ•ã‚¡ã‚¤ãƒ«åã¨ã‚³ãƒ¼ãƒ‰ã®è¾æ›¸
            
        Returns:
            {
                'has_issues': bool,
                'issues': [
                    {
                        'type': 'model_init_data_mismatch',
                        'model': 'Enemy',
                        'tablename': 'enemies',
                        'field': 'max_health',
                        'model_file': 'models/enemy.py',
                        'init_file': 'database.py',
                        'message': "Required field 'max_health' missing in init data"
                    }
                ],
                'affected_files': ['database.py'],
                'model_definitions': {model_name: {field: type_info}}
            }
        """
        logger.debug("[Cross-File] Starting consistency check...")
        
        issues = []
        affected_files = set()
        model_definitions = {}
        
        # Step 1: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æŠ½å‡º
        all_model_info = []
        for filename, code in generated_code.items():
            if filename.startswith('models/') and filename.endswith('.py'):
                if filename == 'models/__init__.py':
                    continue
                model_info_list = self._extract_model_required_fields(code, filename)
                all_model_info.extend(model_info_list)
                
                # ä¿®æ­£ç”¨ã«ãƒ¢ãƒ‡ãƒ«å®šç¾©ã‚’ä¿å­˜
                for info in model_info_list:
                    model_definitions[info['model_name']] = {
                        'tablename': info['tablename'],
                        'required_fields': info['required_fields'],
                        'field_definitions': info.get('field_definitions', {}),
                        'source_file': filename
                    }
        
        logger.debug(f"[Cross-File] Found {len(all_model_info)} model(s) with required fields")
        for info in all_model_info:
            logger.debug(f"[Cross-File]   {info['model_name']} ({info['tablename']}): {info['required_fields']}")
        
        # Step 2: database.py ã‹ã‚‰åˆæœŸãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æŠ½å‡º
        init_data_fields = {}
        if 'database.py' in generated_code:
            init_data_fields = self._extract_init_data_fields(generated_code['database.py'])
            logger.debug(f"[Cross-File] Found init data for tables: {list(init_data_fields.keys())}")
            for table, fields in init_data_fields.items():
                logger.debug(f"[Cross-File]   {table}: {fields}")
        else:
            logger.debug("[Cross-File] No database.py found, skipping init data check")
            return {'has_issues': False, 'issues': [], 'affected_files': [], 'model_definitions': {}}
        
        # Step 3: çªåˆãƒã‚§ãƒƒã‚¯
        for model_info in all_model_info:
            tablename = model_info['tablename']
            model_name = model_info['model_name']
            required_fields = model_info['required_fields']
            model_file = model_info['source_file']
            
            # åˆæœŸãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã¿ãƒã‚§ãƒƒã‚¯
            if tablename not in init_data_fields:
                logger.debug(f"[Cross-File] No init data for table '{tablename}', skipping")
                continue
            
            provided_fields = init_data_fields[tablename]
            
            for field in required_fields:
                if field not in provided_fields:
                    issue = {
                        'type': 'model_init_data_mismatch',
                        'model': model_name,
                        'tablename': tablename,
                        'field': field,
                        'model_file': model_file,
                        'init_file': 'database.py',
                        'message': f"Required field '{field}' missing in init data for {model_name}"
                    }
                    issues.append(issue)
                    affected_files.add('database.py')
                    logger.debug(f"[Cross-File] âŒ Issue found: {issue['message']}")
        
        has_issues = len(issues) > 0
        
        if has_issues:
            logger.debug(f"[Cross-File] Found {len(issues)} consistency issue(s)")
        else:
            logger.debug("[Cross-File] âœ… No consistency issues found")
        
        return {
            'has_issues': has_issues,
            'issues': issues,
            'affected_files': list(affected_files),
            'model_definitions': model_definitions
        }
    
    def _extract_model_required_fields(
        self, 
        model_code: str, 
        filename: str
    ) -> List[Dict]:
        """
        ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æŠ½å‡º
        
        1ãƒ•ã‚¡ã‚¤ãƒ«è¤‡æ•°ãƒ¢ãƒ‡ãƒ«å¯¾å¿œï¼ˆclassãƒ–ãƒ­ãƒƒã‚¯å˜ä½ã§å‡¦ç†ï¼‰
        
        æŠ½å‡ºæ¡ä»¶:
        - mapped_column() ã‚’å«ã‚€è¡Œ
        - nullable=False ã‚’å«ã‚€ï¼ˆæ˜ç¤ºçš„ã®ã¿ï¼‰
        - default= ã‚’å«ã¾ãªã„
        - primary_key=True ã‚’é™¤å¤–
        - ForeignKey( ã‚’é™¤å¤–
        
        Args:
            model_code: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰
            filename: ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆãƒ­ã‚°ç”¨ï¼‰
            
        Returns:
            [
                {
                    'model_name': 'Enemy',
                    'tablename': 'enemies',
                    'required_fields': ['name', 'enemy_type', 'health', 'max_health', 'damage'],
                    'field_definitions': {'max_health': 'Mapped[int]', ...},
                    'source_file': 'models/enemy.py'
                },
                ...
            ]
        """
        import re
        
        results = []
        lines = model_code.split('\n')
        
        current_class = None
        current_tablename = None
        current_required_fields = []
        current_field_definitions = {}
        
        # ã‚¯ãƒ©ã‚¹å®šç¾©ãƒ‘ã‚¿ãƒ¼ãƒ³
        class_pattern = re.compile(r'^class\s+(\w+)\s*\(.*(?:db\.Model|Base)')
        # ãƒ†ãƒ¼ãƒ–ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³
        tablename_pattern = re.compile(r"__tablename__\s*=\s*['\"](\w+)['\"]")
        # mapped_columnãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆè¤‡æ•°è¡Œå¯¾å¿œã®ãŸã‚æŸ”è»Ÿã«ï¼‰
        mapped_column_pattern = re.compile(
            r'^\s+(\w+):\s*Mapped\[([^\]]+)\]\s*=\s*mapped_column\s*\(([^)]*)\)'
        )
        
        for line in lines:
            # ã‚¯ãƒ©ã‚¹å®šç¾©ã‚’æ¤œå‡º
            class_match = class_pattern.match(line)
            if class_match:
                # å‰ã®ã‚¯ãƒ©ã‚¹ã®çµæœã‚’ä¿å­˜
                if current_class and current_tablename and current_required_fields:
                    results.append({
                        'model_name': current_class,
                        'tablename': current_tablename,
                        'required_fields': current_required_fields,
                        'field_definitions': current_field_definitions,
                        'source_file': filename
                    })
                
                # æ–°ã—ã„ã‚¯ãƒ©ã‚¹ã‚’é–‹å§‹
                current_class = class_match.group(1)
                current_tablename = None
                current_required_fields = []
                current_field_definitions = {}
                continue
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æ¤œå‡º
            tablename_match = tablename_pattern.search(line)
            if tablename_match and current_class:
                current_tablename = tablename_match.group(1)
                continue
            
            # mapped_columnã‚’æ¤œå‡º
            mc_match = mapped_column_pattern.match(line)
            if mc_match and current_class:
                field_name = mc_match.group(1)
                field_type = mc_match.group(2)
                column_args = mc_match.group(3)
                
                # é™¤å¤–æ¡ä»¶
                if 'primary_key=True' in column_args or 'primary_key' in column_args:
                    continue
                if 'ForeignKey(' in column_args or 'ForeignKey (' in column_args:
                    continue
                
                # default= ãŒã‚ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ï¼‰
                if 'default=' in column_args or 'default =' in column_args:
                    continue
                
                # nullable=True ãŒã‚ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—
                if 'nullable=True' in column_args or 'nullable = True' in column_args:
                    continue
                
                # Optionalå‹ã¯ã‚¹ã‚­ãƒƒãƒ—
                if 'Optional[' in field_type:
                    continue
                
                # nullable=False ã®ã¿å¿…é ˆã¨ã—ã¦æ‰±ã†ï¼ˆæ˜ç¤ºçš„æŒ‡å®šã®ã¿ï¼‰
                if 'nullable=False' in column_args or 'nullable = False' in column_args:
                    current_required_fields.append(field_name)
                    current_field_definitions[field_name] = f"Mapped[{field_type}]"
        
        # æœ€å¾Œã®ã‚¯ãƒ©ã‚¹ã®çµæœã‚’ä¿å­˜
        if current_class and current_tablename and current_required_fields:
            results.append({
                'model_name': current_class,
                'tablename': current_tablename,
                'required_fields': current_required_fields,
                'field_definitions': current_field_definitions,
                'source_file': filename
            })
        
        return results
    
    def _extract_init_data_fields(
        self, 
        database_code: str
    ) -> Dict[str, Set[str]]:
        """
        database.py ã‹ã‚‰åˆæœŸãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æŠ½å‡º
        
        ãƒ‘ã‚¿ãƒ¼ãƒ³: default_{tablename} = [...]
        
        Args:
            database_code: database.pyã®ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰
            
        Returns:
            {
                'enemies': {'name', 'enemy_type', 'health', 'damage', ...},
                'weapons': {'name', 'weapon_type', 'damage', ...},
            }
        """
        import re
        
        result = {}
        
        # default_xxx = [ ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
        # ä¾‹: default_weapons = [, default_enemies = [
        init_data_pattern = re.compile(r'default_(\w+)\s*=\s*\[')
        
        # è¾æ›¸ã‚­ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ'key': ã¾ãŸã¯ "key":ï¼‰
        dict_key_pattern = re.compile(r"['\"](\w+)['\"]\s*:")
        
        lines = database_code.split('\n')
        current_table = None
        bracket_depth = 0
        current_fields = set()
        
        for line in lines:
            # æ–°ã—ã„åˆæœŸãƒ‡ãƒ¼ã‚¿å¤‰æ•°ã‚’æ¤œå‡º
            init_match = init_data_pattern.search(line)
            if init_match:
                # å‰ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã®çµæœã‚’ä¿å­˜
                if current_table and current_fields:
                    result[current_table] = current_fields
                
                # å¤‰æ•°åã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æ¨å®šï¼ˆdefault_enemies -> enemiesï¼‰
                var_name = init_match.group(1)
                current_table = var_name
                current_fields = set()
                bracket_depth = line.count('[') - line.count(']')
                continue
            
            # ç¾åœ¨ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ä¸­
            if current_table:
                bracket_depth += line.count('[') - line.count(']')
                bracket_depth += line.count('{') - line.count('}')
                
                # è¾æ›¸ã‚­ãƒ¼ã‚’æŠ½å‡º
                keys = dict_key_pattern.findall(line)
                current_fields.update(keys)
                
                # ãƒ‡ãƒ¼ã‚¿çµ‚äº†ã‚’æ¤œå‡ºï¼ˆãƒ–ãƒ©ã‚±ãƒƒãƒˆæ·±åº¦ãŒ0ä»¥ä¸‹ã‹ã¤ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒã‚ã‚‹ï¼‰
                if bracket_depth <= 0 and current_fields:
                    result[current_table] = current_fields
                    current_table = None
                    current_fields = set()
                    bracket_depth = 0
        
        # æœ€å¾Œã®ãƒ†ãƒ¼ãƒ–ãƒ«ã®çµæœã‚’ä¿å­˜
        if current_table and current_fields:
            result[current_table] = current_fields
        
        return result
    
    def _fix_cross_file_issues_with_llm(
        self,
        generated_code: Dict[str, str],
        issues: List[Dict],
        model_definitions: Dict[str, Dict]
    ) -> Optional[Dict[str, str]]:
        """
        Cross-Fileæ•´åˆæ€§å•é¡Œã‚’LLMã§ä¿®æ­£
        
        Args:
            generated_code: ãƒ•ã‚¡ã‚¤ãƒ«åã¨ã‚³ãƒ¼ãƒ‰ã®è¾æ›¸
            issues: æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã®ãƒªã‚¹ãƒˆ
            model_definitions: ãƒ¢ãƒ‡ãƒ«å®šç¾©æƒ…å ±ï¼ˆå‹æƒ…å ±å«ã‚€ï¼‰
            
        Returns:
            ä¿®æ­£ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®è¾æ›¸ã€å¤±æ•—æ™‚ã¯None
        """
        if not issues:
            return None
        
        # å½±éŸ¿ã‚’å—ã‘ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆdatabase.pyï¼‰ã®ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—
        database_code = generated_code.get('database.py', '')
        if not database_code:
            logger.debug("[Cross-File Fix] database.py not found")
            return None
        
        # å•é¡Œã®è©³ç´°ã‚’æ•´å½¢
        issue_details = []
        for i, issue in enumerate(issues, 1):
            model_name = issue['model']
            field = issue['field']
            
            # ãƒ¢ãƒ‡ãƒ«å®šç¾©ã‹ã‚‰å‹æƒ…å ±ã‚’å–å¾—
            field_type = "Unknown"
            if model_name in model_definitions:
                field_defs = model_definitions[model_name].get('field_definitions', {})
                field_type = field_defs.get(field, "Unknown")
            
            issue_details.append(
                f"{i}. Model: {model_name}, Field: {field}, Type: {field_type}\n"
                f"   Missing in: database.py (default_{issue['tablename']} initialization)"
            )
        
        issues_text = "\n".join(issue_details)
        
        # ãƒ¢ãƒ‡ãƒ«å®šç¾©ã®å‚ç…§æƒ…å ±ã‚’æ•´å½¢
        model_refs = []
        for model_name, info in model_definitions.items():
            if any(issue['model'] == model_name for issue in issues):
                fields_info = []
                for field, field_type in info.get('field_definitions', {}).items():
                    fields_info.append(f"  - {field}: {field_type}")
                model_refs.append(
                    f"Model: {model_name} (table: {info['tablename']})\n" +
                    "\n".join(fields_info)
                )
        
        model_refs_text = "\n\n".join(model_refs)
        
        # ä¿®æ­£ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        fix_prompt = f"""You are fixing a cross-file consistency issue in a Flask/SQLAlchemy application.

## Issue Description
The following required fields are missing in the initialization data (database.py):

{issues_text}

## Model Definitions (for reference)
{model_refs_text}

## Current database.py
```python
{database_code}
```

## Task
Fix the database.py file by adding the missing required fields to the initialization data.

IMPORTANT RULES:
1. For each missing field, add an appropriate default value based on the field type and context
2. For 'max_health', use the same value as 'health' (they should match initially)
3. For other integer fields, use sensible defaults (0 or context-appropriate values)
4. Do NOT modify any model files - only fix database.py
5. Maintain the existing code structure and formatting
6. Output ONLY the fixed database.py file

## Output Format
```python
# database.py
[complete fixed code here]
```
"""
        
        try:
            logger.debug("[Cross-File Fix] Sending fix request to LLM...")
            
            response = self._call_llm_api(fix_prompt)
            
            if not response:
                logger.debug("[Cross-File Fix] No response from LLM")
                return None
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º
            extracted = self._extract_code_from_response(response, ['database.py'])
            
            if extracted and 'database.py' in extracted:
                fixed_code = extracted['database.py']
                
                # ä¿®æ­£ãŒé©ç”¨ã•ã‚ŒãŸã‹ç¢ºèªï¼ˆç°¡æ˜“ãƒã‚§ãƒƒã‚¯ï¼‰
                missing_fields_found = 0
                for issue in issues:
                    field = issue['field']
                    if f"'{field}'" in fixed_code or f'"{field}"' in fixed_code:
                        missing_fields_found += 1
                
                if missing_fields_found > 0:
                    logger.debug(f"[Cross-File Fix] âœ… Fixed {missing_fields_found}/{len(issues)} missing fields")
                    return {'database.py': fixed_code}
                else:
                    logger.debug("[Cross-File Fix] âŒ Fix did not add missing fields")
                    return None
            
            logger.debug("[Cross-File Fix] âŒ Could not extract fixed code from response")
            return None
            
        except Exception as e:
            logger.debug(f"[Cross-File Fix] âŒ Error during fix: {e}")
            return None

    def _generate_repository_map(self) -> str:
        """
        Aideræ–¹å¼ã®ãƒªãƒã‚¸ãƒˆãƒªãƒãƒƒãƒ—ã‚’ç”Ÿæˆ
        
        ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®æ§‹é€ (ã‚¯ãƒ©ã‚¹ã€ãƒ¡ã‚½ãƒƒãƒ‰ã€é–¢æ•°ã®ã‚·ã‚°ãƒãƒãƒ£)ã‚’
        LLMãŒç†è§£ã—ã‚„ã™ã„å½¢å¼ã§å‡ºåŠ›ã—ã¾ã™ã€‚
        
        Returns:
            str: ãƒªãƒã‚¸ãƒˆãƒªãƒãƒƒãƒ—æ–‡å­—åˆ—(ã‚·ã‚°ãƒãƒãƒ£é›†)
                å„ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒ©ã‚¹ãƒ»ãƒ¡ã‚½ãƒƒãƒ‰ãƒ»é–¢æ•°ã®å®šç¾©ã‚’å«ã‚€
        
        Example output:
            cognix/models/user.py:
            class User
                def __init__(self, name, email)
                def validate(self)
            def create_user(username, password)
        
        å‚è€ƒ: https://aider.chat/docs/repomap.html
        """
        if not self.repository_analyzer:
                        return ""
        
        map_parts = []
        
        try:
            # repository_analyzer.repository_data ã‹ã‚‰å„ãƒ•ã‚¡ã‚¤ãƒ«ã®æƒ…å ±ã‚’å–å¾—
            if not hasattr(self.repository_analyzer, 'memory') or \
               not hasattr(self.repository_analyzer.memory, 'repository_data'):
                                return ""
            
            repository_data = self.repository_analyzer.memory.repository_data
            logger.debug(f"[DEBUG] Processing {len(repository_data)} files")
            
            # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚·ã‚°ãƒãƒãƒ£ã‚’æŠ½å‡º
            files_with_content = 0
            for file_path, repo_file in repository_data.items():
                # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’è¿½åŠ 
                file_header = f"\n{file_path}:"
                file_content = []
                
                # ã‚¯ãƒ©ã‚¹ã¨ãã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ 
                if hasattr(repo_file, 'classes') and repo_file.classes:
                    logger.debug(f"[DEBUG] {file_path}: {len(repo_file.classes)} classes")
                    for cls in repo_file.classes:
                        cls_name = cls.get('name', 'UnknownClass')
                        file_content.append(f"  class {cls_name}")
                        
                        # ã‚¯ãƒ©ã‚¹ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ 
                        methods = cls.get('methods', [])
                        logger.debug(f"[DEBUG]  Class {cls_name}: {len(methods)} methods")
                        for method in methods:
                            # ãƒ‡ãƒ¼ã‚¿å‹ã‚’æ¤œè¨¼(å¤ã„ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã¸ã®å¯¾å¿œ)
                            if isinstance(method, dict):
                                method_name = method.get('name', 'unknown_method')
                                params = method.get('params', [])
                                params_str = ', '.join(params)
                                file_content.append(f"    def {method_name}({params_str})")
                            elif isinstance(method, str):
                                # å¤ã„ãƒ‡ãƒ¼ã‚¿æ§‹é€ (æ–‡å­—åˆ—ã®ã¿)
                                logger.debug(f"[DEBUG]  âš  Old format detected: {method}")
                                file_content.append(f"    def {method}()")
                            else:
                                logger.debug(f"[DEBUG]  âš  Unknown method format: {type(method)}")
                else:
                    logger.debug(f"[DEBUG] {file_path}: no classes (hasattr={hasattr(repo_file, 'classes')}, value={getattr(repo_file, 'classes', None)})")
                
                # ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«é–¢æ•°ã‚’è¿½åŠ 
                if hasattr(repo_file, 'functions') and repo_file.functions:
                    logger.debug(f"[DEBUG] {file_path}: {len(repo_file.functions)} functions")
                    for func in repo_file.functions:
                        # ãƒ‡ãƒ¼ã‚¿å‹ã‚’æ¤œè¨¼(å¤ã„ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã¸ã®å¯¾å¿œ)
                        if isinstance(func, dict):
                            func_name = func.get('name', 'unknown_function')
                            params = func.get('params', [])
                            params_str = ', '.join(params)
                            file_content.append(f"  def {func_name}({params_str})")
                        elif isinstance(func, str):
                            # å¤ã„ãƒ‡ãƒ¼ã‚¿æ§‹é€ (æ–‡å­—åˆ—ã®ã¿)
                            logger.debug(f"[DEBUG]  âš  Old format detected: {func}")
                            file_content.append(f"  def {func}()")
                        else:
                            logger.debug(f"[DEBUG]  âš  Unknown function format: {type(func)}")
                else:
                    logger.debug(f"[DEBUG] {file_path}: no functions (hasattr={hasattr(repo_file, 'functions')}, value={getattr(repo_file, 'functions', None)})")
                
                # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®æ§‹é€ æƒ…å ±ã‚’æŠ½å‡º
                if file_path.endswith('.html') or file_path.endswith('.htm'):
                    html_info = self._extract_html_structure(file_path)
                    if html_info:
                        file_content.extend(html_info)
                        logger.debug(f"[DEBUG] {file_path}: extracted HTML structure ({len(html_info)} lines)")
                
                # CSSãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒ©ã‚¹æƒ…å ±ã‚’æŠ½å‡º
                if file_path.endswith('.css') or file_path.endswith('.scss') or file_path.endswith('.sass'):
                    css_info = self._extract_css_structure(file_path)
                    if css_info:
                        file_content.extend(css_info)
                        logger.debug(f"[DEBUG] {file_path}: extracted CSS structure ({len(css_info)} lines)")
                
                # ========================================
                # Phase 2è¿½åŠ : JavaScriptãƒ•ã‚¡ã‚¤ãƒ«ã®æ§‹é€ æƒ…å ±ã‚’æŠ½å‡º
                # ========================================
                if file_path.endswith('.js') or file_path.endswith('.jsx'):
                    js_info = self._extract_javascript_interfaces(file_path)
                    if js_info:
                        file_content.extend(js_info)
                        logger.debug(f"[DEBUG] {file_path}: extracted JavaScript structure ({len(js_info)} lines)")
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã«å†…å®¹ãŒã‚ã‚‹å ´åˆã®ã¿è¿½åŠ 
                if file_content:
                    map_parts.append(file_header)
                    map_parts.extend(file_content)
                    files_with_content += 1
            
            logger.debug(f"[DEBUG] Files with content: {files_with_content}/{len(repository_data)}")
            logger.debug(f"[DEBUG] Total map_parts: {len(map_parts)}")
        
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ç©ºæ–‡å­—åˆ—ã‚’è¿”ã™
            logger.debug(f"âš  Repository map generation failed: {e}")
            import traceback
            traceback.print_exc()
            return ""
        
        result = '\n'.join(map_parts)
        logger.debug(f"[DEBUG] Final map length: {len(result)} chars")
        return result

    def _extract_html_structure(self, file_path: str) -> List[str]:
        """
        HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ§‹é€ æƒ…å ±ã‚’æŠ½å‡ºï¼ˆPhase 2æ‹¡å¼µç‰ˆ - å…¨data-*å±æ€§å¯¾å¿œï¼‰
        
        Args:
            file_path: HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            
        Returns:
            List[str]: HTMLæ§‹é€ æƒ…å ±ã®ãƒªã‚¹ãƒˆ
            
        Phase 2æ‹¡å¼µå†…å®¹:
            - å…¨data-*å±æ€§ã‚’ç¶²ç¾…çš„ã«æŠ½å‡º
            - dataå±æ€§ã®ä¸€è¦§ã‚’ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        """
        try:
            from pathlib import Path
            import re
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
            full_path = Path.cwd() / file_path
            if not full_path.exists():
                return []
            
            content = full_path.read_text(encoding='utf-8', errors='ignore')
            structure = []
            
            # ========================================
            # å…¨data-*å±æ€§ã‚’åé›†ï¼ˆPhase 2è¿½åŠ ï¼‰
            # ========================================
            all_data_attrs = {}  # {attr_name: [values]}
            
            # data-*å±æ€§ã‚’å…¨ã¦æŠ½å‡ºã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
            data_attr_pattern = r'data-([a-zA-Z0-9_-]+)=["\']([^"\']*)["\']'
            data_matches = re.findall(data_attr_pattern, content, re.IGNORECASE)
            
            for attr_name, attr_value in data_matches:
                full_attr = f'data-{attr_name}'
                if full_attr not in all_data_attrs:
                    all_data_attrs[full_attr] = []
                if attr_value and attr_value not in all_data_attrs[full_attr]:
                    all_data_attrs[full_attr].append(attr_value)
            
            # ========================================
            # ãƒœã‚¿ãƒ³è¦ç´ ã‚’æŠ½å‡ºï¼ˆæ—¢å­˜æ©Ÿèƒ½ç¶­æŒï¼‰
            # ========================================
            button_pattern = r'<button[^>]*(?:class=["\']([^"\']*)["\'])?[^>]*(?:data-value=["\']([^"\']*)["\'])?[^>]*(?:id=["\']([^"\']*)["\'])?[^>]*>([^<]*)</button>'
            buttons = re.findall(button_pattern, content, re.IGNORECASE)
            if buttons:
                structure.append("  buttons:")
                for btn_class, data_value, btn_id, text in buttons:
                    btn_info = []
                    if text.strip():
                        btn_info.append(f"'{text.strip()}'")
                    if btn_class:
                        btn_info.append(f"class='{btn_class}'")
                    if data_value:
                        btn_info.append(f"value='{data_value}'")
                    if btn_id:
                        btn_info.append(f"id='{btn_id}'")
                    structure.append(f"    - {' '.join(btn_info)}")
            
            # ========================================
            # å…¥åŠ›è¦ç´ ã‚’æŠ½å‡ºï¼ˆæ—¢å­˜æ©Ÿèƒ½ç¶­æŒï¼‰
            # ========================================
            input_pattern = r'<input[^>]*(?:type=["\']([^"\']*)["\'])?[^>]*(?:id=["\']([^"\']*)["\'])?[^>]*(?:class=["\']([^"\']*)["\'])?[^>]*>'
            inputs = re.findall(input_pattern, content, re.IGNORECASE)
            if inputs:
                structure.append("  inputs:")
                for input_type, input_id, input_class in inputs:
                    input_info = []
                    if input_type:
                        input_info.append(f"type='{input_type}'")
                    if input_id:
                        input_info.append(f"id='{input_id}'")
                    if input_class:
                        input_info.append(f"class='{input_class}'")
                    if input_info:
                        structure.append(f"    - {' '.join(input_info)}")
            
            # ========================================
            # ãƒ•ã‚©ãƒ¼ãƒ è¦ç´ ã‚’æŠ½å‡ºï¼ˆæ—¢å­˜æ©Ÿèƒ½ç¶­æŒï¼‰
            # ========================================
            form_pattern = r'<form[^>]*(?:id=["\']([^"\']*)["\'])?[^>]*(?:action=["\']([^"\']*)["\'])?[^>]*>'
            forms = re.findall(form_pattern, content, re.IGNORECASE)
            if forms:
                structure.append("  forms:")
                for form_id, action in forms:
                    form_info = []
                    if form_id:
                        form_info.append(f"id='{form_id}'")
                    if action:
                        form_info.append(f"action='{action}'")
                    if form_info:
                        structure.append(f"    - {' '.join(form_info)}")
            
            # ========================================
            # Phase 2è¿½åŠ : å…¨data-*å±æ€§ã®ã‚µãƒãƒªãƒ¼
            # ========================================
            if all_data_attrs:
                structure.append("  data-attributes:")
                for attr_name in sorted(all_data_attrs.keys()):
                    values = all_data_attrs[attr_name]
                    # æœ€å¤§5å€‹ã¾ã§è¡¨ç¤º
                    values_display = values[:5]
                    values_str = ', '.join(f"'{v}'" for v in values_display)
                    if len(values) > 5:
                        values_str += f" (and {len(values) - 5} more)"
                    structure.append(f"    - {attr_name}: {values_str}")
            
            return structure
            
        except Exception as e:
            logger.debug(f"âš  HTML structure extraction failed for {file_path}: {e}")
            return []

    def _extract_javascript_interfaces(self, file_path: str) -> List[str]:
            """
            JavaScriptãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰datasetä½¿ç”¨ç®‡æ‰€ã¨dataå±æ€§å‚ç…§ã‚’æŠ½å‡ºï¼ˆPhase 2æ–°è¦å®Ÿè£…ï¼‰
            
            Args:
                file_path: JavaScriptãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
                
            Returns:
                List[str]: JavaScriptæ§‹é€ æƒ…å ±ã®ãƒªã‚¹ãƒˆ
                
            æŠ½å‡ºå†…å®¹:
                - element.dataset.xxx ãƒ‘ã‚¿ãƒ¼ãƒ³
                - querySelector('[data-xxx]') ãƒ‘ã‚¿ãƒ¼ãƒ³
                - getAttribute('data-xxx') ãƒ‘ã‚¿ãƒ¼ãƒ³
                - addEventListenerç™»éŒ²
                - é–¢æ•°å®šç¾©
            """
            try:
                from pathlib import Path
                import re
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
                full_path = Path.cwd() / file_path
                if not full_path.exists():
                    return []
                
                content = full_path.read_text(encoding='utf-8', errors='ignore')
                structure = []
                
                # ========================================
                # datasetä½¿ç”¨ç®‡æ‰€ã‚’æŠ½å‡º
                # ========================================
                dataset_properties = set()
                
                # element.dataset.property ãƒ‘ã‚¿ãƒ¼ãƒ³
                dataset_pattern = r'\.dataset\.([a-zA-Z_][a-zA-Z0-9_]*)'
                matches = re.findall(dataset_pattern, content)
                dataset_properties.update(matches)
                
                if dataset_properties:
                    structure.append("  dataset-usage:")
                    for prop in sorted(dataset_properties):
                        structure.append(f"    - dataset.{prop}")
                
                # ========================================
                # dataå±æ€§ã‚¯ã‚¨ãƒªã‚’æŠ½å‡º
                # ========================================
                data_queries = set()
                
                # querySelector/querySelectorAll('[data-*]') ãƒ‘ã‚¿ãƒ¼ãƒ³
                query_pattern1 = r'querySelector(?:All)?\(["\'][^"\']*\[data-([a-zA-Z0-9_-]+)'
                matches1 = re.findall(query_pattern1, content)
                data_queries.update(matches1)
                
                # getAttribute('data-*') ãƒ‘ã‚¿ãƒ¼ãƒ³
                query_pattern2 = r'getAttribute\(["\']data-([a-zA-Z0-9_-]+)["\']'
                matches2 = re.findall(query_pattern2, content)
                data_queries.update(matches2)
                
                if data_queries:
                    structure.append("  data-queries:")
                    for query in sorted(data_queries):
                        structure.append(f"    - [data-{query}]")
                
                # ========================================
                # ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒŠãƒ¼ç™»éŒ²ã‚’æŠ½å‡º
                # ========================================
                event_listeners = []
                
                # addEventListener ãƒ‘ã‚¿ãƒ¼ãƒ³
                listener_pattern = r'\.addEventListener\(["\']([a-zA-Z]+)["\']\s*,\s*([a-zA-Z_][a-zA-Z0-9_]*)'
                matches = re.findall(listener_pattern, content)
                
                if matches:
                    structure.append("  event-listeners:")
                    event_types = set()
                    for event_type, handler in matches:
                        event_types.add(event_type)
                    
                    for event_type in sorted(event_types):
                        structure.append(f"    - {event_type}")
                
                # ========================================
                # é–¢æ•°å®šç¾©ã‚’æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
                # ========================================
                functions = set()
                
                # functionå®£è¨€
                func_pattern1 = r'function\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*\('
                matches1 = re.findall(func_pattern1, content)
                functions.update(matches1)
                
                # const/let/var é–¢æ•°å¼
                func_pattern2 = r'(?:const|let|var)\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*=\s*(?:function|\([^)]*\)\s*=>)'
                matches2 = re.findall(func_pattern2, content)
                functions.update(matches2)
                
                if functions:
                    structure.append("  functions:")
                    for func in sorted(functions)[:20]:  # æœ€å¤§20å€‹ã¾ã§
                        structure.append(f"    - {func}()")
                
                return structure
                
            except Exception as e:
                logger.debug(f"âš  JavaScript structure extraction failed for {file_path}: {e}")
                return []

    def _extract_css_structure(self, file_path: str) -> List[str]:
            """
            CSSãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚»ãƒ¬ã‚¯ã‚¿æƒ…å ±ã‚’æŠ½å‡º
            
            Args:
                file_path: CSSãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
                
            Returns:
                List[str]: CSSæ§‹é€ æƒ…å ±ã®ãƒªã‚¹ãƒˆ
            """
            try:
                from pathlib import Path
                import re
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
                full_path = Path.cwd() / file_path
                if not full_path.exists():
                    return []
                
                content = full_path.read_text(encoding='utf-8', errors='ignore')
                structure = []
                
                # ã‚¯ãƒ©ã‚¹ã‚»ãƒ¬ã‚¯ã‚¿ã‚’æŠ½å‡º (.classname)
                class_pattern = r'\.([a-zA-Z_][\w-]*)\s*[,{]'
                classes = set(re.findall(class_pattern, content))
                
                # IDã‚»ãƒ¬ã‚¯ã‚¿ã‚’æŠ½å‡º (#idname)
                id_pattern = r'#([a-zA-Z_][\w-]*)\s*[,{]'
                ids = set(re.findall(id_pattern, content))
                
                if classes:
                    structure.append("  classes:")
                    for cls in sorted(classes)[:20]:  # æœ€å¤§20å€‹ã¾ã§
                        structure.append(f"    - .{cls}")
                
                if ids:
                    structure.append("  ids:")
                    for id_name in sorted(ids)[:10]:  # æœ€å¤§10å€‹ã¾ã§
                        structure.append(f"    - #{id_name}")
                
                return structure
                
            except Exception as e:
                logger.debug(f"âš  CSS structure extraction failed for {file_path}: {e}")
                return []

    def _get_project_context(self) -> str:
        """
        ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—(ãƒªãƒã‚¸ãƒˆãƒªãƒãƒƒãƒ—ã‚’å«ã‚€)
        
        LLMãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«æ¸¡ã™ãŸã‚ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
        
        â­ ã‚¹ãƒ†ãƒƒãƒ—0-1ã§æ‹¡å¼µ:
        - ãƒªãƒã‚¸ãƒˆãƒªãƒãƒƒãƒ—å…¨ä½“ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å«ã‚ã‚‹ã‚ˆã†ã«æ”¹ä¿®
        - æ—¢å­˜ã®çµ±è¨ˆæƒ…å ±å‡ºåŠ›ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯(ã‚¨ãƒ©ãƒ¼æ™‚ã®ã¿)ã«å¤‰æ›´
        
        Returns:
            str: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—åˆ—
                åŸºæœ¬ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ + ãƒªãƒã‚¸ãƒˆãƒªãƒãƒƒãƒ—ã‚’å«ã‚€
        """
        context_parts = []
        
        # åŸºæœ¬ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ(Repository Analyzerã®ãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆ)
        if self.repository_analyzer and hasattr(self.repository_analyzer, 'analysis_stats'):
            stats = self.repository_analyzer.analysis_stats
            if stats and stats.get('analyzed_files', 0) > 0:
                # Repository Analyzerã‹ã‚‰æœ€æ–°ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—
                repo_data = self.repository_analyzer.memory.repository_data
                
                # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã¨è¨€èªæƒ…å ±
                # repository_dataã¯ Dict[str, RepositoryFile] å‹
                total_files = len(repo_data)
                
                # è¨€èªæƒ…å ±ã‚’é›†è¨ˆ
                languages = {}
                for repo_file in repo_data.values():
                    lang = repo_file.language
                    if lang:
                        languages[lang] = languages.get(lang, 0) + 1
                
                # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ
                root_dir = self.context.root_dir if self.context else Path.cwd()
                
                # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—åˆ—ã‚’æ§‹ç¯‰
                context_parts.append(f"# Project Context")
                context_parts.append(f"Root: {root_dir}")
                context_parts.append(f"Total files: {total_files}")
                                
                # è¨€èªæƒ…å ±
                if languages:
                    languages_str = ', '.join(
                        f"{lang} ({count})" 
                        for lang, count in sorted(
                            languages.items(), 
                            key=lambda x: x[1], 
                            reverse=True
                        )
                    )
                    context_parts.append(f"Languages: {languages_str}")
        elif self.context:
            # Repository AnalyzerãŒãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            context_str = str(self.context)
            # é•·ã™ãã‚‹å ´åˆã¯çœç•¥(æ—¢å­˜ã®å‹•ä½œã‚’ç¶­æŒ)
            if len(context_str) > 10000:
                context_parts.append(context_str[:10000] + "...")
            else:
                context_parts.append(context_str)
        
        # â­ ãƒªãƒã‚¸ãƒˆãƒªãƒãƒƒãƒ—å…¨ä½“ã‚’å«ã‚ã‚‹(æ–°è¦è¿½åŠ )
        if self.repository_analyzer:
            try:
                # ãƒªãƒã‚¸ãƒˆãƒªãƒãƒƒãƒ—ã‚’ç”Ÿæˆ
                repo_map = self._generate_repository_map()
                
                if repo_map:
                    # ãƒªãƒã‚¸ãƒˆãƒªãƒãƒƒãƒ—ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«è¿½åŠ 
                    context_parts.append("\n# Repository Structure:")
                    context_parts.append(repo_map)
                else:
                    # ãƒªãƒã‚¸ãƒˆãƒªãƒãƒƒãƒ—ãŒç©ºã®å ´åˆã¯çµ±è¨ˆæƒ…å ±ã®ã¿(ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)
                    stats = self.repository_analyzer.analysis_stats
                    if stats and stats.get('analyzed_files', 0) > 0:
                        analyzed_count = stats.get('analyzed_files', 0)
                        total_classes = stats.get('total_classes', 0)
                        total_functions = stats.get('total_functions', 0)
                        
                        context_parts.append(f"\n# Repository Statistics:")
                        context_parts.append(f"Analyzed files: {analyzed_count}")
                        context_parts.append(f"Total classes: {total_classes}")
                        context_parts.append(f"Total functions: {total_functions}")
            
            except Exception as e:
                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯çµ±è¨ˆæƒ…å ±ã®ã¿å‡ºåŠ›(æ—¢å­˜ã®å‹•ä½œ)
                try:
                    stats = self.repository_analyzer.analysis_stats
                    if stats and stats.get('analyzed_files', 0) > 0:
                        analyzed_count = stats.get('analyzed_files', 0)
                        total_classes = stats.get('total_classes', 0)
                        total_functions = stats.get('total_functions', 0)
                        
                        context_parts.append(f"\n# Repository Statistics:")
                        context_parts.append(f"Analyzed files: {analyzed_count}")
                        context_parts.append(f"Total classes: {total_classes}")
                        context_parts.append(f"Total functions: {total_functions}")
                except:
                    # çµ±è¨ˆæƒ…å ±ã®å–å¾—ã‚‚å¤±æ•—ã—ãŸå ´åˆã¯ä½•ã‚‚è¿½åŠ ã—ãªã„
                    pass
        
        # â­ UI Knowledge ã‚’è¿½åŠ ï¼ˆæ–°è¦è¿½åŠ ï¼‰
        ui_knowledge = self._get_ui_knowledge()
        if ui_knowledge:
            context_parts.append("\n# UI Component Knowledge:")
            context_parts.append(ui_knowledge)
        
        return '\n'.join(context_parts)

    def _get_ui_knowledge(self) -> str:
        """
        UI ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®çŸ¥è­˜ã‚’å–å¾—ï¼ˆJSONå½¢å¼ï¼‰
        
        .cognix/ui-knowledge.json ã‹ã‚‰ UI ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ä»•æ§˜ã‚’èª­ã¿è¾¼ã¿ã€
        LLMã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ä½¿ç”¨ã§ãã‚‹æ–‡å­—åˆ—å½¢å¼ã§è¿”ã—ã¾ã™ã€‚
        
        å„ªå…ˆé †ä½:
        1. ~/.cognix/knowledge/ui-knowledge.json (ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®š)
        2. ~/.cognix/ui-knowledge.json (ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šãƒ»ç°¡æ˜“ç‰ˆ)
        3. .cognix/ui-knowledge.json (ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…)
        
        Returns:
            str: UI knowledge JSONæ–‡å­—åˆ—
                 - ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆ: ç©ºæ–‡å­—åˆ—
                 - èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼æ™‚: ç©ºæ–‡å­—åˆ—
        """
        # å„ªå…ˆé †ä½ä»˜ãã§ãƒ‘ã‚¹ã‚’è©¦ã™
        user_cognix_dir = Path.home() / '.cognix'
        
        # å„ªå…ˆé †ä½1: ~/.cognix/knowledge/ui-knowledge.json
        ui_knowledge_file = user_cognix_dir / 'knowledge' / 'ui-knowledge.json'
        
        # å„ªå…ˆé †ä½2: ~/.cognix/ui-knowledge.json
        if not ui_knowledge_file.exists():
            ui_knowledge_file = user_cognix_dir / 'ui-knowledge.json'
        
        # å„ªå…ˆé †ä½3: .cognix/ui-knowledge.json (ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…)
        if not ui_knowledge_file.exists():
            ui_knowledge_file = Path(".cognix/ui-knowledge.json")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ç©ºæ–‡å­—åˆ—ã‚’è¿”ã™
        if not ui_knowledge_file.exists():
            logger.debug("[UI Knowledge] File not found in any location")
            return ""
        
        try:
            # JSON ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
            with open(ui_knowledge_file, 'r', encoding='utf-8') as f:
                knowledge_data = json.load(f)
            
            # JSONæ–‡å­—åˆ—ã«å¤‰æ›ï¼ˆæ•´å½¢æ¸ˆã¿ã€æ—¥æœ¬èªã‚‚ç¶­æŒï¼‰
            knowledge_str = json.dumps(knowledge_data, indent=2, ensure_ascii=False)
            
            # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°
            logger.debug(f"[UI Knowledge] Loaded from {ui_knowledge_file}")
            logger.debug(f"[UI Knowledge] Size: {len(knowledge_str)} chars")
            
            return knowledge_str
        
        except json.JSONDecodeError as e:
            # JSONå½¢å¼ã‚¨ãƒ©ãƒ¼
            logger.debug(f"[UI Knowledge] JSON decode error: {e}")
            return ""
        
        except Exception as e:
            # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼
            logger.debug(f"[UI Knowledge] Failed to load: {e}")
            return ""

    def _get_code_generation_system_prompt(self, goal_constraints: Dict = None) -> str:
        """
        ã‚³ãƒ¼ãƒ‰ç”Ÿæˆç”¨ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—
        
        Args:
            goal_constraints: Goalåˆ¶ç´„æƒ…å ±ï¼ˆPhase 1å®Ÿè£…ï¼‰
        
        Returns:
            ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ–‡å­—åˆ—
        """
        # ğŸ†• ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°: å—ã‘å–ã£ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç¢ºèª
        logger.debug(f"[VERIFY-METHOD] _get_code_generation_system_prompt called")
        logger.debug(f"[VERIFY-METHOD] goal_constraints parameter: {goal_constraints}")
        if goal_constraints:
            logger.debug(f"[VERIFY-METHOD] constraint_strength: {goal_constraints.get('constraint_strength')}")
            logger.debug(f"[VERIFY-METHOD] keep_keywords: {goal_constraints.get('keep_keywords')}")
            logger.debug(f"[VERIFY-METHOD] only_keywords: {goal_constraints.get('only_keywords')}")
        else:
            logger.debug(f"[VERIFY-METHOD] goal_constraints is None or empty - constraint prompt will NOT be added")
        
        # ğŸ†• Pythonãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã‚’å–å¾—
        import sys
        python_version = f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}"
        python_major_minor = f"{sys.version_info.major}.{sys.version_info.minor}"
        logger.debug(f"[VERIFY-METHOD] Python version detected: {python_version}")
        
        base_prompt = f"""You are an expert software developer assistant.

    RUNTIME ENVIRONMENT (CRITICAL - Use compatible package versions):
    - Python Version: {python_version}
    - When generating requirements.txt for Python {python_major_minor}, use ONLY packages compatible with Python {python_major_minor}
    - For Python 3.13+: Use SQLAlchemy>=2.0.25, Flask>=3.0.0, marshmallow>=3.21.0
    - For Python 3.12: Use SQLAlchemy>=2.0.20
    - For Python 3.11 and below: Standard versions are usually compatible
    - ALWAYS prefer latest stable versions of packages for better compatibility

    LIBRARY API COMPATIBILITY (CRITICAL - Use modern API, NOT deprecated):
    - marshmallow 3.x: Use `load_default=` instead of `missing=` for default values on load
    - marshmallow 3.x: Use `dump_default=` instead of `default=` for default values on dump
    - marshmallow 3.x: Example: fields.Int(load_default=0) NOT fields.Int(missing=0)
    - SQLAlchemy 2.x: Prefer `mapped_column()` over `Column()` for new ORM models
    - SQLAlchemy 2.x: Use `select()` instead of `Query.filter()` for new code
    - Flask 3.x: Use `app.json.provider_class` instead of deprecated JSON APIs
    - Pydantic 2.x: Use `model_validator` instead of `validator`, `field_validator` instead of `root_validator`

    HTML/CSS/JS FILE PLACEMENT AND REFERENCE RULES (CRITICAL):
    
    FOR FLASK/BACKEND PROJECTS (CRITICAL - Most common case):
    - ALWAYS use absolute paths starting with /static/:
      CORRECT: <link rel="stylesheet" href="/static/css/styles.css">
      CORRECT: <script src="/static/js/app.js"></script>
      WRONG:   <link rel="stylesheet" href="css/styles.css">
      WRONG:   <script src="js/app.js"></script>
    - WHY: Flask serves static files at /static/ URL path. Even if index.html is inside 
      static/ directory, the browser URL is typically '/' (via send_from_directory),
      so relative paths resolve incorrectly and cause 404 errors.
    - For Jinja2 templates, use url_for():
      BEST: <link rel="stylesheet" href="{{ url_for('static', filename='css/styles.css') }}">
    
    FOR STATIC HTML (No server, opening directly in browser):
    - Use relative paths when opening HTML directly via file://
      CORRECT: <link rel="stylesheet" href="css/styles.css">
    
    GENERAL RULES:
    - ALWAYS generate ALL files referenced in HTML:
      If HTML has <link href="/static/css/styles.css">, you MUST generate static/css/styles.css
      If HTML has <script src="/static/js/app.js">, you MUST generate static/js/app.js
    - CSS and JS files MUST be in the same relative directory structure as referenced in HTML
    - NEVER create nested static/static/ directories - this indicates a path error
    
    DEFAULT: When in doubt, use /static/ prefix (Flask-style absolute paths).

    __init__.py FILES (CRITICAL - Keep them simple):
    - __init__.py files should contain ONLY import statements and __all__ exports
    - Do NOT include placeholder classes, fallback implementations, or try/except import wrappers
    - Do NOT include TODO comments or "implement later" notes
    - Trust that referenced modules exist (they are generated in the same session)
    - CORRECT example:
      ```python
      from .user_service import UserService
      from .product_service import ProductService
      
      __all__ = ['UserService', 'ProductService']
      ```
    - WRONG example (do NOT do this):
      ```python
      try:
          from .user_service import UserService
      except ImportError:
          class UserService:  # Placeholder - DO NOT DO THIS
              pass
      ```

    Your role is to generate high-quality, production-ready code based on user requirements.

    CRITICAL RULES:
    1. Always follow the exact filename specified in the user's request
    2. When asked to generate a specific file (e.g., routes/auth.js), you MUST return that exact filename
    3. Use the ## File: filename format to specify filenames
    4. Write clean, well-documented, and maintainable code
    5. Follow industry best practices and modern coding standards
    6. Include appropriate error handling and edge case management
    7. Add helpful comments for complex logic
    8. Ensure code is secure and follows security best practices

    CODE QUALITY STANDARDS:
    - Use meaningful variable and function names
    - Keep functions focused and single-purpose
    - Avoid code duplication (DRY principle)
    - Write modular and reusable code
    - Include proper type hints/annotations when applicable
    - Follow the style guide of the target language
    - Optimize for readability over cleverness

    SECURITY CONSIDERATIONS:
    - Validate all user inputs
    - Use parameterized queries for database operations
    - Implement proper authentication and authorization
    - Avoid exposing sensitive information
    - Use secure random number generators
    - Follow OWASP guidelines

    DEFENSIVE CODING REQUIREMENTS (CRITICAL - Prevents Runtime Errors):
    
    All functions that access game objects or shared state MUST include guard conditions.
    This prevents null reference errors, state-related bugs, and race conditions.

    1. NULL/UNDEFINED CHECKS (Required for any shared object access):
       Every function accessing objects like currentPiece, player, selectedItem, etc. MUST start with a guard:
       ```javascript
       function anyFunctionUsingSharedObject() {{
           if (!sharedObject) return;
           // ... rest of code
       }}
       ```
       Apply to: draw(), update(), render(), and any function that reads object properties.

    2. STATE GUARDS (Required for input handlers and game loops):
       Every input handler and update function MUST verify state before processing:
       ```javascript
       function handleInput(e) {{
           if (gameState !== 'playing') return;
           // ... handle input
       }}
       
       function gameLoop(timestamp) {{
           requestAnimationFrame(gameLoop);
           if (gameState !== 'playing') return;
           // ... update game
       }}
       ```

    3. SAFE SETTIMEOUT/CALLBACK PATTERN (Required for delayed operations):
       When using setTimeout with state-dependent functions, ALWAYS re-check state inside callback:
       ```javascript
       // WRONG - state may change during timeout
       setTimeout(spawnEnemy, 100);
       
       // CORRECT - re-check state
       setTimeout(() => {{
           if (gameState !== 'playing') return;
           spawnEnemy();
       }}, 100);
       ```

    4. BOUNDS CHECKING (Required for array/grid access):
       All array or grid access MUST validate indices:
       ```javascript
       function getCell(x, y) {{
           if (x < 0 || x >= COLS || y < 0 || y >= ROWS) return null;
           return grid[y][x];
       }}
       ```

    5. STATE TRANSITION SAFETY (Required for state changes):
       When transitioning states (start, pause, gameover), ALWAYS clean up and initialize properly:
       ```javascript
       function startGame() {{
           // Clear previous state completely
           clearAllTimers();
           resetAllVariables();
           gameState = 'playing';
           // Then initialize
           init();
       }}
       ```

    6. EVENT HANDLER SAFETY (Required for UI interactions):
       All click/touch handlers MUST check state and object existence:
       ```javascript
       button.addEventListener('click', () => {{
           if (gameState !== 'playing') return;
           if (!targetObject) return;
           performAction();
       }});
       ```

    APPLY THESE PATTERNS TO:
    - Games (currentPiece, player, enemies, bullets, particles)
    - Forms (selectedItem, currentUser, formData)
    - Canvas apps (context, currentShape, selectedTool)
    - Any application with shared mutable state

    OUTPUT FORMAT:
    When generating multiple files, use this format:
    ## File: filename1.ext
    ```
    code for file 1
    ```

    ## File: filename2.ext
    ```
    code for file 2
    ```

    When generating a single file, you may optionally use:
    ## File: filename.ext
    ```
    code
    ```

    Or simply provide the code directly if the filename is clear from context.

    USING REPOSITORY MAP:
    
    If a Repository Map is provided in the context, USE IT to understand the project structure:
    
    1. IDENTIFY ALL REQUIRED FILES:
    - The map shows existing files with their classes, methods, functions, HTML buttons, and CSS classes
    - When modifying functionality, check ALL related files in the map
    - Example: If asked to "add multiplication to calculator":
        * Check map for index.html or calculator.html (UI with buttons - you'll see existing button list)
        * Check map for calculator.js (logic with calculation methods - you'll see add(), subtract())
        * Check map for styles.css (styling for buttons - you'll see CSS classes)
        * MODIFY ALL RELEVANT FILES: If buttons are missing, add them to HTML. If methods are missing, add them to JS.

    2. GENERATE COMPLETE SOLUTIONS:
    - CRITICAL: When adding new functionality (like new operators), you MUST modify BOTH UI and logic files
    - DO NOT generate only UI changes without updating the underlying logic files
    - DO NOT generate only logic changes without updating the UI files
    - DO NOT ignore related files shown in the repository map
    - Include ALL files needed to fulfill the requirement completely
    - Example: Adding multiplication REQUIRES both HTML button changes AND JavaScript method additions

    3. RESPECT EXISTING CODE STRUCTURE:
   - Use abstractions and patterns shown in the map
   - Follow the existing project architecture
   - Maintain consistency with existing code style
   - Do NOT embed CSS in HTML if styles.css exists
   - Do NOT rewrite entire files when small changes suffice
    
    CRITICAL: If the repository map shows related files (like calculator.js when working on calculator.html),
    you MUST modify all relevant files to create a complete, functional solution.

    HTML/JAVASCRIPT INTERFACE CONSISTENCY:
    
    When working with HTML and JavaScript files together, ensure data attribute consistency:
    
    1. HTML DATA ATTRIBUTES:
       - All data-* attributes in HTML (e.g., data-action, data-number) MUST be referenced in JavaScript
       - Example: <button data-action="add"> requires JavaScript: element.dataset.action
    
    2. JAVASCRIPT DATASET USAGE:
       - All dataset properties accessed in JavaScript MUST exist in HTML
       - Example: element.dataset.operation requires HTML: data-operation="..."
    
    3. INTERFACE VALIDATION:
       - Before generating code, verify that:
         * Every HTML data-* attribute has a corresponding JavaScript reference
         * Every JavaScript dataset access has a corresponding HTML data-* attribute
       - If mismatches are detected, fix them in the generated code
    
    4. COMMON PATTERNS TO CHECK:
       - Event listeners accessing element.dataset.* properties
       - querySelectorAll('[data-*]') selectors
       - getAttribute('data-*') calls
    
    CRITICAL: Data attribute mismatches between HTML and JavaScript are a common source of runtime errors.
    Always ensure complete consistency between HTML data-* attributes and JavaScript dataset usage.

    ## Cross-File Consistency and Reference Integrity (CRITICAL)

    ### Configuration File Patterns (Python)
    When creating config.py or similar configuration files:
    - If you define a variable at module level (e.g., DATABASE_PATH = "..."), 
      you MUST also define it as a class attribute if it will be accessed via class instances
    - Example of CORRECT pattern:
      ```python
      # Module level
      DATABASE_PATH = os.path.join(BASE_DIR, 'db.sqlite')
      
      class Config:
          # Class attribute (REQUIRED for instance access)
          DATABASE_PATH = DATABASE_PATH
          DATABASE_URI = f"sqlite:///{{DATABASE_PATH}}"
      ```
    - Example of INCORRECT pattern that causes AttributeError:
      ```python
      # Module level
      DATABASE_PATH = os.path.join(BASE_DIR, 'db.sqlite')
      
      class Config:
          # DATABASE_PATH is NOT defined as class attribute âŒ
          DATABASE_URI = f"sqlite:///{{DATABASE_PATH}}"  # This works at module level
      
      # In another file:
      config = get_config()  # Returns Config instance
      config.DATABASE_PATH  # âŒ AttributeError: type object 'Config' has no attribute 'DATABASE_PATH'
      ```

    ### Import and Attribute Access Verification
    Before using any attribute from an imported module:
    - VERIFY that the attribute actually exists in the target module
    - If importing from config.py, check what attributes are defined as class attributes vs module variables
    - Use consistent access patterns throughout the codebase

    ### Cross-File Reference Checklist
    Before generating any file that imports from another file:
    1. Review the imported module's structure (class vs module level definitions)
    2. Ensure all accessed attributes are actually defined in the target
    3. Use the same access pattern consistently (e.g., if using config.DATABASE_PATH, ensure DATABASE_PATH is a class attribute)
    4. For configuration values, prefer class attributes over module variables for better testability and consistency

    ## EMOJI USAGE POLICY (CRITICAL)
    
    NEVER use emojis (ğŸ“, âœï¸, â¤ï¸, ğŸ‰, âœ¨, etc.) in UI text, headers, or content unless explicitly requested by the user.
    
    WHY: Emojis make applications look unprofessional and cheap. Professional applications use clean text and proper iconography.
    
    RULES:
    1. DO NOT use emojis in:
       - Headings (<h1>, <h2>, etc.)
       - Button text
       - Labels and placeholders
       - Empty states and messages
       - Footer text
       - Any user-facing text
    
    2. EXCEPTION: Only use emojis if the user specifically requests them in their goal
       - Examples of explicit requests: "add emoji", "use ğŸ‰ icon", "include â¤ï¸ symbol"
       - If the request is ambiguous, default to NO emojis
    
    3. USE INSTEAD:
       - SVG icons (preferred for web apps)
       - Icon fonts (Font Awesome, Material Icons, Heroicons)
       - Clean text with proper typography
       - CSS styling and colors for visual interest
       - Professional unicode symbols (â€¢, â†’, âœ“, Ã—) where minimal decoration is needed
    
    CORRECT EXAMPLES:
    âœ… <h1>My Todo List</h1>
    âœ… <h1 class="app-title">Todo List</h1>
    âœ… <button class="btn-add">Add Task</button>
    âœ… <button class="btn-add"><svg>...</svg> Add</button>
    âœ… <p class="empty-text">No tasks yet. Add one to get started!</p>
    âœ… <p>Built with HTML, CSS & JavaScript</p>
    
    INCORRECT EXAMPLES (NEVER DO THIS):
    âŒ <h1>ğŸ“ My Todo List</h1>
    âŒ <h1>My Todo List âœï¸</h1>
    âŒ <button class="btn-add">Add âœï¸</button>
    âŒ <button class="btn-add">Add +</button> â† Use <svg> or icon font instead
    âŒ <p class="empty-text">âœ¨ No tasks yet. Add one to get started!</p>
    âŒ <p>Built with â¤ï¸ using vanilla JavaScript</p>
    
    REMEMBER: 
    - Professional applications are emoji-free by default
    - Clean typography > Emojis
    - Icons > Emojis
    - If unsure, DO NOT use emojis

    ## MANDATORY FEATURE IMPLEMENTATION (CRITICAL - DO NOT SKIP)

    When the user's goal mentions specific features, they are MANDATORY requirements, not optional enhancements.
    You MUST implement ALL features explicitly mentioned in the goal, even if the code becomes longer.

    ### Priority Rules:
    1. NEVER skip features due to code length or token limits
    2. If you cannot implement everything, prioritize in this order:
       - Core functionality (game logic, main features) - HIGHEST
       - Audio/Sound (Web Audio API, effects) - HIGH
       - Visual effects (particles, animations) - HIGH
       - UI feedback (popups, warnings, transitions) - HIGH
       - Polish/decoration (gradients, glows) - MEDIUM
    3. NEVER silently omit features - if truly impossible, add a TODO comment explaining why

    ### Mandatory Feature Keywords:
    When you see these keywords in the goal, they MUST be implemented:

    **Audio/Sound (MUST implement if mentioned):**
    - "Web Audio API" â†’ Implement AudioContext with oscillators/buffers
    - "sound effects" / "sfx" â†’ Implement playable sound functions
    - "audio" / "music" / "bgm" â†’ Implement audio playback system
    - "mute" / "volume" â†’ Implement audio control

    **Visual Effects (MUST implement if mentioned):**
    - "particle" / "particles" â†’ Implement particle system with spawn/update/render
    - "screen shake" / "shake" â†’ Implement camera/canvas shake effect
    - "slow motion" / "slowmo" â†’ Implement time scale modification
    - "flash" / "screen flash" â†’ Implement overlay flash effect
    - "explosion" â†’ Implement explosion animation with particles
    - "trail" / "motion trail" â†’ Implement position history rendering

    **UI Feedback (MUST implement if mentioned):**
    - "popup" / "float text" / "floating" â†’ Implement animated text that rises and fades
    - "WARNING" / "alert" â†’ Implement flashing warning text/screen
    - "combo" â†’ Implement combo counter with visual feedback
    - "transition" â†’ Implement screen/state transition animations

    **Visual Style (MUST implement if mentioned):**
    - "glassmorphism" / "frosted" / "blur" â†’ Implement backdrop-filter blur effect
    - "neon" / "glow" â†’ Implement CSS/canvas glow effects (box-shadow, filter)
    - "CRT" / "scanline" / "retro" â†’ Implement scanline overlay effect
    - "gradient" â†’ Implement gradient backgrounds/fills
    - "parallax" â†’ Implement multi-layer scrolling effect
    - "vignette" â†’ Implement edge darkening effect

    **Background Effects (MUST implement if mentioned):**
    - "animated background" â†’ Implement moving/animated background elements
    - "grid" / "grid lines" â†’ Implement grid pattern (static or animated)
    - "star field" / "stars" â†’ Implement star/particle background

    **Controls (MUST implement if mentioned):**
    - Key bindings (e.g., "P for pause", "M for mute") â†’ Implement keyboard handlers
    - Mouse controls â†’ Implement mouse event handlers
    - Touch controls â†’ Implement touch event handlers

    ### Implementation Patterns:

    **Web Audio API Basic Pattern:**
    ```javascript
    const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
    function playSound(frequency, duration, type = 'sine') {{
      const oscillator = audioCtx.createOscillator();
      const gainNode = audioCtx.createGain();
      oscillator.connect(gainNode);
      gainNode.connect(audioCtx.destination);
      oscillator.frequency.value = frequency;
      oscillator.type = type;
      gainNode.gain.setValueAtTime(0.3, audioCtx.currentTime);
      gainNode.gain.exponentialRampToValueAtTime(0.01, audioCtx.currentTime + duration);
      oscillator.start(audioCtx.currentTime);
      oscillator.stop(audioCtx.currentTime + duration);
    }}
    ```

    **Particle System Basic Pattern:**
    ```javascript
    const particles = [];
    function createParticle(x, y, color) {{
      particles.push({{ x, y, color, vx: (Math.random()-0.5)*10, vy: (Math.random()-0.5)*10, life: 1.0, decay: 0.02 }});
    }}
    function updateParticles(dt) {{
      for (let i = particles.length - 1; i >= 0; i--) {{
        const p = particles[i];
        p.x += p.vx * dt; p.y += p.vy * dt; p.life -= p.decay;
        if (p.life <= 0) particles.splice(i, 1);
      }}
    }}
    ```

    **Screen Shake Basic Pattern:**
    ```javascript
    let shakeAmount = 0;
    function triggerShake(intensity) {{ shakeAmount = intensity; }}
    function applyShake(ctx) {{
      if (shakeAmount > 0.5) {{
        ctx.translate((Math.random()-0.5)*shakeAmount, (Math.random()-0.5)*shakeAmount);
        shakeAmount *= 0.9;
      }}
    }}
    ```

    **Floating Text Basic Pattern:**
    ```javascript
    const floatingTexts = [];
    function createFloatingText(x, y, text, color) {{
      floatingTexts.push({{ x, y, text, color, vy: -2, alpha: 1.0, decay: 0.02 }});
    }}
    function renderFloatingTexts(ctx) {{
      for (let i = floatingTexts.length - 1; i >= 0; i--) {{
        const ft = floatingTexts[i];
        ctx.globalAlpha = ft.alpha;
        ctx.fillStyle = ft.color;
        ctx.fillText(ft.text, ft.x, ft.y);
        ft.y += ft.vy; ft.alpha -= ft.decay;
        if (ft.alpha <= 0) floatingTexts.splice(i, 1);
      }}
      ctx.globalAlpha = 1.0;
    }}
    ```

    ### Technology Choice Rules:
    - "Pure HTML + CSS + JS" / "vanilla" / "no framework" â†’ DO NOT use React/Vue/Angular
    - "React" / "React component" â†’ Use React
    - "Vue" / "Vue component" â†’ Use Vue
    - No specification â†’ Choose appropriate technology based on complexity
    
    CRITICAL: If the goal says "Pure HTML + CSS + JS" or "no frameworks", you MUST use vanilla JavaScript only.

    Remember: Always prioritize code quality, security, and maintainability."""
        
        # ğŸ†• ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ç¦æ­¢ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€å¼·åˆ¶ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ 
        if getattr(self, '_no_framework_mode', False):
            no_framework_prompt = """

    ## âš ï¸ FRAMEWORK PROHIBITION (ABSOLUTE - VIOLATING THIS = COMPLETE REJECTION)

    YOU ARE STRICTLY PROHIBITED FROM USING ANY FRAMEWORKS.

    **FORBIDDEN (DO NOT USE UNDER ANY CIRCUMSTANCES):**
    - React, Vue, Angular, Svelte, or ANY JavaScript framework
    - JSX syntax (no <Component />, no className, no onClick={handler})
    - import React from 'react' or any framework imports
    - useState, useEffect, useRef, or any React hooks
    - package.json with framework dependencies (react, vue, angular)
    - Component-based architecture

    **REQUIRED (YOU MUST USE):**
    - Pure HTML5 (<html>, <head>, <body>, <div>, <canvas>, etc.)
    - Pure CSS3 (no CSS-in-JS, no styled-components)
    - Pure vanilla JavaScript:
      * document.getElementById(), document.querySelector()
      * element.addEventListener()
      * Direct DOM manipulation
    - Single HTML file with embedded <style> and <script>, OR separate .html, .css, .js files

    **CORRECT FILE STRUCTURE:**
    - index.html (main HTML with structure)
    - styles.css (all CSS styles)
    - script.js (ALL application logic in vanilla JS - THIS MUST CONTAIN REAL CODE)

    **EXAMPLES OF WHAT YOU MUST GENERATE:**
    ```javascript
    // script.js - CORRECT (vanilla JS)
    const canvas = document.getElementById('gameCanvas');
    const ctx = canvas.getContext('2d');
    document.addEventListener('keydown', handleKeyDown);
    function gameLoop() { requestAnimationFrame(gameLoop); }
    ```

    **EXAMPLES OF WHAT IS FORBIDDEN:**
    ```jsx
    // FORBIDDEN - React/JSX
    import React from 'react';
    const App = () => { return <canvas ref={canvasRef} />; };
    ```

    âš ï¸ IF YOU GENERATE REACT/VUE/ANGULAR CODE, THE ENTIRE OUTPUT WILL BE REJECTED.
    âš ï¸ script.js MUST CONTAIN ACTUAL GAME/APP LOGIC, NOT COMMENTS ABOUT OTHER FILES.
"""
            base_prompt += no_framework_prompt
            logger.debug("[System Prompt] Added no-framework prohibition prompt")
        
        # ğŸ†• Phase 1å®Ÿè£…: Goalåˆ¶ç´„ã®å‹•çš„è¿½åŠ 
        logger.debug(f"[VERIFY-METHOD] Checking constraint condition...")
        logger.debug(f"[VERIFY-METHOD] goal_constraints is truthy: {bool(goal_constraints)}")
        if goal_constraints:
            logger.debug(f"[VERIFY-METHOD] constraint_strength value: {goal_constraints.get('constraint_strength')}")
            logger.debug(f"[VERIFY-METHOD] constraint_strength == 'strict': {goal_constraints.get('constraint_strength') == 'strict'}")
        
        # ğŸ†• Phase 1ãƒ«ãƒ¼ãƒ«ï¼ˆåŸºæœ¬ãƒ«ãƒ¼ãƒ«ï¼‰ã‚’å¸¸ã«é©ç”¨ï¼ˆå˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆæ™‚ã‚‚æœ‰åŠ¹åŒ–ï¼‰
        try:
            rule_parser = FileReferenceRuleParser(logger=logger)
            rule_parser.load_rules(project_root=self.workspace_path)
            # Phase 1ï¼ˆCritical/Highå„ªå…ˆåº¦ï¼‰ã®ãƒ«ãƒ¼ãƒ«ã®ã¿å–å¾—
            phase1_rules = rule_parser.get_applicable_rules(
                filepaths=['*.html', '*.css', '*.js', '*.py'],  # æ±ç”¨çš„ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—
                max_phase=1  # Phase 1ã®ã¿
            )
            if phase1_rules:
                logger.debug(f"[System Prompt] Applying {len(phase1_rules)} Phase 1 rules to system prompt")
                rules_prompt = "\n\n    ## File Reference Rules (Always Applied)\n"
                for rule in phase1_rules:
                    rules_prompt += f"\n    {rule.prompt}\n"
                base_prompt += rules_prompt
        except Exception as e:
            logger.debug(f"[System Prompt] Failed to load Phase 1 rules: {e}")
        
        if goal_constraints and goal_constraints.get("constraint_strength") == "strict":
            logger.debug(f"[VERIFY-METHOD] âœ… Constraint condition TRUE - generating constraint prompt")
            constraint_prompt = self._build_constraint_enforcement_prompt(goal_constraints)
            logger.debug(f"[VERIFY-METHOD] Constraint prompt generated, length: {len(constraint_prompt)} chars")
            logger.debug(f"[VERIFY-METHOD] Adding constraint prompt to base_prompt")
            final_prompt = base_prompt + "\n\n" + constraint_prompt
            logger.debug(f"[VERIFY-METHOD] Final system prompt length: {len(final_prompt)} chars")
            return final_prompt
        
        # åˆ¶ç´„ãªã—ã®å ´åˆã¯ãƒ™ãƒ¼ã‚¹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿”ã™
        logger.debug(f"[VERIFY-METHOD] âŒ Constraint condition FALSE - returning base_prompt only")
        logger.debug(f"[VERIFY-METHOD] Base prompt length: {len(base_prompt)} chars")
        return base_prompt
    def _build_constraint_enforcement_prompt(self, constraints: Dict) -> str:
        """
        åˆ¶ç´„å¼·åˆ¶ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆï¼ˆğŸ†• Phase 1å®Ÿè£…ï¼‰
        
        Args:
            constraints: Goalåˆ¶ç´„æƒ…å ±
        
        Returns:
            åˆ¶ç´„å¼·åˆ¶ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ–‡å­—åˆ—
        """
        enforcement = ["CRITICAL CONSTRAINTS (MUST FOLLOW):"]
        
        if constraints.get("keep_keywords"):
            enforcement.append(f"\n1. KEEP EXISTING: {', '.join(constraints['keep_keywords'])}")
            enforcement.append("   - DO NOT modify these elements")
            enforcement.append("   - DO NOT change response formats")
            enforcement.append("   - DO NOT alter existing routes/endpoints")
            enforcement.append("   - DO NOT change existing behavior")
        
        if constraints.get("only_keywords"):
            enforcement.append(f"\n2. ONLY FILES: {', '.join(constraints['only_keywords'])}")
            enforcement.append("   - ONLY create explicitly specified files")
            enforcement.append("   - DO NOT add convenience files (HTML, CSS, etc.)")
            enforcement.append("   - DO NOT add any other files not explicitly mentioned")
        
        if constraints.get("forbidden_actions"):
            enforcement.append(f"\n3. FORBIDDEN ACTIONS:")
            for action in constraints["forbidden_actions"]:
                enforcement.append(f"   - {action}")
        
        enforcement.append("\nâš ï¸ CRITICAL: Violating these constraints will result in COMPLETE REJECTION of the code.")
        enforcement.append("âš ï¸ CRITICAL: Follow these constraints EXACTLY as specified.")
        
        return "\n".join(enforcement)


    def _determine_language(self, goal: str) -> str:
        """Determine programming language from goal"""
        goal_lower = goal.lower()
        
        if "html" in goal_lower or "web page" in goal_lower:
            return "html"
        elif "javascript" in goal_lower or ".js" in goal_lower:
            return "javascript"
        elif "css" in goal_lower:
            return "css"
        elif "python" in goal_lower or ".py" in goal_lower:
            return "python"
        elif "java " in goal_lower and "javascript" not in goal_lower:
            return "java"
        elif "c++" in goal_lower or "cpp" in goal_lower:
            return "cpp"
        elif "rust" in goal_lower:
            return "rust"
        else:
            return "python"  # Default

    def _detect_multi_file_requirement(self, goal: str) -> dict:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‹ã‚‰è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ç”ŸæˆãŒå¿…è¦ã‹ã‚’æ¤œå‡º
        
        æ–°å®Ÿè£…: Detector Pipeline ã‚’ä½¿ç”¨
        
        Args:
            goal: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å®Ÿè£…ç›®æ¨™
        
        Returns:
            {
                'required': bool,
                'reason': str,
                'file_types': list,
                'confidence': str,
                'detectors': list
            }
        """
        try:
            # Detector Pipeline ã‚’ä½¿ç”¨
            detector = FileRequirementDetector(
                logger=logger,
                repository_analyzer=self.repository_analyzer
            )
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
            context = {}
            
            # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°è¿½åŠ 
            if hasattr(self, '_complexity_assessor') and \
               hasattr(self._complexity_assessor, 'existing_files'):
                context['existing_files'] = self._complexity_assessor.existing_files
            
            # æ¤œå‡ºå®Ÿè¡Œ
            aggregated_result = detector.detect(goal, context)
            
            # æ—¢å­˜å½¢å¼ã«å¤‰æ›
            return aggregated_result.to_legacy_dict()
        
        except Exception as e:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚¨ãƒ©ãƒ¼æ™‚ã¯å®‰å…¨ãªå€¤ã‚’è¿”ã™
            logger.error(f"[Multi-File Detection] Pipeline error: {e}")
            import traceback
            logger.error(f"[Multi-File Detection] Traceback: {traceback.format_exc()}")
            return {
                'required': False,
                'reason': f'Detection pipeline error: {e}',
                'file_types': [],
                'confidence': 'low',
                'detectors': []
            }

    def _build_multi_file_prompt(
        self, 
        expected_files: Optional[List[FileRequirement]] = None,
        max_phase: int = 3
    ) -> str:
        """
        è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆã®ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ï¼ˆãƒ«ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ï¼‰
        
        ãƒ«ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.cognix/file_reference_rules.mdï¼‰ã‹ã‚‰é©ç”¨å¯èƒ½ãªãƒ«ãƒ¼ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€
        ãƒ•ã‚¡ã‚¤ãƒ«å‚ç…§æ•´åˆæ€§ã«é–¢ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å‹•çš„ã«ç”Ÿæˆã™ã‚‹ã€‚
        
        Args:
            expected_files: ç”Ÿæˆäºˆå®šã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆï¼ˆFileRequirementã®ãƒªã‚¹ãƒˆï¼‰
            max_phase: é©ç”¨ã™ã‚‹æœ€å¤§ãƒ•ã‚§ãƒ¼ã‚ºï¼ˆ1: åŸºæœ¬ãƒ«ãƒ¼ãƒ«ã®ã¿, 2: æ‹¡å¼µãƒ«ãƒ¼ãƒ«ã¾ã§, 3: ã™ã¹ã¦ï¼‰
        
        Returns:
            æ§‹ç¯‰ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ–‡å­—åˆ—
        """
        if not expected_files:
            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆãŒãªã„å ´åˆã¯åŸºæœ¬çš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã¿
            return """
    IMPORTANT: This project requires modifications to multiple files.
    Use the repository map to identify ALL files that need changes.
    Return multiple files using this format:
    ## File: filename1
```
    code for file 1
```

    ## File: filename2
```
    code for file 2
```
    """
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆã‚’æŠ½å‡º
        filepaths = [f.filepath for f in expected_files]
        
        # åŸºæœ¬çš„ãªãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ
        file_list = '\n'.join([f'- {f.filepath}' for f in expected_files])
        
        prompt = f"""
    CRITICAL: You MUST generate ALL of the following files. Do NOT skip any file. Do NOT merge files into one.
    
    Required files (generate ALL):
    {file_list}
    
    IMPORTANT RULES:
    1. Generate EACH file separately using the ## File: format
    2. Do NOT create any files other than those listed above
    3. Do NOT merge multiple files into one (e.g., do NOT create a single "styles.css" instead of multiple CSS files)
    4. Each file must be complete and functional
    
    """
        
        # ãƒ«ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§é©ç”¨å¯èƒ½ãªãƒ«ãƒ¼ãƒ«ã‚’å–å¾—
        try:
            rule_parser = FileReferenceRuleParser(logger=logger)
            rule_parser.load_rules(project_root=self.workspace_path)
            applicable_rules = rule_parser.get_applicable_rules(filepaths, max_phase=max_phase)
            
            if applicable_rules:
                logger.debug(f"[Prompt Construction] ğŸ“‹ Applying {len(applicable_rules)} file reference rules")
                
                # é©ç”¨å¯èƒ½ãªãƒ«ãƒ¼ãƒ«ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ 
                for rule in applicable_rules:
                    logger.debug(f"[Prompt Construction] Applying rule: {rule.name} (Phase {rule.phase}, {rule.priority})")
                    prompt += rule.prompt + "\n\n"
            else:
                logger.debug("[Prompt Construction] No applicable file reference rules found")
        
        except Exception as e:
            logger.warning(f"[Prompt Construction] Failed to load file reference rules: {e}")
            # ãƒ«ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¦ã‚‚ã€åŸºæœ¬çš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯æä¾›
        
        # å…±é€šã®æŒ‡ç¤º
        prompt += f"""
    OUTPUT FORMAT (strictly follow):
    For EACH of the {len(expected_files)} files listed above, use this exact format:
    
    ## File: filename1
```
    code for file 1
```

    ## File: filename2
```
    code for file 2
```

    Remember: Generate ALL {len(expected_files)} files. Missing even one file is a critical error.
    """
        
        return prompt



    def _generate_fallback_code(self, goal: str) -> Dict[str, str]:
        """
        Generate basic fallback code when LLM is unavailable
        
        ä¿®æ­£ç‚¹:
        - å…¨è¨€èªå¯¾å¿œ(HTML/JavaScript/CSS/Pythonç­‰)
        - å„è¨€èªã®åŸºæœ¬ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå®Ÿè£…
        """
        goal_lower = goal.lower()
        
        # ============================================
        # HTMLå½¢å¼ã®åˆ¤å®š
        # ============================================
        if "html" in goal_lower or "web page" in goal_lower or "website" in goal_lower:
            if "clock" in goal_lower:
                filename = "digital_clock.html"
                code = '''<!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Digital Clock</title>
        <style>
            body {
                margin: 0;
                padding: 0;
                display: flex;
                justify-content: center;
                align-items: center;
                min-height: 100vh;
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            }
            
            .clock-container {
                background: rgba(255, 255, 255, 0.1);
                backdrop-filter: blur(10px);
                border-radius: 20px;
                padding: 60px 80px;
                box-shadow: 0 8px 32px 0 rgba(31, 38, 135, 0.37);
                border: 1px solid rgba(255, 255, 255, 0.18);
            }
            
            .clock {
                font-size: 80px;
                font-weight: 700;
                color: #fff;
                text-shadow: 0 0 20px rgba(255, 255, 255, 0.5),
                            0 0 40px rgba(255, 255, 255, 0.3);
                letter-spacing: 8px;
            }
            
            .date {
                font-size: 24px;
                color: rgba(255, 255, 255, 0.8);
                margin-top: 20px;
                text-align: center;
            }
        </style>
    </head>
    <body>
        <div class="clock-container">
            <div class="clock" id="clock">00:00:00</div>
            <div class="date" id="date"></div>
        </div>
        
        <script>
            function updateClock() {
                const now = new Date();
                
                // Time
                const hours = String(now.getHours()).padStart(2, '0');
                const minutes = String(now.getMinutes()).padStart(2, '0');
                const seconds = String(now.getSeconds()).padStart(2, '0');
                document.getElementById('clock').textContent = `${hours}:${minutes}:${seconds}`;
                
                // Date
                const options = { weekday: 'long', year: 'numeric', month: 'long', day: 'numeric' };
                const dateStr = now.toLocaleDateString('en-US', options);
                document.getElementById('date').textContent = dateStr;
            }
            
            updateClock();
            setInterval(updateClock, 1000);
        </script>
    </body>
    </html>'''
            
            elif "calculator" in goal_lower:
                filename = "calculator.html"
                code = '''<!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Calculator</title>
        <style>
            body {
                margin: 0;
                padding: 0;
                display: flex;
                justify-content: center;
                align-items: center;
                min-height: 100vh;
                background: #f0f0f0;
                font-family: Arial, sans-serif;
            }
            
            .calculator {
                background: #fff;
                border-radius: 10px;
                box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
                padding: 20px;
                width: 300px;
            }
            
            .display {
                background: #222;
                color: #fff;
                font-size: 32px;
                padding: 20px;
                text-align: right;
                border-radius: 5px;
                margin-bottom: 20px;
                min-height: 50px;
            }
            
            .buttons {
                display: grid;
                grid-template-columns: repeat(4, 1fr);
                gap: 10px;
            }
            
            button {
                padding: 20px;
                font-size: 18px;
                border: none;
                border-radius: 5px;
                cursor: pointer;
                background: #f5f5f5;
                transition: background 0.2s;
            }
            
            button:hover {
                background: #e0e0e0;
            }
            
            button.operator {
                background: #ff9500;
                color: white;
            }
            
            button.operator:hover {
                background: #e08500;
            }
        </style>
    </head>
    <body>
        <div class="calculator">
            <div class="display" id="display">0</div>
            <div class="buttons">
                <button onclick="clearDisplay()">C</button>
                <button onclick="appendToDisplay('/')" class="operator">/</button>
                <button onclick="appendToDisplay('*')" class="operator">*</button>
                <button onclick="deleteLast()">â†</button>
                
                <button onclick="appendToDisplay('7')">7</button>
                <button onclick="appendToDisplay('8')">8</button>
                <button onclick="appendToDisplay('9')">9</button>
                <button onclick="appendToDisplay('-')" class="operator">-</button>
                
                <button onclick="appendToDisplay('4')">4</button>
                <button onclick="appendToDisplay('5')">5</button>
                <button onclick="appendToDisplay('6')">6</button>
                <button onclick="appendToDisplay('+')" class="operator">+</button>
                
                <button onclick="appendToDisplay('1')">1</button>
                <button onclick="appendToDisplay('2')">2</button>
                <button onclick="appendToDisplay('3')">3</button>
                <button onclick="calculate()" class="operator" style="grid-row: span 2">=</button>
                
                <button onclick="appendToDisplay('0')" style="grid-column: span 2">0</button>
                <button onclick="appendToDisplay('.')">.</button>
            </div>
        </div>
        
        <script>
            let currentDisplay = '0';
            
            function updateDisplay() {
                document.getElementById('display').textContent = currentDisplay;
            }
            
            function appendToDisplay(value) {
                if (currentDisplay === '0') {
                    currentDisplay = value;
                } else {
                    currentDisplay += value;
                }
                updateDisplay();
            }
            
            function clearDisplay() {
                currentDisplay = '0';
                updateDisplay();
            }
            
            function deleteLast() {
                currentDisplay = currentDisplay.slice(0, -1) || '0';
                updateDisplay();
            }
            
            function calculate() {
                try {
                    currentDisplay = String(eval(currentDisplay));
                } catch (e) {
                    currentDisplay = 'Error';
                }
                updateDisplay();
            }
        </script>
    </body>
    </html>'''
            
            else:
                # æ±ç”¨HTML
                filename = "index.html"
                code = f'''<!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>{goal.title()}</title>
        <style>
            body {{
                font-family: Arial, sans-serif;
                margin: 40px;
                background: #f5f5f5;
            }}
            .container {{
                max-width: 800px;
                margin: 0 auto;
                background: white;
                padding: 40px;
                border-radius: 8px;
                box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            }}
            h1 {{
                color: #333;
            }}
        </style>
    </head>
    <body>
        <div class="container">
            <h1>{goal}</h1>
            <p>Implementation generated by Cognix Semi-Auto Engine</p>
        </div>
    </body>
    </html>'''
            
            return {filename: code}
        
        # ============================================
        # JavaScriptå½¢å¼ã®åˆ¤å®š
        # ============================================
        elif "javascript" in goal_lower or "js " in goal_lower or ".js" in goal_lower:
            filename = "script.js"
            
            if "game" in goal_lower:
                code = '''/**
    * Simple Game Implementation
    * Generated by Cognix Semi-Auto Engine
    */

    class Game {
        constructor() {
            this.score = 0;
            this.isRunning = false;
        }
        
        start() {
            console.log('Game starting...');
            this.isRunning = true;
            this.score = 0;
            this.gameLoop();
        }
        
        gameLoop() {
            if (!this.isRunning) return;
            
            // Game logic here
            this.update();
            this.render();
            
            requestAnimationFrame(() => this.gameLoop());
        }
        
        update() {
            // Update game state
            this.score++;
        }
        
        render() {
            // Render game
            console.log(`Score: ${this.score}`);
        }
        
        stop() {
            console.log('Game stopped');
            this.isRunning = false;
        }
    }

    // Initialize and start game
    const game = new Game();
    game.start();

    // Stop after 5 seconds (example)
    setTimeout(() => game.stop(), 5000);
    '''
            else:
                # æ±ç”¨JavaScript
                code = f'''/**
    * {goal}
    * Generated by Cognix Semi-Auto Engine
    */

    class Application {{
        constructor() {{
            this.data = [];
            this.init();
        }}
        
        init() {{
            console.log('Initializing application...');
            this.setupEventListeners();
        }}
        
        setupEventListeners() {{
            // Add event listeners here
            document.addEventListener('DOMContentLoaded', () => {{
                this.run();
            }});
        }}
        
        run() {{
            console.log('Application running');
            // Main application logic here
        }}
        
        handleError(error) {{
            console.error('Error:', error);
        }}
    }}

    // Initialize application
    try {{
        const app = new Application();
    }} catch (error) {{
        console.error('Failed to initialize:', error);
    }}
    '''
            
            return {filename: code}
        
        # ============================================
        # CSSå½¢å¼ã®åˆ¤å®š
        # ============================================
        elif "css" in goal_lower or "stylesheet" in goal_lower:
            filename = "styles.css"
            code = f'''/**
    * {goal}
    * Generated by Cognix Semi-Auto Engine
    */

    /* ==========================================
    Reset and Base Styles
    ========================================== */

    * {{
        margin: 0;
        padding: 0;
        box-sizing: border-box;
    }}

    html {{
        font-size: 16px;
        scroll-behavior: smooth;
    }}

    body {{
        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        line-height: 1.6;
        color: #333;
        background: #f5f5f5;
    }}

    /* ==========================================
    Layout
    ========================================== */

    .container {{
        max-width: 1200px;
        margin: 0 auto;
        padding: 20px;
    }}

    /* ==========================================
    Typography
    ========================================== */

    h1, h2, h3, h4, h5, h6 {{
        margin-bottom: 1rem;
        font-weight: 600;
    }}

    h1 {{ font-size: 2.5rem; }}
    h2 {{ font-size: 2rem; }}
    h3 {{ font-size: 1.75rem; }}

    p {{
        margin-bottom: 1rem;
    }}

    /* ==========================================
    Utilities
    ========================================== */

    .text-center {{ text-align: center; }}
    .text-right {{ text-align: right; }}
    .mt-1 {{ margin-top: 0.5rem; }}
    .mt-2 {{ margin-top: 1rem; }}
    .mb-1 {{ margin-bottom: 0.5rem; }}
    .mb-2 {{ margin-bottom: 1rem; }}

    /* ==========================================
    Responsive Design
    ========================================== */

    @media (max-width: 768px) {{
        .container {{
            padding: 10px;
        }}
        
        h1 {{ font-size: 2rem; }}
        h2 {{ font-size: 1.5rem; }}
    }}
    '''
            
            return {filename: code}
        
        # ============================================
        # Pythonå½¢å¼ã®åˆ¤å®š
        # ============================================
        elif "calculator" in goal_lower and "python" in goal_lower:
            filename = "calculator.py"
            code = '''#!/usr/bin/env python3
    """
    Simple Calculator
    Generated by Cognix Semi-Auto Engine
    """

    class Calculator:
        """Basic calculator with arithmetic operations"""
        
        def add(self, a: float, b: float) -> float:
            """Add two numbers"""
            return a + b
        
        def subtract(self, a: float, b: float) -> float:
            """Subtract b from a"""
            return a - b
        
        def multiply(self, a: float, b: float) -> float:
            """Multiply two numbers"""
            return a * b
        
        def divide(self, a: float, b: float) -> float:
            """Divide a by b"""
            if b == 0:
                raise ValueError("Cannot divide by zero")
            return a / b


    def main():
        """Main function"""
        calc = Calculator()
        
        err_console.print("Simple Calculator")
        err_console.print("-" * 40)
        
        try:
            a = float(input("Enter first number: "))
            op = input("Enter operation (+, -, *, /): ")
            b = float(input("Enter second number: "))
            
            if op == '+':
                result = calc.add(a, b)
            elif op == '-':
                result = calc.subtract(a, b)
            elif op == '*':
                result = calc.multiply(a, b)
            elif op == '/':
                result = calc.divide(a, b)
            else:
                err_console.print(f"Unknown operation: {op}")
                return
            
            err_console.print(f"Result: {a} {op} {b} = {result}")
            
        except ValueError as e:
            err_console.print(f"Input error: {e}")
        except Exception as e:
            err_console.print(f"Calculation error: {e}")


    if __name__ == "__main__":
        main()
    '''
            
            return {filename: code}
        
        # ============================================
        # Javaå½¢å¼ã®åˆ¤å®š
        # ============================================
        elif "java " in goal_lower and "javascript" not in goal_lower:
            filename = "Main.java"
            code = f'''/**
    * {goal}
    * Generated by Cognix Semi-Auto Engine
    */

    public class Main {{
        public static void main(String[] args) {{
            System.out.println("Application starting...");
            
            try {{
                // Main application logic here
                run();
            }} catch (Exception e) {{
                System.err.println("Error: " + e.getMessage());
                e.printStackTrace();
            }}
        }}
        
        private static void run() {{
            // Implementation for: {goal}
            System.out.println("Running application");
        }}
    }}
    '''
            
            return {filename: code}
        
        # ============================================
        # C++å½¢å¼ã®åˆ¤å®š
        # ============================================
        elif "c++" in goal_lower or "cpp" in goal_lower:
            filename = "main.cpp"
            code = f'''/**
    * {goal}
    * Generated by Cognix Semi-Auto Engine
    */

    #include <iostream>
    #include <string>

    using namespace std;

    class Application {{
    public:
        Application() {{
            cout << "Application initialized" << endl;
        }}
        
        void run() {{
            cout << "Running application..." << endl;
            // Implementation for: {goal}
        }}
        
        ~Application() {{
            cout << "Application terminated" << endl;
        }}
    }};

    int main() {{
        try {{
            Application app;
            app.run();
            return 0;
        }} catch (const exception& e) {{
            cerr << "Error: " << e.what() << endl;
            return 1;
        }}
    }}
    '''
            
            return {filename: code}
        
        # ============================================
        # Rustå½¢å¼ã®åˆ¤å®š
        # ============================================
        elif "rust" in goal_lower:
            filename = "main.rs"
            code = f'''/**
    * {goal}
    * Generated by Cognix Semi-Auto Engine
    */

    fn main() {{
        println!("Application starting...");
        
        match run() {{
            Ok(_) => println!("Application completed successfully"),
            Err(e) => eprintln!("Error: {{}}", e),
        }}
    }}

    fn run() -> Result<(), Box<dyn std::error::Error>> {{
        // Implementation for: {goal}
        println!("Running application");
        Ok(())
    }}
    '''
            
            return {filename: code}
        
        # ============================================
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ(Python)
        # ============================================
        else:
            filename = "main.py"
            code = f'''#!/usr/bin/env python3
    """
    {goal.title()} Implementation
    Generated by Cognix Semi-Auto Engine
    """


    def main():
        """Main function for {goal}"""
        err_console.print("Implementation for: {goal}")
        
        try:
            # Add your implementation here
            pass
            
        except Exception as e:
            err_console.print(f"Error: {{e}}")


    if __name__ == "__main__":
        main()
    '''
            
            return {filename: code}

    def _calculate_lint_error_score(
        self,
        file_errors: List[Dict],
        file_warnings: List[Dict]
    ) -> float:
        """
        Lintã‚¨ãƒ©ãƒ¼ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆã‚¨ãƒ©ãƒ¼ç¨®é¡åˆ¥ã®é‡ã¿ä»˜ã‘ï¼‰
        
        ã‚¨ãƒ©ãƒ¼åˆ†é¡:
        - Critical (E999): æ§‹æ–‡ã‚¨ãƒ©ãƒ¼
        - High (E9ç³»): ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã€æ–‡æ³•ã‚¨ãƒ©ãƒ¼
        - Medium (F4ç³», F8ç³»): æœªä½¿ç”¨å¤‰æ•°ã€importé †åº
        - Low (E5ç³»ç­‰): ã‚¹ã‚¿ã‚¤ãƒ«é•åï¼ˆè¡ŒãŒé•·ã„ç­‰ï¼‰
        - Environment (F821ã®importé–¢é€£): ç’°å¢ƒä¾å­˜ã‚¨ãƒ©ãƒ¼ï¼ˆæ¸›ç‚¹ãªã—ï¼‰
        
        Args:
            file_errors: ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ©ãƒ¼ãƒªã‚¹ãƒˆ
            file_warnings: ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®è­¦å‘Šãƒªã‚¹ãƒˆ
        
        Returns:
            0.0-1.0ã®ã‚¹ã‚³ã‚¢ï¼ˆ1.0ãŒå®Œç’§ï¼‰
        """
        # ã‚¨ãƒ©ãƒ¼åˆ†é¡å®šç¾©
        CRITICAL_CODES = ['E999']  # æ§‹æ–‡ã‚¨ãƒ©ãƒ¼
        HIGH_CODES = ['E9']  # E9ã§å§‹ã¾ã‚‹ï¼ˆã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆç­‰ï¼‰
        MEDIUM_CODES = ['F4', 'F8', 'F401', 'F841']  # æœªä½¿ç”¨import/å¤‰æ•°
        ENV_CODES = ['F821', 'F823']  # undefined nameï¼ˆç’°å¢ƒä¾å­˜ã®å¯èƒ½æ€§ï¼‰
        # ãã‚Œä»¥å¤–ï¼ˆE501, E701ç­‰ï¼‰ã¯LOW
        
        critical_count = 0
        high_count = 0
        medium_count = 0
        low_count = 0
        env_count = 0
        
        for error in file_errors:
            code = error.get('code', '')
            message = error.get('message', '').lower()
            
            # ç’°å¢ƒä¾å­˜ã‚¨ãƒ©ãƒ¼ã®åˆ¤å®š
            if any(code.startswith(ec) for ec in ENV_CODES):
                # F821/F823ã§importé–¢é€£ã®ã‚¨ãƒ©ãƒ¼ã¯ç’°å¢ƒå•é¡Œ
                if 'import' in message or 'module' in message:
                    env_count += 1
                    continue
            
            # ã‚¨ãƒ©ãƒ¼åˆ†é¡
            if any(code.startswith(cc) for cc in CRITICAL_CODES):
                critical_count += 1
            elif any(code.startswith(hc) for hc in HIGH_CODES):
                high_count += 1
            elif any(code.startswith(mc) for mc in MEDIUM_CODES):
                medium_count += 1
            else:
                low_count += 1
        
        # è­¦å‘Šã¯è»½å¾®
        warning_count = len(file_warnings)
        
        # ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆé‡ã¿ä»˜ã‘ï¼‰
        score = 1.0
        score -= critical_count * 0.15   # Critical: -15% / å€‹ï¼ˆæ§‹æ–‡ã‚¨ãƒ©ãƒ¼ï¼‰
        score -= high_count * 0.10       # High: -10% / å€‹ï¼ˆã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆç­‰ï¼‰
        score -= medium_count * 0.05     # Medium: -5% / å€‹ï¼ˆæœªä½¿ç”¨å¤‰æ•°ï¼‰
        score -= low_count * 0.02        # Low: -2% / å€‹ï¼ˆã‚¹ã‚¿ã‚¤ãƒ«é•åï¼‰
        score -= warning_count * 0.01    # Warning: -1% / å€‹
        # ç’°å¢ƒä¾å­˜ã‚¨ãƒ©ãƒ¼ï¼ˆenv_countï¼‰ã¯æ¸›ç‚¹ã—ãªã„
        
        # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°å‡ºåŠ›
        if critical_count + high_count + medium_count + low_count + env_count > 0:
            logger.debug(
                f"[Quality] Lint errors breakdown: Critical={critical_count}, High={high_count}, "
                f"Medium={medium_count}, Low={low_count}, Env={env_count}, Warnings={warning_count}"
            )
        
        return max(0.0, score)

    def _get_error_breakdown(
        self,
        errors: List[Dict],
        warnings: List[Dict]
    ) -> Dict[str, int]:
        """
        Lintã‚¨ãƒ©ãƒ¼ã‚’ç¨®é¡åˆ¥ã«åˆ†é¡ã—ã¦ã‚«ã‚¦ãƒ³ãƒˆï¼ˆè¡¨ç¤ºç”¨ï¼‰
        
        Args:
            errors: ã‚¨ãƒ©ãƒ¼ãƒªã‚¹ãƒˆï¼ˆå…¨ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
            warnings: è­¦å‘Šãƒªã‚¹ãƒˆï¼ˆå…¨ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
        
        Returns:
            å„ã‚«ãƒ†ã‚´ãƒªã®ã‚¨ãƒ©ãƒ¼æ•°ã®è¾æ›¸
            {
                'critical': int,
                'high': int,
                'medium': int,
                'low': int,
                'env': int,
                'warnings': int
            }
        """
        # ã‚¨ãƒ©ãƒ¼åˆ†é¡å®šç¾©ï¼ˆ_calculate_lint_error_scoreã¨åŒã˜ï¼‰
        CRITICAL_CODES = ['E999']  # æ§‹æ–‡ã‚¨ãƒ©ãƒ¼
        HIGH_CODES = ['E9']  # E9ã§å§‹ã¾ã‚‹ï¼ˆã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆç­‰ï¼‰
        MEDIUM_CODES = ['F4', 'F8', 'F401', 'F841']  # æœªä½¿ç”¨import/å¤‰æ•°
        ENV_CODES = ['F821', 'F823']  # undefined nameï¼ˆç’°å¢ƒä¾å­˜ã®å¯èƒ½æ€§ï¼‰
        # ãã‚Œä»¥å¤–ï¼ˆE501, E701ç­‰ï¼‰ã¯LOW
        
        critical_count = 0
        high_count = 0
        medium_count = 0
        low_count = 0
        env_count = 0
        
        for error in errors:
            code = error.get('code', '')
            message = error.get('message', '').lower()
            
            # ç’°å¢ƒä¾å­˜ã‚¨ãƒ©ãƒ¼ã®åˆ¤å®š
            if any(code.startswith(ec) for ec in ENV_CODES):
                # F821/F823ã§importé–¢é€£ã®ã‚¨ãƒ©ãƒ¼ã¯ç’°å¢ƒå•é¡Œ
                if 'import' in message or 'module' in message:
                    env_count += 1
                    continue
            
            # ã‚¨ãƒ©ãƒ¼åˆ†é¡
            if any(code.startswith(cc) for cc in CRITICAL_CODES):
                critical_count += 1
            elif any(code.startswith(hc) for hc in HIGH_CODES):
                high_count += 1
            elif any(code.startswith(mc) for mc in MEDIUM_CODES):
                medium_count += 1
            else:
                low_count += 1
        
        return {
            'critical': critical_count,
            'high': high_count,
            'medium': medium_count,
            'low': low_count,
            'env': env_count,
            'warnings': len(warnings)
        }

    def _detect_placeholders_in_comments(
        self,
        code: str,
        ext: str
    ) -> tuple[int, list[str]]:
        """
        ã‚³ãƒ¡ãƒ³ãƒˆå†…ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã®ã¿ã‚’æ¤œå‡º
        
        æ­£å½“ãªã‚³ãƒ¼ãƒ‰ï¼ˆHTMLå±æ€§ã®placeholderã€CSSç–‘ä¼¼è¦ç´ ::placeholderç­‰ï¼‰ã¯é™¤å¤–
        
        Args:
            code: ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰
            ext: ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ï¼ˆ'py', 'js', 'html', 'css'ç­‰ï¼‰
        
        Returns:
            (ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼æ•°, æ¤œå‡ºã•ã‚ŒãŸè¡Œã®ãƒªã‚¹ãƒˆ)
        """
        import re
        
        placeholder_keywords = [
            r'TODO:',
            r'FIXME:',
            r'XXX:',
            r'\bplaceholder\b',  # å˜èªã¨ã—ã¦
            r'not yet implemented',
            r'coming soon',
            r'implement this',
            r'add.*here when.*is implemented',
        ]
        
        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ­£å½“ãªä½¿ç”¨æ³•ï¼‰
        exclude_patterns = [
            # === é–¢æ•°/ã‚³ãƒ¼ãƒ‰å®šç¾©ãƒ‘ã‚¿ãƒ¼ãƒ³ ===
            r'def\s+\w*placeholder\w*',     # é–¢æ•°å®šç¾©: def generate_placeholder_url
            r'function\s+\w*placeholder\w*', # JSé–¢æ•°å®šç¾©: function generatePlaceholder
            r'\w*placeholder\w*\s*\(',       # é–¢æ•°å‘¼ã³å‡ºã—: generate_placeholder_url(
            
            # === ã‚¢ã‚»ãƒƒãƒˆ/ãƒªã‚½ãƒ¼ã‚¹ç”Ÿæˆãƒ‘ã‚¿ãƒ¼ãƒ³ ===
            r'placeholder.*url',             # placeholder URLç”Ÿæˆ
            r'placeholder.*image',           # placeholder imageç”Ÿæˆ
            r'generate.*placeholder',        # generate_placeholderç³»
            
            # === è¨­è¨ˆèª¬æ˜ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆèª¤æ¤œçŸ¥é˜²æ­¢ï¼‰===
            r'placeholder\s+for\s+\w+',      # "placeholder for sprite", "placeholder for image"
            r'as\s+.*placeholder',           # "as a placeholder", "as simple placeholder"
            r'\(placeholder\s+for',          # "(placeholder for ...)"
            r'placeholder\s+until',          # "placeholder until assets are ready"
            r'simple\s+.*placeholder',       # "simple rectangle placeholder"
            r'temporary\s+placeholder',      # "temporary placeholder"ï¼ˆè¨­è¨ˆä¸Šã®èª¬æ˜ï¼‰
            r'serve\s+as\s+.*placeholder',   # "serve as a placeholder"
            r'act\s+as\s+.*placeholder',     # "act as a placeholder"
            r'used\s+as\s+.*placeholder',    # "used as a placeholder"
            
            # === ææ¡ˆ/å¯èƒ½æ€§ã‚³ãƒ¡ãƒ³ãƒˆ ===
            r'use.*placeholder',             # use placeholderï¼ˆèª¬æ˜ã‚³ãƒ¡ãƒ³ãƒˆï¼‰
            r'add\s+a\s+placeholder',        # add a placeholderï¼ˆææ¡ˆã‚³ãƒ¡ãƒ³ãƒˆï¼‰
            r'could\s+add.*placeholder',     # could add a placeholderï¼ˆææ¡ˆã‚³ãƒ¡ãƒ³ãƒˆï¼‰
            r'may\s+add.*placeholder',       # may add a placeholderï¼ˆææ¡ˆã‚³ãƒ¡ãƒ³ãƒˆï¼‰
            r'might\s+add.*placeholder',     # might add a placeholderï¼ˆææ¡ˆã‚³ãƒ¡ãƒ³ãƒˆï¼‰
        ]
        
        detected_lines = []
        placeholder_count = 0
        
        # è¨€èªåˆ¥ã®ã‚³ãƒ¡ãƒ³ãƒˆæŠ½å‡º
        if ext == 'py':
            # Pythonã®ã‚³ãƒ¡ãƒ³ãƒˆ: # ã‹ã‚‰è¡Œæœ«ã¾ã§
            comment_pattern = r'#.*$'
            for match in re.finditer(comment_pattern, code, re.MULTILINE | re.IGNORECASE):
                comment_text = match.group(0)
                for keyword in placeholder_keywords:
                    if re.search(keyword, comment_text, re.IGNORECASE):
                        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
                        is_excluded = False
                        for exclude in exclude_patterns:
                            if re.search(exclude, comment_text, re.IGNORECASE):
                                is_excluded = True
                                break
                        
                        if not is_excluded:
                            placeholder_count += 1
                            detected_lines.append(comment_text.strip())
                            break  # åŒã˜ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã§è¤‡æ•°ã‚«ã‚¦ãƒ³ãƒˆã—ãªã„
        
        elif ext in ['js', 'jsx', 'ts', 'tsx']:
            # JavaScript/TypeScript
            # 1. å˜è¡Œã‚³ãƒ¡ãƒ³ãƒˆ: // ã‹ã‚‰è¡Œæœ«ã¾ã§
            single_line_pattern = r'//.*$'
            for match in re.finditer(single_line_pattern, code, re.MULTILINE | re.IGNORECASE):
                comment_text = match.group(0)
                for keyword in placeholder_keywords:
                    if re.search(keyword, comment_text, re.IGNORECASE):
                        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
                        is_excluded = False
                        for exclude in exclude_patterns:
                            if re.search(exclude, comment_text, re.IGNORECASE):
                                is_excluded = True
                                break
                        
                        if not is_excluded:
                            placeholder_count += 1
                            detected_lines.append(comment_text.strip())
                            break
            
            # 2. è¤‡æ•°è¡Œã‚³ãƒ¡ãƒ³ãƒˆ: /* ... */
            multi_line_pattern = r'/\*.*?\*/'
            for match in re.finditer(multi_line_pattern, code, re.DOTALL | re.IGNORECASE):
                comment_text = match.group(0)
                for keyword in placeholder_keywords:
                    if re.search(keyword, comment_text, re.IGNORECASE):
                        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
                        is_excluded = False
                        for exclude in exclude_patterns:
                            if re.search(exclude, comment_text, re.IGNORECASE):
                                is_excluded = True
                                break
                        
                        if not is_excluded:
                            placeholder_count += 1
                            # è¤‡æ•°è¡Œã®å ´åˆã¯æœ€åˆã®è¡Œã®ã¿è¡¨ç¤º
                            first_line = comment_text.split('\n')[0].strip()
                            detected_lines.append(first_line + '...')
                            break
        
        elif ext in ['html', 'htm', 'xml']:
            # HTML/XMLã‚³ãƒ¡ãƒ³ãƒˆ: <!-- ... -->
            comment_pattern = r'<!--.*?-->'
            for match in re.finditer(comment_pattern, code, re.DOTALL | re.IGNORECASE):
                comment_text = match.group(0)
                for keyword in placeholder_keywords:
                    if re.search(keyword, comment_text, re.IGNORECASE):
                        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
                        is_excluded = False
                        for exclude in exclude_patterns:
                            if re.search(exclude, comment_text, re.IGNORECASE):
                                is_excluded = True
                                break
                        
                        if not is_excluded:
                            placeholder_count += 1
                            # è¤‡æ•°è¡Œã®å ´åˆã¯æœ€åˆã®è¡Œã®ã¿è¡¨ç¤º
                            first_line = comment_text.split('\n')[0].strip()
                            detected_lines.append(first_line + '...' if '\n' in comment_text else comment_text.strip())
                            break
        
        elif ext == 'css':
            # CSSã‚³ãƒ¡ãƒ³ãƒˆ: /* ... */
            comment_pattern = r'/\*.*?\*/'
            for match in re.finditer(comment_pattern, code, re.DOTALL | re.IGNORECASE):
                comment_text = match.group(0)
                for keyword in placeholder_keywords:
                    if re.search(keyword, comment_text, re.IGNORECASE):
                        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
                        is_excluded = False
                        for exclude in exclude_patterns:
                            if re.search(exclude, comment_text, re.IGNORECASE):
                                is_excluded = True
                                break
                        
                        if not is_excluded:
                            placeholder_count += 1
                            first_line = comment_text.split('\n')[0].strip()
                            detected_lines.append(first_line + '...' if '\n' in comment_text else comment_text.strip())
                            break
        
        return placeholder_count, detected_lines

    def _assess_code_quality(
        self, 
        generated_code: Dict[str, str], 
        complexity: str = "medium", 
        goal: str = "",
        lint_result: Optional[Dict[str, Any]] = None
    ) -> Dict[str, float]:
        """
        ã‚³ãƒ¼ãƒ‰å“è³ªã‚’è©•ä¾¡(RequirementValidatorçµ±åˆç‰ˆ)
        
        æ”¹å–„ç‚¹:
        - ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚«ãƒ¼ã¨è¦ä»¶æ¤œè¨¼å™¨ã‚’åˆæœŸåŒ–
        - goalãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¿½åŠ (è¦ä»¶æ¤œè¨¼ç”¨ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
        - ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½ã‚’çµ±åˆ
        - è¦ä»¶å……è¶³åº¦æ¤œè¨¼ã‚’çµ±åˆ
        - å®Ÿè¡Œå¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯ã‚’è¿½åŠ 
        - è¤‡é›‘åº¦ã‚’è€ƒæ…®ã—ãŸè©•ä¾¡åŸºæº–ã¯æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç¶­æŒ
        
        Args:
            generated_code: ãƒ•ã‚¡ã‚¤ãƒ«åã¨ã‚³ãƒ¼ãƒ‰ã®è¾æ›¸
            complexity: "simple" | "medium" | "complex"
            goal: å®Ÿè£…ç›®æ¨™(è¦ä»¶æ¤œè¨¼ç”¨ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
            
        Returns:
            ãƒ•ã‚¡ã‚¤ãƒ«åã¨å“è³ªã‚¹ã‚³ã‚¢ã®è¾æ›¸
        """
        # ==========================================
        # ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚«ãƒ¼ã¨è¦ä»¶æ¤œè¨¼å™¨ã‚’åˆæœŸåŒ–
        # ==========================================
        use_advanced_checks = False
        dep_checker = None
        req_validator = None
        
        try:
            from cognix.dependency_checker import DependencyChecker
            from cognix.requirement_validator import RequirementValidator
            dep_checker = DependencyChecker()
            req_validator = RequirementValidator()
            use_advanced_checks = True
        except ImportError:
            dep_checker = None
            req_validator = None
            use_advanced_checks = False
        except Exception as e:
            logger.debug(f"âš  Advanced checks unavailable: {e}")
            dep_checker = None
            req_validator = None
            use_advanced_checks = False
        
        quality_scores = {}
        all_issues = []
        
        # ==========================================
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«è©•ä¾¡: è¦ä»¶å……è¶³åº¦
        # ==========================================
        requirement_score = 1.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        
        if use_advanced_checks and req_validator and goal:
            try:
                requirement_result = req_validator.validate_requirements(goal, generated_code)
                requirement_score = requirement_result['fulfillment_score']
                
                if not requirement_result['is_complete']:
                    all_issues.extend(requirement_result['issues'])
                    logger.debug("[Quality] Requirement Validation Issues:")
                    for issue in requirement_result['issues']:
                        logger.debug(f"[Quality]   {issue}")
            except Exception as e:
                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨
                logger.debug(f"[Quality] Requirement validation error: {e}")
        
        # ==========================================
        # ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥è©•ä¾¡
        # ==========================================
        # ==========================================
        # ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥è©•ä¾¡
        # ==========================================
        for filename, code in generated_code.items():
            ext = filename.split('.')[-1].lower()
            
            # åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            lines_code = code.split('\n')
            non_empty_lines = [l for l in lines_code if l.strip()]
            
            # ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã®ã‚«ã‚¦ãƒ³ãƒˆï¼ˆè¤‡æ•°è¨€èªå¯¾å¿œï¼‰
            comment_lines = []
            for line in lines_code:
                stripped = line.strip()
                if ext in ['py']:
                    if stripped.startswith('#'):
                        comment_lines.append(line)
                elif ext in ['js', 'ts', 'jsx', 'tsx']:
                    if stripped.startswith('//') or stripped.startswith('/*') or stripped.startswith('*'):
                        comment_lines.append(line)
                elif ext in ['html', 'xml']:
                    if '<!--' in stripped:
                        comment_lines.append(line)
                elif ext in ['css']:
                    if stripped.startswith('/*'):
                        comment_lines.append(line)
            
            # ğŸš¨ é‡è¦: ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼æ¤œå‡ºï¼ˆã‚³ãƒ¡ãƒ³ãƒˆå†…ã®ã¿ï¼‰
            placeholder_score = 1.0
            placeholder_count, detected_lines = self._detect_placeholders_in_comments(code, ext)
            
            if placeholder_count > 0:
                # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ãŒã‚ã‚‹ã¨å¤§å¹…æ¸›ç‚¹
                placeholder_score = max(0.1, 1.0 - (placeholder_count * 0.3))
                all_issues.append(f"{filename}: {placeholder_count} placeholder(s) detected")
                logger.debug(f"[Quality] {filename}: Found {placeholder_count} placeholders in comments, score={placeholder_score}")
                # æ¤œå‡ºã•ã‚ŒãŸã‚³ãƒ¡ãƒ³ãƒˆè¡Œã‚’è©³ç´°ãƒ­ã‚°ã«å‡ºåŠ›
                for i, line in enumerate(detected_lines[:3], 1):  # æœ€å¤§3ä»¶
                    logger.debug(f"[Quality]   Placeholder {i}: {line}")
                if len(detected_lines) > 3:
                    logger.debug(f"[Quality]   ... and {len(detected_lines) - 3} more")
            
            # ğŸš¨ é‡è¦: ç©ºãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º
            if len(non_empty_lines) == 0:
                placeholder_score = 0.0
                all_issues.append(f"{filename}: Empty file")
                logger.debug(f"[Quality] {filename}: Empty file detected")
            
            # ğŸš¨ é‡è¦: ã‚³ãƒ¡ãƒ³ãƒˆæ¯”ç‡ãƒã‚§ãƒƒã‚¯
            comment_ratio_score = 1.0
            if len(non_empty_lines) > 0:
                comment_ratio = len(comment_lines) / len(non_empty_lines)
                if comment_ratio > 0.7:
                    # ã‚³ãƒ¡ãƒ³ãƒˆãŒ70%ä»¥ä¸Šã¯ãƒšãƒŠãƒ«ãƒ†ã‚£
                    comment_ratio_score = max(0.3, 1.0 - comment_ratio)
                    all_issues.append(f"{filename}: Too many comments ({int(comment_ratio*100)}%)")
                    logger.debug(f"[Quality] {filename}: Comment ratio {int(comment_ratio*100)}%, score={comment_ratio_score}")
            
            # é•·ã•ãƒã‚§ãƒƒã‚¯ã¨ã‚³ãƒ¡ãƒ³ãƒˆç‡ã¯å‰Šé™¤ï¼ˆQualityã¨ç›´æ¥é–¢ä¿‚ãªã—ï¼‰
            # ä»£ã‚ã‚Šã«Lintã‚¨ãƒ©ãƒ¼ã¨Complexityã‚’é‡è¦–

            
            # æ§‹é€ ãƒã‚§ãƒƒã‚¯(Pythonã®å ´åˆ)
            structure_score = 1.0
            if ext == 'py':
                has_main = 'def ' in code or 'class ' in code
                has_imports = 'import ' in code
                structure_score = 0.5 + (0.25 if has_main else 0) + (0.25 if has_imports else 0)
            
            # ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯(æ–°è¦è¿½åŠ )
            dependency_score = 1.0
            if use_advanced_checks and dep_checker and ext == 'py':
                try:
                    dep_result = dep_checker.check_code_dependencies(code)
                    if dep_result['has_missing']:
                        missing_count = len(dep_result['missing_dependencies'])
                        dependency_score = max(0.5, 1.0 - (missing_count * 0.1))
                        all_issues.append(f"{filename}: {missing_count} missing dependencies")
                except Exception:
                    pass
            
            # Lintã‚¨ãƒ©ãƒ¼ã‚¹ã‚³ã‚¢ï¼ˆã‚¨ãƒ©ãƒ¼ç¨®é¡åˆ¥ã®é‡ã¿ä»˜ã‘ï¼‰
            lint_error_score = 1.0
            if lint_result and lint_result.get('errors'):
                # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ©ãƒ¼ã¨è­¦å‘Šã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿
                file_errors = [e for e in lint_result['errors'] if e['file'] == filename]
                file_warnings = [w for w in lint_result.get('warnings', []) if w['file'] == filename]
                
                # ã‚¨ãƒ©ãƒ¼ç¨®é¡åˆ¥ã®ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆæ–°è¦ãƒ¡ã‚½ãƒƒãƒ‰ï¼‰
                lint_error_score = self._calculate_lint_error_score(file_errors, file_warnings)
                
                if len(file_errors) > 0 or len(file_warnings) > 0:
                    all_issues.append(f"{filename}: {len(file_errors)} errors, {len(file_warnings)} warnings")
            
            # å®Ÿè¡Œå¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯(æ–°è¦è¿½åŠ )
            executable_score = 1.0
            if ext == 'py':
                try:
                    import ast
                    ast.parse(code)
                    executable_score = 1.0
                except SyntaxError:
                    executable_score = 0.3
                    all_issues.append(f"{filename}: Syntax error detected")
            
            # ã‚³ãƒ¼ãƒ‰è¤‡é›‘åº¦ã‚¹ã‚³ã‚¢ï¼ˆæ–°è¦è¿½åŠ  - McCabeè¤‡é›‘åº¦ï¼‰
            complexity_score = 1.0
            if ext == 'py':
                try:
                    import radon.complexity as radon_complexity
                    
                    # McCabeè¤‡é›‘åº¦ã‚’è¨ˆç®—
                    results = radon_complexity.cc_visit(code)
                    if results:
                        avg_complexity = sum(r.complexity for r in results) / len(results)
                        
                        # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°: 1-10: é«˜å“è³ª, 11-20: ä¸­å“è³ª, 21+: ä½å“è³ª
                        if avg_complexity <= 10:
                            complexity_score = 1.0
                        elif avg_complexity <= 20:
                            complexity_score = 0.7
                        else:
                            complexity_score = 0.4
                            all_issues.append(f"{filename}: High complexity ({avg_complexity:.1f})")
                except ImportError:
                    # radonãŒåˆ©ç”¨ä¸å¯ã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                    complexity_score = 1.0
                except Exception:
                    complexity_score = 1.0
            
            # è¤‡é›‘åº¦ã«å¿œã˜ãŸé‡ã¿ä»˜ã‘ï¼ˆæ–°æŒ‡æ¨™é©ç”¨ï¼‰
            if complexity == "simple":
                weights = {
                    'executable': 0.35,      # 35% - æœ€é‡è¦ï¼ˆæ§‹æ–‡ã‚¨ãƒ©ãƒ¼ãªã—ï¼‰
                    'lint_error': 0.25,      # 25% - Lintã‚¨ãƒ©ãƒ¼
                    'structure': 0.20,       # 20% - æ§‹é€ 
                    'dependency': 0.15,      # 15% - ä¾å­˜é–¢ä¿‚
                    'complexity': 0.05,      # 5% - è¤‡é›‘åº¦ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                }
            elif complexity == "medium":
                weights = {
                    'executable': 0.30,      # 30% - æœ€é‡è¦
                    'lint_error': 0.25,      # 25% - Lintã‚¨ãƒ©ãƒ¼
                    'dependency': 0.20,      # 20% - ä¾å­˜é–¢ä¿‚
                    'structure': 0.15,       # 15% - æ§‹é€ 
                    'complexity': 0.10,      # 10% - è¤‡é›‘åº¦
                }
            else:  # complex
                weights = {
                    'executable': 0.25,      # 25% - æœ€é‡è¦
                    'lint_error': 0.25,      # 25% - Lintã‚¨ãƒ©ãƒ¼
                    'dependency': 0.25,      # 25% - ä¾å­˜é–¢ä¿‚
                    'complexity': 0.15,      # 15% - è¤‡é›‘åº¦ï¼ˆé‡è¦ï¼‰
                    'structure': 0.10,       # 10% - æ§‹é€ 
                }
            
            # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆæ¯”ç‡ã‚’è¿½åŠ ï¼‰
            file_score = (
                executable_score * weights['executable'] +
                lint_error_score * weights['lint_error'] +
                dependency_score * weights['dependency'] +
                structure_score * weights['structure'] +
                complexity_score * weights['complexity']
            )
            
            # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°: ä¸­é–“ã‚¹ã‚³ã‚¢ã®è©³ç´°
            logger.debug(
                f"[Quality] {filename}: Scores breakdown:\n"
                f"  - executable: {executable_score:.2f} Ã— {weights['executable']:.2f} = {executable_score * weights['executable']:.4f}\n"
                f"  - lint_error: {lint_error_score:.2f} Ã— {weights['lint_error']:.2f} = {lint_error_score * weights['lint_error']:.4f}\n"
                f"  - dependency: {dependency_score:.2f} Ã— {weights['dependency']:.2f} = {dependency_score * weights['dependency']:.4f}\n"
                f"  - structure: {structure_score:.2f} Ã— {weights['structure']:.2f} = {structure_score * weights['structure']:.4f}\n"
                f"  - complexity: {complexity_score:.2f} Ã— {weights['complexity']:.2f} = {complexity_score * weights['complexity']:.4f}\n"
                f"  - RAW total: {file_score:.4f}"
            )
            
            # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆæ¯”ç‡ã«ã‚ˆã‚‹è¿½åŠ ãƒšãƒŠãƒ«ãƒ†ã‚£
            raw_file_score = file_score  # ãƒšãƒŠãƒ«ãƒ†ã‚£é©ç”¨å‰ã‚’ä¿å­˜
            file_score = file_score * placeholder_score * comment_ratio_score
            
            # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°: ãƒšãƒŠãƒ«ãƒ†ã‚£é©ç”¨ã®è©³ç´°
            logger.debug(
                f"[Quality] {filename}: Penalty application:\n"
                f"  - placeholder_score: {placeholder_score:.4f}\n"
                f"  - comment_ratio_score: {comment_ratio_score:.4f}\n"
                f"  - {raw_file_score:.4f} (before) Ã— {placeholder_score:.4f} Ã— {comment_ratio_score:.4f} = {file_score:.4f} (after)"
            )
            
            # Review issuesãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«å˜ä½ï¼‰
            review_penalty = 0.0
            if hasattr(self, '_zen_summary') and self._zen_summary:
                review_issues_list = self._zen_summary.get("review", {}).get("issues", [])
                if review_issues_list:
                    # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«è©²å½“ã™ã‚‹issueã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                    file_review_issues = [i for i in review_issues_list if i.get('file', '') == filename]
                    file_review_count = len(file_review_issues)
                    if file_review_count > 0:
                        review_penalty = min(file_review_count * 0.02, 0.40)  # 1issue=2%, æœ€å¤§40%
                        file_score = file_score * (1.0 - review_penalty)
                        logger.debug(
                            f"[Quality] {filename}: Review penalty:\n"
                            f"  - review_issues: {file_review_count}\n"
                            f"  - review_penalty: {review_penalty:.2f} ({int(review_penalty * 100)}%)\n"
                            f"  - FINAL after review: {round(file_score, 2)} ({int(file_score * 100)}%)"
                        )
            
            # æœ€çµ‚ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°
            logger.debug(
                f"[Quality] {filename}: FINAL: {round(file_score, 2)} ({int(file_score * 100)}%)"
            )
            
            # è¦ä»¶å……è¶³åº¦ã‚’åæ˜ 
            #file_score = file_score * requirement_score
            
            quality_scores[filename] = round(file_score, 2)
        
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ã‚·ãƒ¥ãƒ¼ã®è¡¨ç¤º
        if all_issues:
            logger.debug("[Quality] Quality Issues Detected:")
            for issue in all_issues[:5]:  # æœ€å¤§5ä»¶è¡¨ç¤º
                logger.debug(f"[Quality]   {issue}")
            if len(all_issues) > 5:
                logger.debug(f"[Quality]   ... and {len(all_issues) - 5} more issues")
        
        return quality_scores
    
    def _format_quality_display(
        self,
        base_quality_scores: Dict[str, float],
        lint_errors: int = 0,
        lint_warnings: int = 0,
        review_issues: int = 0
    ) -> Tuple[float, str]:
        """
        å“è³ªã‚¹ã‚³ã‚¢ã®ç·åˆè©•ä¾¡ã¨è¡¨ç¤ºç”¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆPhase 1: è¡¨ç¤ºã®ã¿ï¼‰
        
        æ³¨æ„: å†…éƒ¨ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆé–¾å€¤ãƒã‚§ãƒƒã‚¯ç­‰ï¼‰ã«ã¯å½±éŸ¿ã—ãªã„
        
        Args:
            base_quality_scores: ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã®åŸºæœ¬å“è³ªã‚¹ã‚³ã‚¢
            lint_errors: Lintã‚¨ãƒ©ãƒ¼æ•°
            lint_warnings: Lintè­¦å‘Šæ•°
            review_issues: Code Review issueæ•°
        
        Returns:
            (overall_score, display_string)
        """
        # åŸºæœ¬å“è³ªã‚¹ã‚³ã‚¢ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«å¹³å‡ï¼‰
        # â€» å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚³ã‚¢ã«ã¯æ—¢ã«lint/reviewãƒšãƒŠãƒ«ãƒ†ã‚£ãŒé©ç”¨æ¸ˆã¿
        if base_quality_scores:
            overall = sum(base_quality_scores.values()) / len(base_quality_scores)
        else:
            overall = 1.0
        
        # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        overall_pct = int(overall * 100)
        
        # æ®‹èª²é¡ŒãŒãªã„å ´åˆã¯ã‚·ãƒ³ãƒ—ãƒ«è¡¨ç¤º
        if lint_errors == 0 and lint_warnings == 0 and review_issues == 0:
            return (overall, f"â“˜ Overall Quality:\n  {overall_pct}%")
        
        # æ®‹èª²é¡ŒãŒã‚ã‚‹å ´åˆã¯æƒ…å ±ã‚’è¡¨ç¤ºï¼ˆãƒšãƒŠãƒ«ãƒ†ã‚£ã¯æ—¢ã«ãƒ•ã‚¡ã‚¤ãƒ«å˜ä½ã§é©ç”¨æ¸ˆã¿ï¼‰
        remaining_details = []
        if lint_errors > 0:
            remaining_details.append(f"{lint_errors} lint error{'s' if lint_errors > 1 else ''}")
        if lint_warnings > 0:
            remaining_details.append(f"{lint_warnings} lint warning{'s' if lint_warnings > 1 else ''}")
        if review_issues > 0:
            remaining_details.append(f"{review_issues} review issue{'s' if review_issues > 1 else ''}")
        
        remaining_str = ", ".join(remaining_details) + " remaining"
        
        return (overall, f"â“˜ Overall Quality:\n  {overall_pct}% ({remaining_str})")
   
    def _assess_html_quality(self, code: str, complexity: str = "medium") -> float:
        """HTML specific quality assessment"""
        score = 0.0
        
        # HTMLåŸºæœ¬æ§‹é€ 
        if '<!DOCTYPE html>' in code:
            score += 0.15
        
        if '<html' in code and '</html>' in code:
            score += 0.1
        
        if '<head>' in code and '</head>' in code:
            score += 0.1
        
        if '<body>' in code and '</body>' in code:
            score += 0.1
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        if '<meta charset=' in code:
            score += 0.05
        
        if 'viewport' in code:
            score += 0.05
        
        # ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚°
        if '<style>' in code or 'style=' in code:
            score += 0.1
        
        # ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
        if '<script>' in code:
            score += 0.1
        
        # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯è¦ç´ (Complexæ™‚ã®ã¿è©•ä¾¡)
        if complexity == "complex":
            semantic_tags = ['<header>', '<nav>', '<main>', '<section>', '<article>', '<footer>']
            if any(tag in code for tag in semantic_tags):
                score += 0.05
        
        return score


    def _assess_css_quality(self, code: str, complexity: str = "medium") -> float:
        """CSS specific quality assessment"""
        score = 0.0
        
        # ã‚»ãƒ¬ã‚¯ã‚¿ã®å­˜åœ¨
        if '{' in code and '}' in code:
            score += 0.15
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–ãƒ‡ã‚¶ã‚¤ãƒ³(Mediumä»¥ä¸Š)
        if complexity in ["medium", "complex"]:
            if '@media' in code:
                score += 0.15
        
        # ãƒ¢ãƒ€ãƒ³CSSæ©Ÿèƒ½
        if 'flexbox' in code or 'display: flex' in code:
            score += 0.1
        
        if 'grid' in code or 'display: grid' in code:
            score += 0.1
        
        # å¤‰æ•°ãƒ»ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£(Complexæ™‚ã®ã¿)
        if complexity == "complex":
            if '--' in code and 'var(' in code:
                score += 0.1
        
        # ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³
        if '@keyframes' in code or 'animation:' in code or 'transition:' in code:
            score += 0.1
        
        return score


    def _assess_js_quality(self, code: str, complexity: str = "medium") -> float:
        """JavaScript specific quality assessment"""
        score = 0.0
        
        # é–¢æ•°å®šç¾©
        if 'function ' in code or '=>' in code:
            score += 0.15
        
        # ãƒ¢ãƒ€ãƒ³JSæ§‹æ–‡
        if 'const ' in code or 'let ' in code:
            score += 0.1
        
        # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°(Mediumä»¥ä¸Š)
        if complexity in ["medium", "complex"]:
            if 'try' in code and 'catch' in code:
                score += 0.15
        
        # ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒŠãƒ¼
        if 'addEventListener' in code or 'onclick' in code:
            score += 0.1
        
        # DOMæ“ä½œ
        if any(method in code for method in ['getElementById', 'querySelector', 'createElement']):
            score += 0.1
        
        # ã‚³ãƒ¡ãƒ³ãƒˆ
        if '//' in code or '/*' in code:
            score += 0.1
        
        return score


    def _assess_python_quality(self, code: str, complexity: str = "medium") -> float:
        """Python specific quality assessment(è¤‡é›‘åº¦è€ƒæ…®ç‰ˆ)"""
        score = 0.0
        
        # Simple: åŸºæœ¬çš„ãªå‹•ä½œã®ã¿è©•ä¾¡
        if complexity == "simple":
            # é–¢æ•°å®šç¾©(ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
            if 'def ' in code:
                score += 0.15
            
            # åŸºæœ¬çš„ãªæ§‹é€ ãŒã‚ã‚Œã°OK
            if 'print' in code or 'return' in code:
                score += 0.15
            
            # ã‚·ãƒ³ãƒ—ãƒ«ãªã‚³ãƒ¼ãƒ‰ãªã®ã§ç·©ãè©•ä¾¡
            return score
        
        # Medium: æ¨™æº–çš„ãªå“è³ªè¦æ±‚
        if 'def ' in code:
            score += 0.2
        
        if 'try:' in code and 'except' in code:
            score += 0.2
        
        if '"""' in code or "'''" in code:
            score += 0.2
        
        if 'if __name__ == "__main__"' in code:
            score += 0.1
        
        # Complex: ã‚ˆã‚Šé«˜ã„å“è³ªè¦æ±‚
        if complexity == "complex":
            # å‹ãƒ’ãƒ³ãƒˆ
            if '->' in code or ': ' in code:
                score += 0.1
            
            # ã‚¯ãƒ©ã‚¹å®šç¾©
            if 'class ' in code:
                score += 0.1
        
        return score


    def _assess_compiled_language_quality(self, code: str, complexity: str = "medium") -> float:
        """Quality assessment for compiled languages (Java, C++, etc.)"""
        score = 0.0
        
        # Simple: åŸºæœ¬æ§‹é€ ã®ã¿
        if complexity == "simple":
            if 'main' in code:
                score += 0.3
            return score
        
        # é–¢æ•°/ãƒ¡ã‚½ãƒƒãƒ‰å®šç¾©
        if any(keyword in code for keyword in ['void ', 'int ', 'public ', 'private ', 'fn ']):
            score += 0.15
        
        # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
        if any(keyword in code for keyword in ['try', 'catch', 'throw', 'Result<']):
            score += 0.15
        
        # ã‚³ãƒ¡ãƒ³ãƒˆ
        if '//' in code or '/*' in code:
            score += 0.1
        
        # mainé–¢æ•°
        if 'main' in code:
            score += 0.1
        
        # ã‚¯ãƒ©ã‚¹å®šç¾©(Mediumä»¥ä¸Š)
        if complexity in ["medium", "complex"]:
            if any(keyword in code for keyword in ['class ', 'struct ', 'impl ']):
                score += 0.2
        
        return score


    def _assess_generic_quality(self, code: str, complexity: str = "medium") -> float:
        """Generic quality assessment for unknown file types"""
        score = 0.0
        
        # Simple: ç·©ã„åŸºæº–
        if complexity == "simple":
            if len(code.strip()) > 0:
                score += 0.3
            return score
        
        # ã‚³ãƒ¡ãƒ³ãƒˆã®å­˜åœ¨
        if any(marker in code for marker in ['//', '/*', '#', '--']):
            score += 0.2
        
        # æ§‹é€ åŒ–ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰(æ‹¬å¼§ã®ä½¿ç”¨)
        if '{' in code and '}' in code:
            score += 0.15
        
        # ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆ(è¤‡æ•°ã‚¹ãƒšãƒ¼ã‚¹ã¾ãŸã¯ã‚¿ãƒ–)
        lines = code.split('\n')
        indented_lines = [line for line in lines if line.startswith('    ') or line.startswith('\t')]
        if len(indented_lines) > len(lines) * 0.3:
            score += 0.15
        
        return score

    def _generate_recommendations(self, goal: str, analysis: str, 
                                generated_code: Dict[str, str], 
                                quality_scores: Dict[str, float]) -> List[str]:
        """Generate recommendations based on code quality scores
        
        Only shows recommendations when quality score is below 0.7
        """
        recommendations = []
        
        # å“è³ªã‚¹ã‚³ã‚¢ã«åŸºã¥ãæ¨å¥¨
        for filename, score in quality_scores.items():
            ext = filename.split('.')[-1].lower()
            
            if score < 0.7:
                recommendations.append(f"Consider improving code quality in {filename} (score: {score:.2f})")
                
                # è¨€èªåˆ¥ã®å…·ä½“çš„æ”¹å–„ææ¡ˆ
                if ext == 'html':
                    if score < 0.5:
                        recommendations.append(f"  â†’ Add proper HTML structure, meta tags, and semantic elements")
                elif ext == 'py':
                    if score < 0.5:
                        recommendations.append(f"  â†’ Add functions, error handling, and documentation")
        
        
        return recommendations

    def apply_generated_code(self, result: SemiAutoResult) -> ApplyResult:
        """Apply generated code to files"""
        if not result.success:
            return ApplyResult(
                success=False,
                error="Cannot apply code from failed result"
            )
        
        try:
            applied_files = []
            backup_paths = []
            
            for filename, code in result.generated_code.items():
                file_path = self.workspace_path / filename
                
                # Create backup if file exists
                if file_path.exists() and self.config and self.config.get("auto_backup", True):
                    backup_path = self._create_backup(file_path)
                    if backup_path:
                        backup_paths.append(str(backup_path))
                
                # Write new content
                try:
                    # ===== è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è‡ªå‹•ä½œæˆ =====
                    file_path.parent.mkdir(parents=True, exist_ok=True)

                    file_path.write_text(code, encoding='utf-8')
                    applied_files.append(str(file_path))
                    
                except Exception as e:
                    return ApplyResult(
                        success=False,
                        error=f"Failed to write {filename}: {str(e)}"
                    )
            
            return ApplyResult(
                success=True,
                applied_files=applied_files,
                backup_paths=backup_paths,
                quality_scores=result.quality_scores
            )
            
        except Exception as e:
            return ApplyResult(
                success=False,
                error=f"Code application failed: {str(e)}"
            )

    def _create_backup(self, file_path: Path) -> Optional[Path]:
        """Create backup of existing file"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_name = f"{file_path.name}.{timestamp}.backup"
            backup_path = self.backup_dir / backup_name
            
            import shutil
            shutil.copy2(file_path, backup_path)
            
            return backup_path
            
        except Exception:
            return None

    def process_message(self, message: str, auto_apply: bool = False) -> Dict[str, Any]:
        """
        Process a message containing code modifications
        
        Args:
            message: Message text containing code blocks
            auto_apply: If True, automatically apply modifications
            
        Returns:
            Dictionary with processing results:
            - modifications: List of extracted modifications
            - preview: Preview of changes
            - applied: Whether changes were applied
            - errors: Any errors encountered
        """
        result = {
            'modifications': [],
            'preview': '',
            'applied': False,
            'errors': [],
            'timestamp': datetime.now().isoformat()
        }
        
        try:
            # Extract modifications from message
            modifications = self.extractor.extract_modifications(message)
            result['modifications'] = modifications
            
            if not modifications:
                result['errors'].append("No valid code modifications found in message")
                return result
                
            # Generate preview
            result['preview'] = self._generate_preview(modifications)
            
            # Apply modifications if requested
            if auto_apply:
                apply_result = self._apply_modifications(modifications)
                result['applied'] = apply_result['success']
                if apply_result['errors']:
                    result['errors'].extend(apply_result['errors'])
                    
            # Log session
            self.session_log.append(result)
            
        except Exception as e:
            result['errors'].append(f"Processing error: {str(e)}")
            
        return result
    
    def _generate_preview(self, modifications: List[Dict[str, Any]]) -> str:
        """Generate a preview of modifications to be applied"""
        preview_lines = []
        preview_lines.append("=== MODIFICATION PREVIEW ===\n")
        
        for i, mod in enumerate(modifications, 1):
            preview_lines.append(f"{i}. {mod['type'].upper()}: {mod['filename']}")
            
            if mod['type'] == 'create':
                preview_lines.append(f"   Creating new file with {len(mod['content'].splitlines())} lines")
                
            elif mod['type'] == 'update':
                line_info = ""
                if 'line_range' in mod:
                    start = mod['line_range']['start']
                    end = mod['line_range']['end']
                    line_info = f" (lines {start}-{end})"
                    
                preview_lines.append(f"   Updating existing file{line_info}")
                preview_lines.append(f"   New content: {len(mod['content'].splitlines())} lines")
                
            elif mod['type'] == 'delete':
                preview_lines.append("   File will be deleted")
                
            preview_lines.append("")
            
        return "\n".join(preview_lines)
    
    def _apply_modifications(self, modifications: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Apply modifications to files"""
        result = {
            'success': True,
            'errors': [],
            'modified_files': [],
            'backed_up_files': []
        }
        
        try:
            for mod in modifications:
                file_result = self._apply_single_modification(mod)
                
                if file_result['success']:
                    result['modified_files'].append(mod['filename'])
                    if file_result.get('backed_up'):
                        result['backed_up_files'].append(mod['filename'])
                else:
                    result['success'] = False
                    result['errors'].extend(file_result['errors'])
                    
        except Exception as e:
            result['success'] = False
            result['errors'].append(f"Application error: {str(e)}")
            
        return result
    
    def _apply_single_modification(self, mod: Dict[str, Any]) -> Dict[str, Any]:
        """Apply a single modification to a file"""
        result = {'success': False, 'errors': [], 'backed_up': False}
        
        try:
            filepath = self.workspace_path / mod['filename']
            
            # Validate file path
            if not self.extractor.validate_file_path(mod['filename']):
                result['errors'].append(f"Invalid file path: {mod['filename']}")
                return result
            
            # â­â­â­ Safety Guard: Force createâ†’update if file exists â­â­â­
            # LLMãŒæ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾ã—ã¦type='create'ã‚’èª¤ã£ã¦è¿”ã—ãŸå ´åˆã®é˜²å¾¡
            # ã“ã‚Œã«ã‚ˆã‚Šã€æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ä¿æŒã—ãŸã¾ã¾updateãŒå®Ÿè¡Œã•ã‚Œã‚‹
            if mod['type'] == 'create' and filepath.exists():
                logger.warning(
                    f"[Safety Guard] File '{mod['filename']}' already exists but type='create' specified. "
                    f"Automatically converting to type='update' to preserve existing content."
                )
                # modã‚’è¾æ›¸ã‚³ãƒ”ãƒ¼ã—ã¦'type'ã ã‘å¤‰æ›´
                mod = {**mod, 'type': 'update'}
                
            # Handle different modification types
            if mod['type'] == 'create':
                result = self._create_file(filepath, mod['content'])
                
            elif mod['type'] == 'update':
                result = self._update_file(filepath, mod)
                
            elif mod['type'] == 'delete':
                result = self._delete_file(filepath)
                
            else:
                result['errors'].append(f"Unknown modification type: {mod['type']}")
                
        except Exception as e:
            result['errors'].append(f"File modification error: {str(e)}")
            
        return result
    
    def _create_file(self, filepath: Path, content: str) -> Dict[str, Any]:
        """Create a new file with content"""
        result = {'success': False, 'errors': []}
        
        try:
            # Create parent directories if needed
            filepath.parent.mkdir(parents=True, exist_ok=True)
            
            # Check if file already exists
            if filepath.exists():
                # Backup existing file
                backup_result = self._backup_file(filepath)
                if not backup_result['success']:
                    result['errors'].extend(backup_result['errors'])
                    return result
                result['backed_up'] = True
                
            # Write new content
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content)
                
            result['success'] = True
            
        except Exception as e:
            result['errors'].append(f"File creation error: {str(e)}")
            
        return result
    
    def _update_file(self, filepath: Path, mod: Dict[str, Any]) -> Dict[str, Any]:
        """Update an existing file"""
        result = {'success': False, 'errors': []}
        
        try:
            if not filepath.exists():
                result['errors'].append(f"File not found for update: {filepath}")
                return result
                
            # Backup original file
            backup_result = self._backup_file(filepath)
            if not backup_result['success']:
                result['errors'].extend(backup_result['errors'])
                return result
            result['backed_up'] = True
            
            # Read original content
            with open(filepath, 'r', encoding='utf-8') as f:
                original_lines = f.readlines()
                
            # Apply modification
            if 'line_range' in mod:
                # Line-specific update
                start_line = mod['line_range']['start'] - 1  # Convert to 0-based
                end_line = mod['line_range']['end']
                
                new_lines = mod['content'].split('\n')
                # Add newlines except for the last line
                new_lines = [line + '\n' for line in new_lines[:-1]] + [new_lines[-1]]
                
                # Replace specified lines
                updated_lines = (
                    original_lines[:start_line] + 
                    new_lines + 
                    original_lines[end_line:]
                )
            else:
                # Full file replacement
                updated_lines = mod['content'].split('\n')
                updated_lines = [line + '\n' for line in updated_lines[:-1]] + [updated_lines[-1]]
                
            # Write updated content
            with open(filepath, 'w', encoding='utf-8') as f:
                f.writelines(updated_lines)
                
            result['success'] = True
            
        except Exception as e:
            result['errors'].append(f"File update error: {str(e)}")
            
        return result
    
    def _delete_file(self, filepath: Path) -> Dict[str, Any]:
        """Delete a file"""
        result = {'success': False, 'errors': []}
        
        try:
            if not filepath.exists():
                result['errors'].append(f"File not found for deletion: {filepath}")
                return result
                
            # Backup before deletion
            backup_result = self._backup_file(filepath)
            if not backup_result['success']:
                result['errors'].extend(backup_result['errors'])
                return result
            result['backed_up'] = True
            
            # Delete file
            filepath.unlink()
            result['success'] = True
            
        except Exception as e:
            result['errors'].append(f"File deletion error: {str(e)}")
            
        return result
    
    def _backup_file(self, filepath: Path) -> Dict[str, Any]:
        """Create a backup of a file"""
        result = {'success': False, 'errors': []}
        
        try:
            if not filepath.exists():
                result['errors'].append(f"Cannot backup non-existent file: {filepath}")
                return result
                
            # Generate backup filename with timestamp
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_name = f"{filepath.name}.{timestamp}.backup"
            backup_path = self.backup_dir / backup_name
            
            # Copy file to backup location
            import shutil
            shutil.copy2(filepath, backup_path)
            
            result['success'] = True
            result['backup_path'] = str(backup_path)
            
        except Exception as e:
            result['errors'].append(f"Backup error: {str(e)}")
            
        return result
    
    def list_backups(self) -> List[Dict[str, str]]:
        """List available backup files"""
        backups = []
        
        if not self.backup_dir.exists():
            return backups
            
        for backup_file in self.backup_dir.glob("*.backup"):
            # Parse backup filename to extract original name and timestamp
            name_parts = backup_file.name.split('.')
            if len(name_parts) >= 3:
                original_name = '.'.join(name_parts[:-2])
                timestamp = name_parts[-2]
                
                backups.append({
                    'original_name': original_name,
                    'backup_path': str(backup_file),
                    'timestamp': timestamp,
                    'size': backup_file.stat().st_size
                })
                
        return sorted(backups, key=lambda x: x['timestamp'], reverse=True)
    
    def restore_backup(self, backup_path: str, target_path: Optional[str] = None) -> Dict[str, Any]:
        """Restore a file from backup"""
        result = {'success': False, 'errors': []}
        
        try:
            backup_file = Path(backup_path)
            if not backup_file.exists():
                result['errors'].append(f"Backup file not found: {backup_path}")
                return result
                
            if target_path:
                target_file = self.workspace_path / target_path
            else:
                # Extract original filename from backup name
                name_parts = backup_file.name.split('.')
                original_name = '.'.join(name_parts[:-2])
                target_file = self.workspace_path / original_name
                
            # Create target directory if needed
            target_file.parent.mkdir(parents=True, exist_ok=True)
            
            # Copy backup to target location
            import shutil
            shutil.copy2(backup_file, target_file)
            
            result['success'] = True
            result['restored_to'] = str(target_file)
            
        except Exception as e:
            result['errors'].append(f"Restore error: {str(e)}")
            
        return result
    
    def get_session_log(self) -> List[Dict[str, Any]]:
        """Get the current session log"""
        return self.session_log.copy()
    
    def clear_session_log(self):
        """Clear the session log"""
        self.session_log.clear()
    
    def cleanup_temp_files(self):
        """Clean up temporary files"""
        if self.temp_dir and self.temp_dir.exists():
            import shutil
            shutil.rmtree(self.temp_dir)
            self.temp_dir.mkdir(exist_ok=True)
    
    def get_status(self) -> Dict[str, Any]:
        """Get current engine status"""
        return {
            'workspace_path': str(self.workspace_path),
            'backup_dir': str(self.backup_dir),
            'temp_dir': str(self.temp_dir),
            'session_entries': len(self.session_log),
            'available_backups': len(self.list_backups()),
            'workspace_exists': self.workspace_path.exists(),
            'backup_dir_exists': self.backup_dir.exists()
        }

    # ==========================================
    # Phase 2 Auto-Completion Methods
    # ==========================================
    
    def _get_required_phase2_files(
        self, 
        goal: str, 
        project_type: str, 
        generated_files: Dict[str, str]
    ) -> List[str]:
        """
        Phase 2ã§å¿…é ˆã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ¤å®šï¼ˆæ”¹å–„ç‰ˆï¼‰
        
        project_typeã¨goalã®å†…å®¹ã€ç”Ÿæˆæ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†æã‚’çµ„ã¿åˆã‚ã›ã¦
        å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç²¾å¯†ã«åˆ¤å®šã—ã¾ã™ã€‚
        
        Args:
            goal: å®Ÿè£…ç›®æ¨™ï¼ˆGoalã‹ã‚‰ç›´æ¥å–å¾—ï¼‰
            project_type: _detect_project_type()ã§åˆ¤å®šã•ã‚ŒãŸã‚¿ã‚¤ãƒ—
            generated_files: Phase 1+2ã§ç”Ÿæˆã•ã‚ŒãŸå…¨ãƒ•ã‚¡ã‚¤ãƒ«
        
        Returns:
            å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆï¼ˆãƒ‘ã‚¹å½¢å¼ï¼‰
        """
        required = []
        goal_lower = goal.lower()
        
        # web_app ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆã®ã¿JavaScriptãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œæŸ»
        if project_type != 'web_app':
            logger.debug(f"[Phase 2 Required] project_type={project_type}, skipping JS check")
            return required
        
        # æ¡ä»¶1: Goalã«JavaScripté–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹ã‹
        js_keywords = [
            'javascript', 'frontend', 'front-end', 'vanilla js',
            'vanilla javascript', 'client-side', 'dom', 'interactive',
            'web app', 'web application', 'html', 'css'
        ]
        has_js_keywords = any(kw in goal_lower for kw in js_keywords)
        
        # æ¡ä»¶2: HTMLãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¦ã„ã‚‹ã‹
        html_files = [f for f in generated_files.keys() if f.endswith('.html')]
        has_html = len(html_files) > 0
        
        logger.debug(f"[Phase 2 Required] JS keywords: {has_js_keywords}, HTML files: {html_files}")
        
        # ä¸¡æ¡ä»¶ã‚’æº€ãŸã™å ´åˆã€ã¾ãŸã¯æ˜ç¤ºçš„ã«ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ãŒè¦æ±‚ã•ã‚Œã¦ã„ã‚‹å ´åˆ
        if has_js_keywords or has_html:
            # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å¿…è¦ãªJSãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŠ½å‡º
            js_path = self._extract_js_path_from_html(generated_files, html_files)
            
            if js_path:
                required.append(js_path)
                logger.debug(f"[Phase 2 Required] Detected required JS file: {js_path}")
            elif has_html:
                # HTMLã¯ã‚ã‚‹ãŒã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ‘ã‚¹ãŒä¸æ˜ãªå ´åˆã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹ã‚’ä½¿ç”¨
                default_path = 'static/js/app.js'
                required.append(default_path)
                logger.debug(f"[Phase 2 Required] Using default JS path: {default_path}")
        
        return required

    def _extract_js_path_from_html(
        self, 
        generated_files: Dict[str, str], 
        html_files: List[str]
    ) -> Optional[str]:
        """
        HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å‚ç…§ã•ã‚Œã¦ã„ã‚‹JSãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŠ½å‡º
        
        <script src="...">ã‚¿ã‚°ã‚’è§£æã—ã€æœ€ã‚‚é©åˆ‡ãªJSãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’è¿”ã—ã¾ã™ã€‚
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸå…¨ãƒ•ã‚¡ã‚¤ãƒ«
            html_files: HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ
        
        Returns:
            JSãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆä¾‹: 'static/js/app.js'ï¼‰ã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯None
        """
        # æ¤œç´¢å¯¾è±¡ã®JSãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³
        target_js_names = ['app.js', 'main.js', 'script.js', 'index.js']
        
        for html_file in html_files:
            if html_file not in generated_files:
                continue
            
            html_content = generated_files[html_file]
            
            # <script src="...">ã‚¿ã‚°ã‚’æŠ½å‡º
            script_pattern = re.compile(r'<script[^>]+src=["\']([^"\']+)["\']', re.IGNORECASE)
            matches = script_pattern.findall(html_content)
            
            for src in matches:
                # å¯¾è±¡ã®JSãƒ•ã‚¡ã‚¤ãƒ«åãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹
                for target in target_js_names:
                    if target in src:
                        # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è€ƒæ…®ã—ã¦ãƒ‘ã‚¹ã‚’æ§‹ç¯‰
                        html_dir = str(Path(html_file).parent)
                        
                        if html_dir == '.' or html_dir == '':
                            # ãƒ«ãƒ¼ãƒˆã«HTMLãŒã‚ã‚‹å ´åˆ
                            if src.startswith('/'):
                                js_path = src[1:]  # å…ˆé ­ã®/ã‚’é™¤å»
                            else:
                                js_path = src
                        else:
                            # ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«HTMLãŒã‚ã‚‹å ´åˆ
                            js_path = str(Path(html_dir) / src)
                        
                        # ãƒ‘ã‚¹ã®æ­£è¦åŒ–
                        js_path = js_path.replace('\\', '/').replace('//', '/')
                        
                        logger.debug(f"[JS Path Extraction] Found: {js_path} (from {html_file})")
                        return js_path
        
        # index.html ã®å†…å®¹ã‚’ç¢ºèªã—ã¦æ¨æ¸¬
        for html_file in html_files:
            if 'index' in html_file.lower():
                html_dir = str(Path(html_file).parent)
                if html_dir == '.' or html_dir == '':
                    return 'static/js/app.js'
                else:
                    return f'{html_dir}/js/app.js'
        
        return None

    def _find_missing_required_files(
        self, 
        required_files: List[str], 
        generated_files: Dict[str, str]
    ) -> List[str]:
        """
        ä¸è¶³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡ºï¼ˆæ”¹å–„ç‰ˆã‚¹ãƒãƒ¼ãƒˆãƒãƒƒãƒãƒ³ã‚°ï¼‰
        
        ãƒ‘ã‚¹ã®æ­£è¦åŒ–ã¨é¡ä¼¼ãƒ•ã‚¡ã‚¤ãƒ«åã®è¨±å®¹ã«ã‚ˆã‚Šã€
        ã‚ˆã‚ŠæŸ”è»Ÿã«ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ã‚’åˆ¤å®šã—ã¾ã™ã€‚
        
        Args:
            required_files: å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®è¾æ›¸
        
        Returns:
            ä¸è¶³ã—ã¦ã„ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ
        """
        if not required_files:
            return []
        
        missing = []
        generated_keys = list(generated_files.keys())
        
        # æ­£è¦åŒ–ã—ãŸç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ
        normalized_generated = {}
        for key in generated_keys:
            normalized = self._normalize_filepath(key)
            normalized_generated[normalized] = key
        
        for required in required_files:
            found = False
            normalized_required = self._normalize_filepath(required)
            
            # 1. æ­£è¦åŒ–å¾Œã®å®Œå…¨ä¸€è‡´
            if normalized_required in normalized_generated:
                found = True
                logger.debug(f"[Missing Check] âœ“ Found (exact): {required}")
            
            # 2. ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿ã§ä¸€è‡´ï¼ˆãƒ‘ã‚¹ãŒç•°ãªã‚‹å ´åˆï¼‰
            if not found:
                required_name = Path(required).name
                for gen_key in generated_keys:
                    gen_name = Path(gen_key).name
                    if required_name == gen_name:
                        found = True
                        logger.debug(f"[Missing Check] âœ“ Found (name match): {required} as {gen_key}")
                        break
            
            # 3. é¡ä¼¼ãƒ•ã‚¡ã‚¤ãƒ«åã®è¨±å®¹ï¼ˆapp.js, main.js, script.jsç­‰ï¼‰
            if not found:
                required_ext = Path(required).suffix
                js_alternatives = ['app.js', 'main.js', 'script.js', 'index.js', 'application.js']
                
                if required_ext == '.js':
                    for gen_key in generated_keys:
                        gen_name = Path(gen_key).name
                        if gen_name in js_alternatives:
                            found = True
                            logger.debug(f"[Missing Check] âœ“ Found (alternative): {required} as {gen_key}")
                            break
            
            if not found:
                missing.append(required)
                logger.debug(f"[Missing Check] âœ— Missing: {required}")
        
        return missing
    
    def _normalize_filepath(self, filepath: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æ­£è¦åŒ–ï¼ˆæ¯”è¼ƒç”¨ï¼‰"""
        # ãƒãƒƒã‚¯ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã«å¤‰æ›
        normalized = filepath.replace('\\', '/')
        # é‡è¤‡ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’é™¤å»
        while '//' in normalized:
            normalized = normalized.replace('//', '/')
        # å…ˆé ­ã®ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’é™¤å»
        normalized = normalized.lstrip('/')
        # å°æ–‡å­—åŒ–
        normalized = normalized.lower()
        return normalized

    def _auto_complete_phase2_files(
        self,
        missing_files: List[str],
        all_files: Dict[str, str],
        goal: str,
        project_type: str,
        detected_language: str
    ) -> Dict[str, str]:
        """
        Phase 2ã§ä¸è¶³ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•ç”Ÿæˆ
        
        æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ´»ç”¨ã—ã€LLMã«ä¸è¶³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã•ã›ã¾ã™ã€‚
        
        Args:
            missing_files: ä¸è¶³ã—ã¦ã„ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ
            all_files: Phase 1+2 ã§ç”Ÿæˆã•ã‚ŒãŸå…¨ãƒ•ã‚¡ã‚¤ãƒ«
            goal: å…ƒã®Goal
            project_type: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¿ã‚¤ãƒ—
            detected_language: æ¤œå‡ºã•ã‚ŒãŸè¨€èª
        
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®è¾æ›¸
        """
        completed = {}
        
        for missing_file in missing_files:
            logger.debug(f"[Phase 2 Auto-Complete] Generating: {missing_file}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã®èª¬æ˜ã‚’å–å¾—
            file_description = self._get_file_description_for_completion(missing_file, project_type)
            
            # âš ï¸ CRITICAL: existing_files ã‚’è¨­å®š
            # ComplexityAssessor ã¯åˆæœŸåŒ–æ™‚ã«ãƒ‡ã‚£ã‚¹ã‚¯ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã™ã‚‹ãŒã€
            # Phase 1/Phase 2 ã§ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã¯ãƒ¡ãƒ¢ãƒªå†…ã«ã®ã¿å­˜åœ¨ã™ã‚‹ã€‚
            # ãã®ãŸã‚ã€æ˜ç¤ºçš„ã« existing_files ã‚’æ›´æ–°ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
            if hasattr(self, '_complexity_assessor'):
                self._complexity_assessor.existing_files = all_files.copy()
                logger.debug(f"[Phase 2 Auto-Complete] Set existing_files: {len(all_files)} files")
            
            # completion_goal ã‚’æ§‹ç¯‰
            completion_goal = f"""Generate the missing file: {missing_file}

File purpose: {file_description}

Project type: {project_type}
Language: {detected_language}

CRITICAL REQUIREMENTS:
1. Generate ONLY this file - do NOT regenerate any other files
2. The file must integrate with the existing application architecture
3. You will see the existing files in context - use them to understand the project structure

For JavaScript frontend files (app.js, main.js, script.js):
- Use vanilla JavaScript (no frameworks like React, Vue, Angular)
- Use Fetch API for backend API calls
- Implement complete DOM manipulation and event listeners
- Handle all CRUD operations (Create, Read, Update, Delete)
- Add proper error handling with try-catch
- Include loading indicators and user feedback
- Match the API endpoints defined in the backend files

MANDATORY IMPLEMENTATION:
- This file MUST be complete and functional
- NO placeholder comments like "// TODO: implement"
- NO incomplete functions
- FULL implementation of all required features

Generate complete, production-ready code for {missing_file} only."""
            
            try:
                # âš ï¸ CRITICAL: force_filename ã‚’ä½¿ç”¨
                # force_filename ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§ï¼š
                # 1. Multi-File Detection ãŒã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã‚‹ï¼ˆ4305-4308è¡Œã®æ—¢å­˜æ¡ä»¶åˆ†å²ï¼‰
                # 2. LLMãŒè¿”ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç„¡è¦–ã—ã¦ã€force_filenameã§ä¿å­˜ã•ã‚Œã‚‹
                # 3. å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ç”ŸæˆãŒä¿è¨¼ã•ã‚Œã‚‹
                result = self._generate_code_implementation(
                    goal=completion_goal,
                    analysis=f"Auto-completing missing file: {missing_file}",
                    plan=f"Generate {missing_file} to complete Phase 2",
                    complexity='simple',
                    skip_validation=True,  # post-validation ã¯ä¸è¦
                    skip_multi_stage=True,  # å¤šæ®µéšç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—
                    force_filename=missing_file
                )
                
                if result and missing_file in result:
                    completed[missing_file] = result[missing_file]
                    # âš ï¸ é‡è¦: æ¬¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆæ™‚ã«ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å«ã¾ã‚Œã‚‹ã‚ˆã†ã€
                    # all_files ã‚’æ›´æ–°
                    all_files[missing_file] = result[missing_file]
                    logger.debug(f"[Phase 2 Auto-Complete] âœ“ Successfully generated: {missing_file}")
                else:
                    logger.warning(f"[Phase 2 Auto-Complete] âœ— Failed to generate: {missing_file}")
                    if result:
                        logger.warning(f"[Phase 2 Auto-Complete]   Result keys: {list(result.keys())}")
                    else:
                        logger.warning(f"[Phase 2 Auto-Complete]   Result was None or empty")
            
            except Exception as e:
                logger.error(f"[Phase 2 Auto-Complete] âœ— Exception while generating {missing_file}: {e}")
                import traceback
                logger.error(f"[Phase 2 Auto-Complete]   Traceback: {traceback.format_exc()}")
        
        return completed

    def _get_file_description_for_completion(self, missing_file: str, project_type: str) -> str:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ã®èª¬æ˜ã‚’è¿”ã™ï¼ˆAuto-Completionç”¨ï¼‰
        
        Args:
            missing_file: ãƒ•ã‚¡ã‚¤ãƒ«å/ãƒ‘ã‚¹
            project_type: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¿ã‚¤ãƒ—
        
        Returns:
            ãƒ•ã‚¡ã‚¤ãƒ«ã®èª¬æ˜æ–‡
        """
        file_name = Path(missing_file).name.lower()
        
        # JavaScriptãƒ•ã‚¡ã‚¤ãƒ«ã®èª¬æ˜
        js_descriptions = {
            'app.js': 'Frontend JavaScript application - handles all UI interactions, API calls via Fetch API, DOM manipulation, event listeners, form submissions, CRUD operations, toast notifications, and loading indicators',
            'main.js': 'Main JavaScript entry point - initializes the application, sets up event listeners, and coordinates modules',
            'script.js': 'JavaScript implementation - contains all client-side logic and user interactions',
            'index.js': 'JavaScript index module - exports and coordinates other modules',
            'application.js': 'Application JavaScript - main application logic and UI handling',
        }
        
        if file_name in js_descriptions:
            return js_descriptions[file_name]
        
        # æ‹¡å¼µå­ã«ã‚ˆã‚‹æ±ç”¨èª¬æ˜
        ext = Path(missing_file).suffix.lower()
        generic_descriptions = {
            '.js': 'JavaScript module implementing client-side functionality',
            '.ts': 'TypeScript module implementing typed client-side functionality',
            '.css': 'Stylesheet for visual styling',
            '.html': 'HTML document for user interface',
            '.py': 'Python module implementing server-side functionality',
        }
        
        if ext in generic_descriptions:
            return generic_descriptions[ext]
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        return f'{missing_file} implementation'

    # ==========================================
    # å…±é€šLLMå‘¼ã³å‡ºã—ãƒ¡ã‚½ãƒƒãƒ‰
    # ==========================================
    
    def _call_llm_api(self, prompt: str, system_prompt: str = None, max_tokens: int = 8000) -> Optional[str]:
        """
        LLM APIã‚’å‘¼ã³å‡ºã™å…±é€šãƒ¡ã‚½ãƒƒãƒ‰
        
        Args:
            prompt: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ–‡å­—åˆ—
            system_prompt: ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        
        Returns:
            ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã€å¤±æ•—æ™‚ã¯None
        """
        if not self.llm_manager:
            logger.debug("[LLM API] llm_manager not available")
            return None
        
        try:
            if system_prompt is None:
                system_prompt = "You are an expert software engineer. Generate clean, working code."
            
            response = self.llm_manager.generate_response(
                prompt=prompt,
                system_prompt=system_prompt,
                max_tokens=max_tokens
            )
            
            if hasattr(response, 'content'):
                return response.content
            elif isinstance(response, str):
                return response
            else:
                logger.debug("[LLM API] Unexpected response type")
                return None
                
        except Exception as e:
            logger.debug(f"[LLM API] Error: {e}")
            return None

    # ==========================================
    # G-1: HTML-CSS ã‚¯ãƒ©ã‚¹æ•´åˆæ€§æ¤œè¨¼
    # ==========================================
    
    def _validate_html_css_class_consistency(
        self,
        generated_files: Dict[str, str]
    ) -> List[str]:
        """
        HTMLã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ãŒCSSã«æœªå®šç¾©ã®ã‚¯ãƒ©ã‚¹ã‚’æ¤œå‡º
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            CSSã«æœªå®šç¾©ã®ã‚¯ãƒ©ã‚¹åãƒªã‚¹ãƒˆ
        """
        import re
        
        # 1. HTMLã‹ã‚‰å…¨ã‚¯ãƒ©ã‚¹ã‚’æŠ½å‡º
        html_classes = set()
        for filepath, content in generated_files.items():
            if filepath.endswith(('.html', '.htm')):
                # class="xxx yyy zzz" ã‚’æŠ½å‡º
                matches = re.findall(r'class=["\']([^"\']*)["\']', content, re.IGNORECASE)
                for match in matches:
                    # ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã§åˆ†å‰²
                    html_classes.update(match.split())
        
        if not html_classes:
            logger.debug("[G-1] No HTML classes found")
            return []
        
        # 2. CSSã‹ã‚‰å…¨ã‚»ãƒ¬ã‚¯ã‚¿ã‚’æŠ½å‡º
        css_classes = set()
        for filepath, content in generated_files.items():
            if filepath.endswith('.css'):
                # .classname ã‚’æŠ½å‡ºï¼ˆæ“¬ä¼¼ã‚¯ãƒ©ã‚¹ :hover ç­‰ã¯é™¤å¤–ï¼‰
                matches = re.findall(r'\.([a-zA-Z_-][a-zA-Z0-9_-]*)(?=[^a-zA-Z0-9_-]|$)', content)
                css_classes.update(matches)
        
        # 3. å·®åˆ†ã‚’æ¤œå‡º
        missing = html_classes - css_classes
        
        # 4. ä¸€èˆ¬çš„ãªãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¯ãƒ©ã‚¹ï¼ˆå¤–éƒ¨CSSãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯æƒ³å®šï¼‰ã¯é™¤å¤–
        utility_prefixes = ('sr-', 'visually-', 'aria-')
        missing = {c for c in missing if not any(c.startswith(p) for p in utility_prefixes)}
        
        if missing:
            logger.debug(f"[G-1] Found {len(missing)} missing CSS classes: {list(missing)[:10]}...")
        else:
            logger.debug("[G-1] All HTML classes are defined in CSS")
        
        return list(missing)
    
    def _auto_complete_missing_css_classes(
        self,
        missing_classes: List[str],
        generated_files: Dict[str, str]
    ) -> Dict[str, str]:
        """
        ä¸è¶³CSSã‚¯ãƒ©ã‚¹ã‚’LLMã§ç”Ÿæˆã—ã¦styles.cssã«è¿½è¨˜
        
        Args:
            missing_classes: ä¸è¶³ã—ã¦ã„ã‚‹CSSã‚¯ãƒ©ã‚¹åã®ãƒªã‚¹ãƒˆ
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«
        
        Returns:
            æ›´æ–°ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«è¾æ›¸
        """
        if not missing_classes:
            return generated_files
        
        # CSSãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç‰¹å®š
        css_files = [f for f in generated_files.keys() if f.endswith('.css')]
        if not css_files:
            logger.debug("[G-1] No CSS file found, skipping auto-completion")
            return generated_files
        
        css_file = css_files[0]  # æœ€åˆã®CSSãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
        existing_css = generated_files[css_file]
        
        # HTMLã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆã‚¯ãƒ©ã‚¹ã®ä½¿ç”¨çŠ¶æ³ã‚’æŠŠæ¡ï¼‰
        html_context = ""
        for filepath, content in generated_files.items():
            if filepath.endswith(('.html', '.htm')):
                # ã‚¯ãƒ©ã‚¹ãŒä½¿ã‚ã‚Œã¦ã„ã‚‹è¦ç´ ã‚’æŠ½å‡º
                import re
                for cls in missing_classes[:10]:  # æœ€å¤§10ã‚¯ãƒ©ã‚¹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
                    pattern = rf'<[^>]*class=["\'][^"\']*\b{re.escape(cls)}\b[^"\']*["\'][^>]*>'
                    matches = re.findall(pattern, content)
                    if matches:
                        html_context += f"\n/* {cls} is used in: {matches[0][:100]} */\n"
        
        # LLMãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        prompt = f"""Generate CSS definitions for these missing classes that are used in HTML but not defined in CSS:

Missing classes: {missing_classes}

{html_context}

Existing CSS structure (for reference):
{existing_css[:2000]}

Requirements:
1. Generate ONLY the CSS rules for the missing classes
2. Match the existing CSS style (colors, spacing, fonts)
3. Include reasonable default styles:
   - For buttons (btn, btn-*): padding, border-radius, cursor, hover states
   - For forms (form-*): proper spacing, borders, focus states
   - For layout (container, *-section): width, margin, padding
   - For icons (stat-icon, *-icon): proper sizing (width, height), display: flex, alignment
   - For modals (modal, modal-*): positioning, overlay, z-index
   - For tables (table-*, products-*): borders, spacing, alignment
4. Do NOT include any explanation, just CSS code
5. Use consistent naming with existing styles

Generate CSS:"""

        logger.debug(f"[G-1] Generating CSS for {len(missing_classes)} missing classes...")
        
        # LLMå‘¼ã³å‡ºã—
        try:
            result = self._generate_code_implementation(
                goal=prompt,
                complexity="simple",
                skip_validation=True,
                skip_multi_stage=True,
                force_filename=None
            )
            
            if result:
                # ç”Ÿæˆã•ã‚ŒãŸCSSã‚’æŠ½å‡º
                new_css = ""
                if isinstance(result, dict):
                    for content in result.values():
                        new_css = content
                        break
                elif isinstance(result, str):
                    new_css = result
                
                # CSSã‚³ãƒ¼ãƒ‰ã®ã¿ã‚’æŠ½å‡ºï¼ˆãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ç­‰ã‚’é™¤å»ï¼‰
                import re
                # ```css ... ``` ãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡º
                css_match = re.search(r'```(?:css)?\s*\n(.*?)```', new_css, re.DOTALL)
                if css_match:
                    new_css = css_match.group(1)
                
                # æ—¢å­˜CSSã«è¿½è¨˜
                if new_css.strip():
                    generated_files[css_file] = existing_css + "\n\n/* ========================================\n   Auto-generated: Missing CSS Classes (G-1)\n   ======================================== */\n" + new_css.strip()
                    logger.debug(f"[G-1] âœ… Added {len(new_css)} chars of CSS to {css_file}")
                else:
                    logger.debug("[G-1] âš  Generated CSS was empty")
            else:
                logger.debug("[G-1] âš  LLM returned no result")
                
        except Exception as e:
            logger.debug(f"[G-1] âš  Error generating CSS: {e}")
        
        return generated_files

    # ==========================================
    # G-2: HTML-JS IDæ•´åˆæ€§æ¤œè¨¼
    # ==========================================
    
    def _validate_html_js_id_consistency(
        self,
        generated_files: Dict[str, str]
    ) -> List[str]:
        """
        JSã§å‚ç…§ã•ã‚Œã¦ã„ã‚‹ãŒHTMLã«å­˜åœ¨ã—ãªã„IDã‚’æ¤œå‡º
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            HTMLã«å­˜åœ¨ã—ãªã„IDåãƒªã‚¹ãƒˆ
        """
        import re
        
        # 1. JSã‹ã‚‰å‚ç…§ã•ã‚Œã¦ã„ã‚‹IDã‚’æŠ½å‡º
        js_ids = set()
        for filepath, content in generated_files.items():
            if filepath.endswith(('.js', '.jsx', '.ts', '.tsx')):
                # getElementById('xxx') ã‚’æŠ½å‡º
                matches = re.findall(r'getElementById\(["\']([^"\']+)["\']\)', content)
                js_ids.update(matches)
                
                # querySelector('#xxx') ã‚’æŠ½å‡º
                matches = re.findall(r'querySelector\(["\']#([^"\'#\s\[]+)["\']', content)
                js_ids.update(matches)
                
                # document.forms.xxx ã‚„ document.xxx ã‚’æŠ½å‡ºï¼ˆä¸€èˆ¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
                matches = re.findall(r'document\.(?:forms\.)?([a-zA-Z_][a-zA-Z0-9_]*)\b', content)
                # ã“ã‚Œã‚‰ã¯IDã§ãªã„å¯èƒ½æ€§ãŒé«˜ã„ã®ã§ã‚¹ã‚­ãƒƒãƒ—
        
        if not js_ids:
            logger.debug("[G-2] No JS ID references found")
            return []
        
        # 2. HTMLã‹ã‚‰å…¨IDã‚’æŠ½å‡º
        html_ids = set()
        for filepath, content in generated_files.items():
            if filepath.endswith(('.html', '.htm')):
                # id="xxx" ã‚’æŠ½å‡º
                matches = re.findall(r'id=["\']([^"\']+)["\']', content, re.IGNORECASE)
                html_ids.update(matches)
        
        # 3. å·®åˆ†ã‚’æ¤œå‡º
        missing = js_ids - html_ids
        
        if missing:
            logger.debug(f"[G-2] Found {len(missing)} missing HTML IDs: {list(missing)[:10]}...")
        else:
            logger.debug("[G-2] All JS-referenced IDs exist in HTML")
        
        return list(missing)
    
    def _auto_complete_missing_html_ids(
        self,
        missing_ids: List[str],
        generated_files: Dict[str, str]
    ) -> Dict[str, str]:
        """
        JSã§å‚ç…§ã•ã‚Œã¦ã„ã‚‹ãŒå­˜åœ¨ã—ãªã„IDã‚’HTMLã«è¿½åŠ 
        
        Args:
            missing_ids: ä¸è¶³ã—ã¦ã„ã‚‹IDåã®ãƒªã‚¹ãƒˆ
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«
        
        Returns:
            æ›´æ–°ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«è¾æ›¸
        """
        if not missing_ids:
            return generated_files
        
        # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç‰¹å®š
        html_files = [f for f in generated_files.keys() if f.endswith(('.html', '.htm'))]
        if not html_files:
            logger.debug("[G-2] No HTML file found, skipping auto-completion")
            return generated_files
        
        html_file = html_files[0]  # æœ€åˆã®HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
        existing_html = generated_files[html_file]
        
        # JSã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆIDã®ä½¿ç”¨çŠ¶æ³ã‚’æŠŠæ¡ï¼‰
        js_context = ""
        for filepath, content in generated_files.items():
            if filepath.endswith(('.js', '.jsx')):
                import re
                for id_name in missing_ids[:15]:  # æœ€å¤§15å€‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
                    # IDãŒä½¿ã‚ã‚Œã¦ã„ã‚‹è¡Œã‚’æŠ½å‡º
                    pattern = rf'.*getElementById\(["\']({re.escape(id_name)})["\'].*'
                    matches = re.findall(pattern, content)
                    if matches:
                        # å‘¨è¾ºã‚³ãƒ¼ãƒ‰ã‚’å–å¾—
                        for line in content.split('\n'):
                            if id_name in line:
                                js_context += f"// {id_name}: {line.strip()[:100]}\n"
                                break
        
        # LLMãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        prompt = f"""Fix the HTML file by adding missing elements with these IDs that are referenced by JavaScript but don't exist in the HTML:

Missing IDs: {missing_ids}

JavaScript usage context:
{js_context}

Current HTML:
{existing_html}

Requirements:
1. Add the missing elements with the correct IDs in appropriate locations
2. Infer the element type from the ID name and JS usage:
   - *Btn, *Button â†’ <button>
   - *Input, *Field â†’ <input>
   - *Select, *Filter â†’ <select>
   - *Container, *Section â†’ <div>
   - *Table, *List â†’ appropriate container
   - *Modal â†’ modal structure
   - *Form â†’ <form>
   - *Page, *Count â†’ <span>
3. Place elements in logical locations based on their purpose
4. Maintain the existing HTML structure and styling
5. Return the COMPLETE updated HTML file

Generate the complete fixed HTML:"""

        logger.debug(f"[G-2] Generating HTML with {len(missing_ids)} missing IDs...")
        
        # LLMå‘¼ã³å‡ºã—
        try:
            result = self._generate_code_implementation(
                goal=prompt,
                complexity="medium",
                skip_validation=True,
                skip_multi_stage=True,
                force_filename=html_file
            )
            
            if result:
                new_html = ""
                if isinstance(result, dict):
                    new_html = result.get(html_file, "")
                    if not new_html:
                        for content in result.values():
                            new_html = content
                            break
                elif isinstance(result, str):
                    new_html = result
                
                # HTMLã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡ºï¼ˆãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ç­‰ã‚’é™¤å»ï¼‰
                import re
                html_match = re.search(r'```(?:html)?\s*\n(.*?)```', new_html, re.DOTALL)
                if html_match:
                    new_html = html_match.group(1)
                
                # æ–°ã—ã„HTMLãŒæœ‰åŠ¹ã‹æ¤œè¨¼
                if new_html.strip() and '<html' in new_html.lower() or '<!doctype' in new_html.lower() or '<body' in new_html.lower():
                    # è¿½åŠ ã•ã‚ŒãŸIDã‚’ç¢ºèª
                    added_ids = []
                    for id_name in missing_ids:
                        if f'id="{id_name}"' in new_html or f"id='{id_name}'" in new_html:
                            added_ids.append(id_name)
                    
                    if added_ids:
                        generated_files[html_file] = new_html.strip()
                        logger.debug(f"[G-2] âœ… Added {len(added_ids)} IDs to {html_file}: {added_ids[:5]}...")
                    else:
                        logger.debug("[G-2] âš  Generated HTML doesn't contain the missing IDs")
                else:
                    logger.debug("[G-2] âš  Generated HTML appears invalid, keeping original")
            else:
                logger.debug("[G-2] âš  LLM returned no result")
                
        except Exception as e:
            logger.debug(f"[G-2] âš  Error generating HTML: {e}")
        
        return generated_files

    # ==========================================
    # G-3: Pythonç’°å¢ƒ-ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸äº’æ›æ€§æ¤œè¨¼
    # ==========================================
    
    def _validate_python_package_compatibility(
        self,
        generated_files: Dict[str, str]
    ) -> List[Dict[str, str]]:
        """
        READMEè¨˜è¼‰ã®Pythonãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨requirements.txtå†…ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®äº’æ›æ€§ã‚’æ¤œè¨¼
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            äº’æ›æ€§å•é¡Œã®ãƒªã‚¹ãƒˆ [{package, current_version, issue, fix}]
        """
        import re
        import sys
        
        # 1. ã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒã‹ã‚‰Pythonãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’å–å¾—ï¼ˆå„ªå…ˆï¼‰
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å®Ÿè¡Œç’°å¢ƒã§å‹•ä½œã™ã‚‹å¿…è¦ãŒã‚ã‚‹ãŸã‚ã€ã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒã‚’å„ªå…ˆ
        python_version = f"{sys.version_info.major}.{sys.version_info.minor}"
        logger.debug(f"[G-3] System Python version: {python_version}")
        
        # READMEã‹ã‚‰ã‚‚å‚è€ƒã¨ã—ã¦å–å¾—ï¼ˆãƒ­ã‚°ç”¨ï¼‰
        readme_python_version = None
        for filepath, content in generated_files.items():
            if filepath.lower().endswith(('readme.md', 'readme.txt', 'readme')):
                # Python 3.13, Python 3.11+, Python 3.10+ ç­‰ã‚’æ¤œå‡º
                match = re.search(r'Python\s*(3\.1[0-9])', content, re.IGNORECASE)
                if match:
                    readme_python_version = match.group(1)
                    break
        
        if readme_python_version:
            logger.debug(f"[G-3] README Python version: {readme_python_version}")
        
        logger.debug(f"[G-3] Using Python version for compatibility check: {python_version}")
        
        # 2. requirements.txtã‹ã‚‰ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’æŠ½å‡º
        requirements_content = None
        for filepath, content in generated_files.items():
            if filepath.lower() == 'requirements.txt':
                requirements_content = content
                break
        
        if not requirements_content:
            logger.debug("[G-3] No requirements.txt found")
            return []
        
        # ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’æŠ½å‡º
        packages = {}
        for line in requirements_content.split('\n'):
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            # package==version, package>=version, package~=version
            match = re.match(r'^([a-zA-Z0-9_-]+)\s*[=<>~!]+\s*([0-9.]+)', line)
            if match:
                packages[match.group(1).lower()] = match.group(2)
        
        logger.debug(f"[G-3] Found {len(packages)} packages in requirements.txt")
        
        # 3. æ—¢çŸ¥ã®äº’æ›æ€§å•é¡Œã‚’ãƒã‚§ãƒƒã‚¯
        # Python 3.13ã¨ã®äº’æ›æ€§å•é¡Œãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
        COMPATIBILITY_ISSUES = {
            '3.13': {
                'sqlalchemy': {
                    'min_version': '2.0.36',
                    'issue': 'SQLAlchemy < 2.0.36 has __firstlineno__/__static_attributes__ conflict with Python 3.13',
                    'fix': '2.0.36'
                },
                'flask-sqlalchemy': {
                    'min_version': '3.1.1',
                    'issue': 'Older versions may have compatibility issues with Python 3.13',
                    'fix': '3.1.1'
                },
                'numpy': {
                    'min_version': '1.26.0',
                    'issue': 'NumPy < 1.26.0 not compatible with Python 3.13',
                    'fix': '1.26.4'
                },
                'pandas': {
                    'min_version': '2.1.0',
                    'issue': 'Pandas < 2.1.0 not compatible with Python 3.13',
                    'fix': '2.2.0'
                },
                'pydantic': {
                    'min_version': '2.5.0',
                    'issue': 'Pydantic < 2.5.0 may have issues with Python 3.13',
                    'fix': '2.6.0'
                },
            },
            '3.12': {
                'sqlalchemy': {
                    'min_version': '2.0.20',
                    'issue': 'SQLAlchemy < 2.0.20 has issues with Python 3.12',
                    'fix': '2.0.36'
                },
            }
        }
        
        issues = []
        
        # Pythonãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«å¯¾å¿œã™ã‚‹äº’æ›æ€§ãƒã‚§ãƒƒã‚¯
        version_key = python_version if python_version in COMPATIBILITY_ISSUES else None
        
        if version_key:
            known_issues = COMPATIBILITY_ISSUES[version_key]
            
            for pkg_name, pkg_version in packages.items():
                if pkg_name in known_issues:
                    issue_info = known_issues[pkg_name]
                    min_version = issue_info['min_version']
                    
                    # ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ¯”è¼ƒ
                    if self._compare_versions(pkg_version, min_version) < 0:
                        issues.append({
                            'package': pkg_name,
                            'current_version': pkg_version,
                            'min_version': min_version,
                            'issue': issue_info['issue'],
                            'fix': issue_info['fix']
                        })
                        logger.debug(f"[G-3] Incompatibility found: {pkg_name}=={pkg_version} (requires >= {min_version})")
        
        if issues:
            logger.debug(f"[G-3] Found {len(issues)} compatibility issues")
        else:
            logger.debug("[G-3] All packages are compatible with Python version")
        
        return issues
    
    def _compare_versions(self, version1: str, version2: str) -> int:
        """
        ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ–‡å­—åˆ—ã‚’æ¯”è¼ƒ
        
        Returns:
            -1: version1 < version2
             0: version1 == version2
             1: version1 > version2
        """
        def normalize(v):
            return [int(x) for x in re.sub(r'[^0-9.]', '', v).split('.')]
        
        import re
        v1_parts = normalize(version1)
        v2_parts = normalize(version2)
        
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
        max_len = max(len(v1_parts), len(v2_parts))
        v1_parts.extend([0] * (max_len - len(v1_parts)))
        v2_parts.extend([0] * (max_len - len(v2_parts)))
        
        for a, b in zip(v1_parts, v2_parts):
            if a < b:
                return -1
            elif a > b:
                return 1
        return 0
    
    def _auto_fix_package_compatibility(
        self,
        issues: List[Dict[str, str]],
        generated_files: Dict[str, str]
    ) -> Dict[str, str]:
        """
        ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸äº’æ›æ€§å•é¡Œã‚’è‡ªå‹•ä¿®æ­£
        
        Args:
            issues: äº’æ›æ€§å•é¡Œã®ãƒªã‚¹ãƒˆ
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«
        
        Returns:
            æ›´æ–°ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«è¾æ›¸
        """
        if not issues:
            return generated_files
        
        # requirements.txtã‚’ç‰¹å®š
        req_file = None
        req_content = None
        for filepath, content in generated_files.items():
            if filepath.lower() == 'requirements.txt':
                req_file = filepath
                req_content = content
                break
        
        if not req_file or not req_content:
            logger.debug("[G-3] No requirements.txt found for fixing")
            return generated_files
        
        # å„å•é¡Œã‚’ä¿®æ­£
        import re
        updated_content = req_content
        
        for issue in issues:
            pkg_name = issue['package']
            fix_version = issue['fix']
            
            # ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸è¡Œã‚’æ¤œç´¢ã—ã¦ç½®æ›
            # å¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã—ãªã„ãƒ‘ã‚¿ãƒ¼ãƒ³
            pattern = rf'^({re.escape(pkg_name)})\s*[=<>~!]+\s*[0-9.]+(.*)$'
            replacement = rf'\1=={fix_version}\2'
            
            updated_content = re.sub(
                pattern,
                replacement,
                updated_content,
                flags=re.MULTILINE | re.IGNORECASE
            )
            
            logger.debug(f"[G-3] Fixed: {pkg_name} -> {fix_version}")
        
        if updated_content != req_content:
            generated_files[req_file] = updated_content
            logger.debug(f"[G-3] âœ… Updated requirements.txt with {len(issues)} fixes")
        
        return generated_files

    # ==========================================
    # G-4: Pythonã‚·ãƒ³ãƒœãƒ«ãƒ¬ãƒ™ãƒ«importæ•´åˆæ€§æ¤œè¨¼
    # ==========================================
    
    def _validate_python_symbol_imports(
        self,
        generated_files: Dict[str, str]
    ) -> List[Dict[str, str]]:
        """
        Pythonãƒ•ã‚¡ã‚¤ãƒ«é–“ã®ã‚·ãƒ³ãƒœãƒ«ãƒ¬ãƒ™ãƒ«importæ•´åˆæ€§ã‚’æ¤œè¨¼
        
        from X import Y ã®å½¢å¼ã§ã€ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«Xã«ã‚·ãƒ³ãƒœãƒ«YãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            ä¸æ•´åˆã®ãƒªã‚¹ãƒˆ [{importer, module, symbol, available_symbols}]
        """
        import re
        
        issues = []
        
        # 1. ç”Ÿæˆã•ã‚ŒãŸPythonãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
        # filepath â†’ module_name
        module_map = {}
        for filepath in generated_files.keys():
            if not filepath.endswith('.py'):
                continue
            # models/product.py â†’ models.product, database.py â†’ database
            module_name = filepath.replace('/', '.').replace('\\', '.').replace('.py', '')
            # __init__.py ã¯ç‰¹åˆ¥å‡¦ç†
            if module_name.endswith('.__init__'):
                module_name = module_name.replace('.__init__', '')
            module_map[module_name] = filepath
            # çŸ­ç¸®åã‚‚ç™»éŒ²ï¼ˆdatabase.py â†’ databaseï¼‰
            short_name = filepath.replace('.py', '').split('/')[-1].split('\\')[-1]
            if short_name != '__init__':
                module_map[short_name] = filepath
        
        logger.debug(f"[G-4] Module map: {list(module_map.keys())}")
        
        # 2. å„ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚·ãƒ³ãƒœãƒ«å®šç¾©ã‚’æŠ½å‡º
        module_symbols = {}
        for filepath, content in generated_files.items():
            if not filepath.endswith('.py'):
                continue
            symbols = self._extract_module_symbols(content)
            module_symbols[filepath] = symbols
            logger.debug(f"[G-4] {filepath}: {len(symbols)} symbols")
        
        # 3. å„ãƒ•ã‚¡ã‚¤ãƒ«ã® from X import Y ã‚’è§£æ
        for filepath, content in generated_files.items():
            if not filepath.endswith('.py'):
                continue
            
            # from X import Y, Z ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º
            # from database import get_db, init_db
            # from models.product import Product
            # from .item_schema import ItemSchema  â† ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¯¾å¿œ
            # from ..models import User  â† è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¯¾å¿œ
            pattern = r'^from\s+([.a-zA-Z0-9_]+)\s+import\s+(.+)$'
            
            for line in content.split('\n'):
                line = line.strip()
                match = re.match(pattern, line)
                if not match:
                    continue
                
                module_name = match.group(1)
                imports_str = match.group(2)
                
                # ã‚³ãƒ¡ãƒ³ãƒˆã‚’é™¤å»
                if '#' in imports_str:
                    imports_str = imports_str.split('#')[0]
                
                # ã‚«ãƒƒã‚³å†…ã®ç¶™ç¶šè¡Œã¯ç°¡ç•¥åŒ–ã®ãŸã‚ç„¡è¦–
                if '(' in imports_str and ')' not in imports_str:
                    continue
                
                # ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚Œã‚‹ã‚·ãƒ³ãƒœãƒ«ã‚’æŠ½å‡º
                imports_str = imports_str.replace('(', '').replace(')', '')
                imported_symbols = [s.strip().split(' as ')[0].strip() for s in imports_str.split(',')]
                imported_symbols = [s for s in imported_symbols if s and s != '*']
                
                # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«å†…ã«å­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
                target_filepath = None
                
                # ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®å‡¦ç† (from .X import ... ã¾ãŸã¯ from ..X import ...)
                if module_name.startswith('.'):
                    # ã‚¤ãƒ³ãƒãƒ¼ãƒˆå…ƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—
                    if '/' in filepath:
                        importer_dir = '/'.join(filepath.split('/')[:-1])
                    elif '\\' in filepath:
                        importer_dir = '\\'.join(filepath.split('\\')[:-1])
                    else:
                        importer_dir = ''
                    
                    # ç›¸å¯¾ãƒ‘ã‚¹ã‚’è§£æ±º
                    # from .item_schema import ... â†’ åŒä¸€ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã® item_schema.py
                    # from ..models import ... â†’ è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã® models/__init__.py
                    relative_module = module_name.lstrip('.')
                    parent_count = len(module_name) - len(relative_module)
                    
                    # è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•
                    path_parts = importer_dir.split('/') if '/' in importer_dir else importer_dir.split('\\')
                    path_parts = [p for p in path_parts if p]  # ç©ºæ–‡å­—é™¤å»
                    
                    for _ in range(parent_count - 1):  # .ã¯åŒä¸€ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãªã®ã§-1
                        if path_parts:
                            path_parts.pop()
                    
                    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‘ã‚¹ã‚’æ§‹ç¯‰
                    if relative_module:
                        relative_parts = relative_module.replace('.', '/').split('/')
                        target_path = '/'.join(path_parts + relative_parts) + '.py'
                    else:
                        # from . import X ã®å ´åˆã¯ __init__.py
                        target_path = '/'.join(path_parts) + '/__init__.py' if path_parts else '__init__.py'
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
                    if target_path in generated_files:
                        target_filepath = target_path
                    else:
                        # __init__.py ã®å ´åˆã‚’è©¦ã™
                        init_path = target_path.replace('.py', '/__init__.py')
                        if init_path in generated_files:
                            target_filepath = init_path
                    
                    if target_filepath:
                        logger.debug(f"[G-4] Resolved relative import: {filepath} -> {module_name} -> {target_filepath}")
                else:
                    # çµ¶å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®å‡¦ç†ï¼ˆå¾“æ¥ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
                    for mod_name, mod_path in module_map.items():
                        if module_name == mod_name or module_name.endswith('.' + mod_name):
                            target_filepath = mod_path
                            break
                
                if not target_filepath:
                    # å¤–éƒ¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆæ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ï¼‰ã¯ã‚¹ã‚­ãƒƒãƒ—
                    continue
                
                # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚·ãƒ³ãƒœãƒ«ã‚’å–å¾—
                available_symbols = module_symbols.get(target_filepath, set())
                
                # å„ã‚·ãƒ³ãƒœãƒ«ã®å­˜åœ¨ç¢ºèª
                for symbol in imported_symbols:
                    if symbol not in available_symbols:
                        issues.append({
                            'importer': filepath,
                            'module': module_name,
                            'module_path': target_filepath,
                            'symbol': symbol,
                            'available_symbols': list(available_symbols)[:10],  # æœ€å¤§10å€‹
                            'import_line': line
                        })
                        logger.debug(f"[G-4] Missing symbol: {filepath} imports '{symbol}' from {module_name}, but not found in {target_filepath}")
        
        if issues:
            logger.debug(f"[G-4] Found {len(issues)} symbol import issues")
        else:
            logger.debug("[G-4] All symbol imports are valid")
        
        return issues
    
    def _extract_module_symbols(self, content: str) -> set:
        """
        Pythonãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰å®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‚·ãƒ³ãƒœãƒ«ã‚’æŠ½å‡º
        
        Args:
            content: Pythonãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹
        
        Returns:
            ã‚·ãƒ³ãƒœãƒ«åã®ã‚»ãƒƒãƒˆ (é–¢æ•°åã€ã‚¯ãƒ©ã‚¹åã€å¤‰æ•°å)
        """
        import re
        
        symbols = set()
        
        # é–¢æ•°å®šç¾©: def function_name(
        func_pattern = r'^def\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*\('
        for match in re.finditer(func_pattern, content, re.MULTILINE):
            symbols.add(match.group(1))
        
        # ã‚¯ãƒ©ã‚¹å®šç¾©: class ClassName
        class_pattern = r'^class\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*[:\(]'
        for match in re.finditer(class_pattern, content, re.MULTILINE):
            symbols.add(match.group(1))
        
        # ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«å¤‰æ•°ä»£å…¥: VARIABLE = value (å¤§æ–‡å­—ã®å®šæ•°)
        const_pattern = r'^([A-Z][A-Z0-9_]*)\s*='
        for match in re.finditer(const_pattern, content, re.MULTILINE):
            symbols.add(match.group(1))
        
        # ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«å¤‰æ•°ä»£å…¥: variable = value (å°æ–‡å­—ã‚‚å«ã‚€ã€ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆãªã—)
        var_pattern = r'^([a-zA-Z_][a-zA-Z0-9_]*)\s*='
        for match in re.finditer(var_pattern, content, re.MULTILINE):
            # ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã•ã‚Œã¦ã„ãªã„è¡Œã®ã¿ï¼ˆãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ï¼‰
            symbols.add(match.group(1))
        
        # __all__ ã§æ˜ç¤ºçš„ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã‚·ãƒ³ãƒœãƒ«
        all_pattern = r'__all__\s*=\s*\[(.*?)\]'
        all_match = re.search(all_pattern, content, re.DOTALL)
        if all_match:
            all_content = all_match.group(1)
            exported = re.findall(r'["\']([a-zA-Z_][a-zA-Z0-9_]*)["\']', all_content)
            symbols.update(exported)
        
        return symbols
    
    def _auto_fix_symbol_imports(
        self,
        issues: List[Dict[str, str]],
        generated_files: Dict[str, str]
    ) -> Dict[str, str]:
        """
        ã‚·ãƒ³ãƒœãƒ«importä¸æ•´åˆã‚’è‡ªå‹•ä¿®æ­£
        
        æ–¹é‡: ä¸è¶³ã—ã¦ã„ã‚‹ã‚·ãƒ³ãƒœãƒ«ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«è¿½åŠ ã™ã‚‹
              ï¼ˆã‚¨ã‚¤ãƒªã‚¢ã‚¹é–¢æ•°ã¨ã—ã¦æ—¢å­˜ã®é¡ä¼¼é–¢æ•°ã‚’å‚ç…§ï¼‰
        
        Args:
            issues: ä¸æ•´åˆã®ãƒªã‚¹ãƒˆ
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«
        
        Returns:
            æ›´æ–°ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«è¾æ›¸
        """
        if not issues:
            return generated_files
        
        # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã”ã¨ã«ä¸è¶³ã‚·ãƒ³ãƒœãƒ«ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        module_fixes = {}
        for issue in issues:
            module_path = issue['module_path']
            if module_path not in module_fixes:
                module_fixes[module_path] = []
            module_fixes[module_path].append(issue)
        
        # å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«ä¸è¶³ã‚·ãƒ³ãƒœãƒ«ã‚’è¿½åŠ 
        for module_path, module_issues in module_fixes.items():
            if module_path not in generated_files:
                continue
            
            content = generated_files[module_path]
            additions = []
            
            for issue in module_issues:
                symbol = issue['symbol']
                available = issue['available_symbols']
                
                # é¡ä¼¼ã‚·ãƒ³ãƒœãƒ«ã‚’æ¢ã™ï¼ˆã‚¨ã‚¤ãƒªã‚¢ã‚¹å€™è£œï¼‰
                alias_target = None
                
                # get_db â†’ get_session, get_db_session ãªã©ã‚’æ¢ã™
                for avail in available:
                    # å®Œå…¨ä¸€è‡´ã®æ¥é ­è¾/æ¥å°¾è¾
                    if symbol in avail or avail in symbol:
                        alias_target = avail
                        break
                    # get_X ã¨ get_X_session ã®é–¢ä¿‚
                    if symbol.replace('_', '') in avail.replace('_', ''):
                        alias_target = avail
                        break
                
                if alias_target:
                    # ã‚¨ã‚¤ãƒªã‚¢ã‚¹é–¢æ•°ã‚’è¿½åŠ 
                    additions.append(f"\n\n# Auto-generated alias for compatibility (G-4)\ndef {symbol}(*args, **kwargs):\n    \"\"\"Alias for {alias_target}\"\"\"\n    return {alias_target}(*args, **kwargs)\n")
                    logger.debug(f"[G-4] Adding alias: {symbol} -> {alias_target} in {module_path}")
                else:
                    # é¡ä¼¼ã‚·ãƒ³ãƒœãƒ«ãŒãªã„å ´åˆã¯ã‚¹ã‚¿ãƒ–é–¢æ•°ã‚’è¿½åŠ 
                    additions.append(f"\n\n# Auto-generated stub (G-4) - TODO: Implement\ndef {symbol}(*args, **kwargs):\n    \"\"\"Auto-generated stub. Implement this function.\"\"\"\n    raise NotImplementedError('{symbol} is not implemented')\n")
                    logger.debug(f"[G-4] Adding stub: {symbol} in {module_path}")
            
            if additions:
                generated_files[module_path] = content + ''.join(additions)
                logger.debug(f"[G-4] âœ… Added {len(additions)} symbols to {module_path}")
        
        return generated_files

    # G-5: Import vs Requirements.txt æ•´åˆæ€§æ¤œè¨¼
    # =========================================
    
    def _validate_import_requirements_consistency(
        self,
        generated_files: Dict[str, str]
    ) -> List[Dict[str, str]]:
        """
        Pythonãƒ•ã‚¡ã‚¤ãƒ«å†…ã®importæ–‡ã¨requirements.txtã®æ•´åˆæ€§ã‚’æ¤œè¨¼
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            ä¸è¶³ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ [{import_name, package_name, recommended_version, source_file}]
        """
        import re
        
        # importå â†’ PyPIãƒ‘ãƒƒã‚±ãƒ¼ã‚¸åã®ãƒãƒƒãƒ”ãƒ³ã‚°
        IMPORT_TO_PACKAGE = {
            'flask': 'Flask',
            'flask_sqlalchemy': 'Flask-SQLAlchemy',
            'flask_cors': 'Flask-CORS',
            'flask_login': 'Flask-Login',
            'flask_migrate': 'Flask-Migrate',
            'flask_wtf': 'Flask-WTF',
            'flask_restful': 'Flask-RESTful',
            'flask_socketio': 'Flask-SocketIO',
            'sqlalchemy': 'SQLAlchemy',
            'marshmallow': 'marshmallow',
            'dotenv': 'python-dotenv',
            'werkzeug': 'Werkzeug',
            'jinja2': 'Jinja2',
            'requests': 'requests',
            'numpy': 'numpy',
            'pandas': 'pandas',
            'PIL': 'Pillow',
            'cv2': 'opencv-python',
            'sklearn': 'scikit-learn',
            'yaml': 'PyYAML',
            'jwt': 'PyJWT',
            'bcrypt': 'bcrypt',
            'passlib': 'passlib',
            'celery': 'celery',
            'redis': 'redis',
            'pymongo': 'pymongo',
            'httpx': 'httpx',
            'aiohttp': 'aiohttp',
            'pydantic': 'pydantic',
            'fastapi': 'fastapi',
            'uvicorn': 'uvicorn',
            'gunicorn': 'gunicorn',
            'pytest': 'pytest',
            'eventlet': 'eventlet',
        }
        
        # æ¨å¥¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼ˆPython 3.13å¯¾å¿œï¼‰
        RECOMMENDED_VERSIONS = {
            'Flask': '3.0.0',
            'Flask-SQLAlchemy': '3.1.1',
            'Flask-CORS': '4.0.0',
            'Flask-Login': '0.6.3',
            'Flask-Migrate': '4.0.5',
            'Flask-WTF': '1.2.1',
            'Flask-RESTful': '0.3.10',
            'Flask-SocketIO': '5.3.6',
            'SQLAlchemy': '2.0.36',
            'marshmallow': '3.21.0',
            'python-dotenv': '1.0.0',
            'Werkzeug': '3.0.1',
            'Jinja2': '3.1.2',
            'requests': '2.31.0',
            'numpy': '1.26.4',
            'pandas': '2.2.0',
            'Pillow': '10.2.0',
            'PyYAML': '6.0.1',
            'PyJWT': '2.8.0',
            'bcrypt': '4.1.2',
            'pydantic': '2.6.0',
            'pytest': '8.0.0',
            'eventlet': '0.35.1',
        }
        
        # æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆé™¤å¤–å¯¾è±¡ï¼‰
        STDLIB_MODULES = {
            'os', 'sys', 'json', 'datetime', 'time', 'math', 'random', 're',
            'collections', 'itertools', 'functools', 'typing', 'enum', 'uuid',
            'hashlib', 'base64', 'copy', 'io', 'pathlib', 'logging', 'traceback',
            'contextlib', 'dataclasses', 'abc', 'threading', 'multiprocessing',
            'subprocess', 'shutil', 'tempfile', 'glob', 'fnmatch', 'pickle',
            'csv', 'html', 'urllib', 'http', 'email', 'mimetypes', 'socket',
            'ssl', 'asyncio', 'concurrent', 'queue', 'struct', 'codecs',
            'locale', 'gettext', 'argparse', 'configparser', 'secrets',
            'statistics', 'decimal', 'fractions', 'numbers', 'cmath',
            'operator', 'string', 'textwrap', 'unicodedata', 'difflib',
            'heapq', 'bisect', 'array', 'weakref', 'types', 'pprint',
            'reprlib', 'warnings', 'contextlib', 'atexit', 'tracemalloc',
            'gc', 'inspect', 'dis', 'ast', 'symtable', 'token', 'keyword',
            'tokenize', 'tabnanny', 'pyclbr', 'compileall', 'zipimport',
            'importlib', 'runpy', 'builtins', 'site', 'sysconfig', 'platform',
            'errno', 'signal', 'select', 'selectors', 'mmap', 'fcntl',
            'posix', 'nt', 'posixpath', 'ntpath', 'genericpath', 'stat',
            'filecmp', 'tarfile', 'zipfile', 'gzip', 'bz2', 'lzma', 'zlib',
            'xml', 'xmlrpc', 'ipaddress', 'ftplib', 'poplib', 'imaplib',
            'nntplib', 'smtplib', 'smtpd', 'telnetlib', 'socketserver',
            'webbrowser', 'cgi', 'cgitb', 'wsgiref', 'unittest', 'doctest',
            'pdb', 'profile', 'cProfile', 'timeit', 'trace', 'faulthandler',
            'cmd', 'shlex', 'code', 'codeop', 'rlcompleter', 'tty', 'pty',
            'termios', 'curses', 'readline', 'getpass', 'netrc', 'ctypes',
            'test', '__future__', 'sqlite3', 'dbm', 'shelve', 'marshal',
        }
        
        # 1. requirements.txtã‹ã‚‰æ—¢å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’æŠ½å‡º
        requirements_content = None
        for filepath, content in generated_files.items():
            if filepath.lower() == 'requirements.txt':
                requirements_content = content
                break
        
        if not requirements_content:
            logger.debug("[G-5] No requirements.txt found, skipping check")
            return []
        
        # æ—¢å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’å°æ–‡å­—ã§æ­£è¦åŒ–ã—ã¦ä¿å­˜
        existing_packages = set()
        for line in requirements_content.split('\n'):
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            # package==version, package>=version, package~=version, package
            match = re.match(r'^([a-zA-Z0-9_-]+)', line)
            if match:
                existing_packages.add(match.group(1).lower().replace('-', '_'))
        
        logger.debug(f"[G-5] Existing packages in requirements.txt: {existing_packages}")
        
        # 2. Pythonãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰importæ–‡ã‚’æŠ½å‡º
        imported_modules = {}  # {import_name: source_file}
        
        for filepath, content in generated_files.items():
            if not filepath.endswith('.py'):
                continue
            
            # import X / from X import Y
            import_pattern = re.compile(r'^(?:from\s+([a-zA-Z_][a-zA-Z0-9_]*)|import\s+([a-zA-Z_][a-zA-Z0-9_]*))', re.MULTILINE)
            
            for match in import_pattern.finditer(content):
                module = match.group(1) or match.group(2)
                if module:
                    # ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã¿ï¼ˆflask_cors â†’ flask_corsï¼‰
                    top_module = module.split('.')[0]
                    
                    # æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯é™¤å¤–
                    if top_module in STDLIB_MODULES:
                        continue
                    
                    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯é™¤å¤–
                    is_internal = False
                    for internal_path in generated_files.keys():
                        if internal_path.endswith('.py'):
                            internal_module = internal_path.replace('/', '.').replace('\\', '.').replace('.py', '')
                            if top_module == internal_module or internal_module.startswith(top_module + '.') or top_module == internal_module.split('.')[-1]:
                                is_internal = True
                                break
                    
                    if not is_internal:
                        if top_module not in imported_modules:
                            imported_modules[top_module] = filepath
        
        logger.debug(f"[G-5] Imported external modules: {list(imported_modules.keys())}")
        
        # 3. ä¸è¶³ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ç‰¹å®š
        missing_packages = []
        
        for import_name, source_file in imported_modules.items():
            # importåã‚’æ­£è¦åŒ–ï¼ˆflask_cors â†’ flask_corsï¼‰
            normalized_import = import_name.lower().replace('-', '_')
            
            # æ—¢ã«requirements.txtã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if normalized_import in existing_packages:
                continue
            
            # ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸åã«å¤‰æ›ï¼ˆflask_cors â†’ Flask-CORSï¼‰
            package_name = IMPORT_TO_PACKAGE.get(normalized_import, import_name)
            package_normalized = package_name.lower().replace('-', '_')
            
            # ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸åã§ã‚‚å†ãƒã‚§ãƒƒã‚¯
            if package_normalized in existing_packages:
                continue
            
            # æ¨å¥¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’å–å¾—
            recommended_version = RECOMMENDED_VERSIONS.get(package_name, '')
            
            missing_packages.append({
                'import_name': import_name,
                'package_name': package_name,
                'recommended_version': recommended_version,
                'source_file': source_file
            })
            
            logger.debug(f"[G-5] Missing package: {import_name} â†’ {package_name} (from {source_file})")
        
        return missing_packages
    
    def _auto_fix_requirements_txt(
        self,
        missing_packages: List[Dict[str, str]],
        generated_files: Dict[str, str]
    ) -> Dict[str, str]:
        """
        ä¸è¶³ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’requirements.txtã«è¿½åŠ 
        
        Args:
            missing_packages: ä¸è¶³ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«
        
        Returns:
            æ›´æ–°ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«è¾æ›¸
        """
        if not missing_packages:
            return generated_files
        
        # requirements.txtã‚’æ¢ã™
        requirements_path = None
        requirements_content = None
        for filepath, content in generated_files.items():
            if filepath.lower() == 'requirements.txt':
                requirements_path = filepath
                requirements_content = content
                break
        
        if not requirements_path:
            return generated_files
        
        # è¿½åŠ ã™ã‚‹ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’æ§‹ç¯‰
        additions = []
        for pkg in missing_packages:
            package_name = pkg['package_name']
            version = pkg['recommended_version']
            
            if version:
                additions.append(f"{package_name}=={version}")
            else:
                additions.append(package_name)
            
            logger.debug(f"[G-5] Adding to requirements.txt: {package_name}=={version if version else '(latest)'}")
        
        # requirements.txtã«è¿½åŠ 
        if additions:
            # æœ«å°¾ã«æ”¹è¡ŒãŒãªã‘ã‚Œã°è¿½åŠ 
            if not requirements_content.endswith('\n'):
                requirements_content += '\n'
            
            # ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’è¿½åŠ 
            requirements_content += '\n'.join(additions) + '\n'
            
            generated_files[requirements_path] = requirements_content
            logger.debug(f"[G-5] âœ… Added {len(additions)} packages to requirements.txt")
        
        return generated_files

    # G-6: ãƒ©ã‚¤ãƒ–ãƒ©ãƒªè¦æ±‚ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œè¨¼
    # =========================================
    
    def _validate_library_requirements(
        self,
        generated_files: Dict[str, str]
    ) -> List[Dict[str, Any]]:
        """
        ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦æ±‚ã™ã‚‹å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œè¨¼
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã®ãƒªã‚¹ãƒˆ [{library, requirement, file, details}]
        """
        import re
        
        issues = []
        
        # å…¨Pythonãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’çµåˆã—ã¦æ¤œç´¢ç”¨ã«ä¿æŒ
        all_python_content = {}
        for filepath, content in generated_files.items():
            if filepath.endswith('.py'):
                all_python_content[filepath] = content
        
        if not all_python_content:
            return issues
        
        # ========================================
        # Check 1: Flask-Login UserMixinç¶™æ‰¿
        # ========================================
        flask_login_used = False
        user_model_file = None
        user_model_has_mixin = False
        
        for filepath, content in all_python_content.items():
            # Flask-Loginã®ä½¿ç”¨ã‚’æ¤œå‡º
            if re.search(r'from\s+flask_login\s+import|import\s+flask_login', content):
                flask_login_used = True
            if re.search(r'login_user|logout_user|current_user|login_required|LoginManager', content):
                flask_login_used = True
            
            # Userãƒ¢ãƒ‡ãƒ«ã‚’æ¤œå‡º
            if re.search(r'class\s+User\s*\(', content):
                user_model_file = filepath
                # UserMixinç¶™æ‰¿ã‚’ç¢ºèª
                if re.search(r'class\s+User\s*\([^)]*UserMixin[^)]*\)', content):
                    user_model_has_mixin = True
                # UserMixinã®importã‚‚ç¢ºèª
                if re.search(r'from\s+flask_login\s+import[^)]*UserMixin', content):
                    user_model_has_mixin = user_model_has_mixin  # importæœ‰ç„¡ã ã‘ã§ã¯åˆ¤æ–­ã—ãªã„
        
        if flask_login_used and user_model_file and not user_model_has_mixin:
            issues.append({
                'library': 'Flask-Login',
                'requirement': 'UserMixin inheritance',
                'file': user_model_file,
                'details': 'User model must inherit from UserMixin for Flask-Login to work',
                'fix_type': 'add_mixin',
                'pattern': {
                    'search': r'class\s+User\s*\(([^)]*)\)',
                    'mixin': 'UserMixin',
                    'import_line': 'from flask_login import UserMixin'
                }
            })
            logger.debug(f"[G-6] Flask-Login: UserMixin not inherited in {user_model_file}")
        
        # ========================================
        # Check 2: Flask-Login user_loaderå®šç¾©
        # ========================================
        login_manager_used = False
        user_loader_defined = False
        login_manager_file = None  # LoginManagerãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«
        app_file = None  # app.pyã¾ãŸã¯main.py
        
        for filepath, content in all_python_content.items():
            if re.search(r'LoginManager\s*\(\s*\)|login_manager\s*=', content):
                login_manager_used = True
                login_manager_file = filepath  # LoginManagerãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¨˜éŒ²
                if 'main' in filepath.lower() or 'app' in filepath.lower():
                    app_file = filepath
            if re.search(r'@\s*login_manager\.user_loader|@\s*\w+\.user_loader', content):
                user_loader_defined = True
        
        if login_manager_used and not user_loader_defined:
            # ä¿®æ­£å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã®å„ªå…ˆé †ä½:
            # 1. LoginManagerãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆuser_loaderã¯LoginManagerã¨åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«ã«å®šç¾©ã™ã¹ãï¼‰
            # 2. app.py ã¾ãŸã¯ main.py
            # 3. ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€åˆã«è¦‹ã¤ã‹ã£ãŸPythonãƒ•ã‚¡ã‚¤ãƒ«
            if login_manager_file:
                target_file = login_manager_file
            elif app_file:
                target_file = app_file
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: generated_filesã‹ã‚‰app.pyã¾ãŸã¯main.pyã‚’æ¢ã™
                for fp in all_python_content.keys():
                    if 'app.py' in fp.lower():
                        target_file = fp
                        break
                    elif 'main.py' in fp.lower():
                        target_file = fp
                        break
                else:
                    target_file = list(all_python_content.keys())[0] if all_python_content else 'app.py'
            
            issues.append({
                'library': 'Flask-Login',
                'requirement': 'user_loader callback',
                'file': target_file,
                'details': '@login_manager.user_loader callback must be defined',
                'fix_type': 'add_user_loader'
            })
            logger.debug(f"[G-6] Flask-Login: user_loader not defined")
        
        # ========================================
        # Check 3: SECRET_KEYè¨­å®š - å‰Šé™¤
        # ========================================
        # ç†ç”±: ã€ŒSECRET_KEYãŒå­˜åœ¨ã™ã‚‹ã‹ã€ã®ãƒã‚§ãƒƒã‚¯ã¯ã€Œãƒ©ã‚¤ãƒ–ãƒ©ãƒªè¦æ±‚ãƒ‘ã‚¿ãƒ¼ãƒ³ã€ã®è²¬å‹™å¤–
        # SECRET_KEYã®å•é¡Œï¼ˆã‚¯ãƒ©ã‚¹å®šç¾©æ™‚ã®raiseç­‰ï¼‰ã¯G-10ã§æ¤œå‡ºã™ã‚‹
        # å‚ç…§: g_checks_structural_analysis.md
        
        # ========================================
        # Check 4: Flask-WTF FlaskFormç¶™æ‰¿
        # ========================================
        flask_wtf_used = False
        form_files_without_inheritance = []
        
        for filepath, content in all_python_content.items():
            if re.search(r'from\s+flask_wtf\s+import|import\s+flask_wtf', content):
                flask_wtf_used = True
            
            # Formã‚¯ãƒ©ã‚¹å®šç¾©ã‚’æ¤œå‡ºï¼ˆFlaskFormã‚’ç¶™æ‰¿ã—ã¦ã„ãªã„ã‚‚ã®ï¼‰
            form_classes = re.findall(r'class\s+(\w+Form)\s*\(([^)]*)\)', content)
            for class_name, parents in form_classes:
                if 'FlaskForm' not in parents and 'Form' in class_name:
                    form_files_without_inheritance.append({
                        'file': filepath,
                        'class': class_name,
                        'parents': parents
                    })
        
        if flask_wtf_used and form_files_without_inheritance:
            for form_info in form_files_without_inheritance:
                issues.append({
                    'library': 'Flask-WTF',
                    'requirement': 'FlaskForm inheritance',
                    'file': form_info['file'],
                    'details': f"Form class '{form_info['class']}' should inherit from FlaskForm",
                    'fix_type': 'add_form_inheritance',
                    'class_name': form_info['class']
                })
                logger.debug(f"[G-6] Flask-WTF: {form_info['class']} not inheriting FlaskForm")
        
        # ========================================
        # Check 5: SQLAlchemy back_populatesåŒæ–¹å‘æ•´åˆæ€§
        # ========================================
        relationships = []  # [(file, model, field, back_populates_target)]
        
        for filepath, content in all_python_content.items():
            # relationshipå®šç¾©ã‚’æŠ½å‡º
            # relationship("TargetModel", back_populates="field_name")
            pattern = r'(\w+):\s*Mapped\[.*?\]\s*=\s*relationship\s*\(\s*["\'](\w+)["\'].*?back_populates\s*=\s*["\'](\w+)["\']'
            matches = re.findall(pattern, content, re.DOTALL)
            
            for field_name, target_model, back_populates_field in matches:
                # ã‚¯ãƒ©ã‚¹åã‚’å–å¾—
                class_match = re.search(r'class\s+(\w+)\s*\([^)]*\):', content[:content.find(field_name)])
                if class_match:
                    model_name = class_match.group(1)
                    relationships.append({
                        'file': filepath,
                        'model': model_name,
                        'field': field_name,
                        'target_model': target_model,
                        'back_populates': back_populates_field
                    })
        
        # åŒæ–¹å‘ãƒã‚§ãƒƒã‚¯
        for rel in relationships:
            # å¯¾å¿œã™ã‚‹back_populatesãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
            counterpart_found = False
            for other_rel in relationships:
                if (other_rel['model'] == rel['target_model'] and 
                    other_rel['field'] == rel['back_populates'] and
                    other_rel['target_model'] == rel['model']):
                    counterpart_found = True
                    break
            
            if not counterpart_found:
                # å¯¾è±¡ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
                target_file = None
                for filepath, content in all_python_content.items():
                    if re.search(rf'class\s+{rel["target_model"]}\s*\(', content):
                        target_file = filepath
                        break
                
                if target_file:
                    issues.append({
                        'library': 'SQLAlchemy',
                        'requirement': 'back_populates bidirectional',
                        'file': target_file,
                        'details': f"Model '{rel['target_model']}' missing back_populates='{rel['back_populates']}' for relationship with '{rel['model']}'",
                        'fix_type': 'add_back_populates',
                        'source_model': rel['model'],
                        'target_model': rel['target_model'],
                        'field_name': rel['back_populates']
                    })
                    logger.debug(f"[G-6] SQLAlchemy: Missing back_populates in {rel['target_model']}")
        
        if issues:
            logger.debug(f"[G-6] Found {len(issues)} library requirement issues")
        else:
            logger.debug("[G-6] All library requirements satisfied")
        
        return issues
    
    def _auto_fix_library_requirements(
        self,
        issues: List[Dict[str, Any]],
        generated_files: Dict[str, str]
    ) -> Dict[str, str]:
        """
        ãƒ©ã‚¤ãƒ–ãƒ©ãƒªè¦æ±‚ãƒ‘ã‚¿ãƒ¼ãƒ³å•é¡Œã‚’LLMã§ä¿®æ­£
        
        Args:
            issues: æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã®ãƒªã‚¹ãƒˆ
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«
        
        Returns:
            æ›´æ–°ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«è¾æ›¸
        """
        if not issues:
            return generated_files
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«å•é¡Œã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        issues_by_file = {}
        for issue in issues:
            filepath = issue['file']
            if filepath not in issues_by_file:
                issues_by_file[filepath] = []
            issues_by_file[filepath].append(issue)
        
        # å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿®æ­£
        for filepath, file_issues in issues_by_file.items():
            if filepath not in generated_files:
                logger.debug(f"[G-6] File not found: {filepath}")
                continue
            
            current_code = generated_files[filepath]
            
            # å•é¡Œã®è©³ç´°ã‚’æ•´å½¢
            issue_descriptions = []
            for i, issue in enumerate(file_issues, 1):
                issue_descriptions.append(
                    f"{i}. [{issue['library']}] {issue['requirement']}\n"
                    f"   Details: {issue['details']}"
                )
            
            issues_text = "\n".join(issue_descriptions)
            
            # ä¿®æ­£ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
            fix_prompt = f"""You are fixing library requirement issues in a Flask application.

## File: {filepath}

## Issues to Fix
{issues_text}

## Current Code
```python
{current_code}
```

## Fix Instructions
1. For UserMixin inheritance: Add 'from flask_login import UserMixin' and add UserMixin to the class inheritance
   - Example: class User(UserMixin, db.Model):
2. For user_loader: Add @login_manager.user_loader decorator with proper callback function
3. For SECRET_KEY: Add SECRET_KEY configuration in the Config class
4. For FlaskForm: Change Form inheritance to FlaskForm and add proper import
5. For back_populates: Add the missing relationship with correct back_populates reference

## Output
Return ONLY the complete fixed Python code. Do not include explanations.
Do not change anything else in the code - only fix the specific issues listed above.

```python
"""
            
            try:
                logger.debug(f"[G-6] Fixing {len(file_issues)} issues in {filepath}...")
                
                response = self._call_llm_api(fix_prompt)
                
                if response:
                    # Pythonã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º
                    import re
                    code_match = re.search(r'```python\s*\n(.*?)```', response, re.DOTALL)
                    if code_match:
                        fixed_code = code_match.group(1).strip()
                    else:
                        # ```python ãŒãªã„å ´åˆã¯å…¨ä½“ã‚’ã‚³ãƒ¼ãƒ‰ã¨ã—ã¦æ‰±ã†
                        fixed_code = response.strip()
                        # å…ˆé ­ã®```pythonã¨æœ«å°¾ã®```ã‚’é™¤å»
                        if fixed_code.startswith('```'):
                            fixed_code = re.sub(r'^```\w*\n?', '', fixed_code)
                        if fixed_code.endswith('```'):
                            fixed_code = fixed_code[:-3].strip()
                    
                    if fixed_code and len(fixed_code) > 100:  # æœ€ä½é™ã®ã‚³ãƒ¼ãƒ‰é•·ãƒã‚§ãƒƒã‚¯
                        generated_files[filepath] = fixed_code
                        logger.debug(f"[G-6] âœ… Fixed {filepath}")
                    else:
                        logger.debug(f"[G-6] âš  Generated code too short for {filepath}")
                else:
                    logger.debug(f"[G-6] âš  No response from LLM for {filepath}")
                    
            except Exception as e:
                logger.debug(f"[G-6] âš  Error fixing {filepath}: {e}")
        
        # G-6ã§ä¿®æ­£ã—ãŸãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’requirements.txtã«ã‚‚è¿½åŠ 
        # å¯¾è±¡: Flask-Login, Flask-WTF, Flask-Bcrypt
        libraries_fixed = {
            issue.get('library') 
            for issue in issues 
            if issue.get('library')
        }
        
        if libraries_fixed:
            # requirements.txtã‚’æ¢ã™
            requirements_path = None
            requirements_content = None
            for fp, content in generated_files.items():
                if fp.lower() == 'requirements.txt':
                    requirements_path = fp
                    requirements_content = content
                    break
            
            if requirements_path and requirements_content:
                # æ—¢å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’æŠ½å‡º
                import re
                existing_packages = set()
                for line in requirements_content.split('\n'):
                    line = line.strip()
                    if not line or line.startswith('#'):
                        continue
                    match = re.match(r'^([a-zA-Z0-9_-]+)', line)
                    if match:
                        existing_packages.add(match.group(1).lower().replace('-', '_'))
                
                # ä¿®æ­£ã•ã‚ŒãŸãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«å¿œã˜ã¦requirements.txtã«è¿½åŠ 
                libraries_to_add = []
                
                # Flask-Login
                if 'Flask-Login' in libraries_fixed and 'flask_login' not in existing_packages:
                    libraries_to_add.append('Flask-Login==0.6.3')
                
                # Flask-WTF
                if 'Flask-WTF' in libraries_fixed and 'flask_wtf' not in existing_packages:
                    libraries_to_add.append('Flask-WTF==1.2.1')
                
                # è¿½åŠ å®Ÿè¡Œ
                if libraries_to_add:
                    if not requirements_content.endswith('\n'):
                        requirements_content += '\n'
                    requirements_content += '\n'.join(libraries_to_add) + '\n'
                    generated_files[requirements_path] = requirements_content
                    logger.debug(f"[G-6] âœ… Added {len(libraries_to_add)} packages to requirements.txt: {', '.join(libraries_to_add)}")
        
        return generated_files

    # G-7: ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ä¾å­˜æ•´åˆæ€§æ¤œè¨¼
    # =========================================
    
    def _validate_frontend_dependencies(
        self,
        generated_files: Dict[str, str]
    ) -> List[Dict[str, Any]]:
        """
        ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ä¾å­˜ã®æ•´åˆæ€§ã‚’æ¤œè¨¼
        - JS import â†” package.json
        - HTML script/link â†” ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨
        - CSS @import â†” ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã®ãƒªã‚¹ãƒˆ [{type, file, dependency, details}]
        """
        import re
        
        issues = []
        
        # ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’ã‚»ãƒƒãƒˆã«
        generated_paths = set(generated_files.keys())
        # å°æ–‡å­—åŒ–ã—ãŸãƒ‘ã‚¹ã‚‚ç”¨æ„ï¼ˆå¤§æ–‡å­—å°æ–‡å­—ã‚’ç„¡è¦–ã—ãŸæ¯”è¼ƒç”¨ï¼‰
        generated_paths_lower = {p.lower() for p in generated_paths}
        
        # ========================================
        # Check 1: HTML script/linkå‚ç…§ â†” ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨
        # ========================================
        for filepath, content in generated_files.items():
            if not filepath.endswith('.html'):
                continue
            
            # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—ï¼ˆç›¸å¯¾ãƒ‘ã‚¹è§£æ±ºç”¨ï¼‰
            html_dir = os.path.dirname(filepath) if os.path.dirname(filepath) else ''
            
            # <script src="..."> ã‚’æ¤œå‡º
            script_refs = re.findall(r'<script[^>]+src=["\']([^"\']+)["\']', content)
            for src in script_refs:
                # å¤–éƒ¨URLï¼ˆhttp://, https://, //ï¼‰ã¯ã‚¹ã‚­ãƒƒãƒ—
                if src.startswith(('http://', 'https://', '//')):
                    continue
                # CDNå‚ç…§ã¯ã‚¹ã‚­ãƒƒãƒ—
                if 'cdn' in src.lower() or 'cloudflare' in src.lower():
                    continue
                
                # ğŸ†• ç›¸å¯¾ãƒ‘ã‚¹ã‚’è§£æ±ºï¼ˆHTMLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’åŸºæº–ï¼‰
                normalized_src = src.lstrip('/')
                if html_dir and not normalized_src.startswith('/'):
                    # ç›¸å¯¾ãƒ‘ã‚¹ã®å ´åˆã€HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰è§£æ±º
                    resolved_path = os.path.normpath(os.path.join(html_dir, normalized_src))
                else:
                    resolved_path = normalized_src
                
                # static/ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã®å‡¦ç†
                if resolved_path.startswith('static/'):
                    check_path = resolved_path
                else:
                    check_path = resolved_path  # ç›¸å¯¾ãƒ‘ã‚¹è§£æ±ºæ¸ˆã¿ãªã®ã§ãã®ã¾ã¾ä½¿ç”¨
                
                # è¤‡æ•°ã®ãƒ‘ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯
                possible_paths = [
                    check_path.lower(),
                    resolved_path.lower(),
                    normalized_src.lower(),
                    f"static/{resolved_path}".lower(),
                ]
                
                if not any(p in generated_paths_lower for p in possible_paths):
                    issues.append({
                        'type': 'missing_script',
                        'file': filepath,
                        'dependency': src,
                        'expected_path': resolved_path,  # è§£æ±ºå¾Œã®ãƒ‘ã‚¹ã‚’ä½¿ç”¨
                        'details': f"Script file '{src}' referenced in HTML but not found"
                    })
                    logger.debug(f"[G-7] Missing script: {src} in {filepath}")
            
            # <link href="..."> ã‚’æ¤œå‡ºï¼ˆCSSï¼‰
            link_refs = re.findall(r'<link[^>]+href=["\']([^"\']+\.css)["\']', content)
            for href in link_refs:
                # å¤–éƒ¨URLã¯ã‚¹ã‚­ãƒƒãƒ—
                if href.startswith(('http://', 'https://', '//')):
                    continue
                
                # ğŸ†• ç›¸å¯¾ãƒ‘ã‚¹ã‚’è§£æ±ºï¼ˆHTMLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’åŸºæº–ï¼‰
                normalized_href = href.lstrip('/')
                if html_dir and not normalized_href.startswith('/'):
                    resolved_path = os.path.normpath(os.path.join(html_dir, normalized_href))
                else:
                    resolved_path = normalized_href
                
                # static/ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã®å‡¦ç†
                if resolved_path.startswith('static/'):
                    check_path = resolved_path
                else:
                    check_path = resolved_path
                
                # è¤‡æ•°ã®ãƒ‘ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯
                possible_paths = [
                    check_path.lower(),
                    resolved_path.lower(),
                    normalized_href.lower(),
                    f"static/{resolved_path}".lower(),
                ]
                
                if not any(p in generated_paths_lower for p in possible_paths):
                    issues.append({
                        'type': 'missing_css',
                        'file': filepath,
                        'dependency': href,
                        'expected_path': resolved_path,
                        'details': f"CSS file '{href}' referenced in HTML but not found"
                    })
                    logger.debug(f"[G-7] Missing CSS: {href} in {filepath}")
        
        # ========================================
        # Check 2: CSS @import â†” ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨
        # ========================================
        for filepath, content in generated_files.items():
            if not filepath.endswith('.css'):
                continue
            
            # CSSãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—
            css_dir = os.path.dirname(filepath) if os.path.dirname(filepath) else ''
            
            # @import url("...") ã¾ãŸã¯ @import "..." ã‚’æ¤œå‡º
            import_refs = re.findall(r'@import\s+(?:url\(["\']?([^"\')\s]+)["\']?\)|["\']([^"\']+)["\'])', content)
            for match in import_refs:
                import_path = match[0] or match[1]
                if not import_path:
                    continue
                
                # å¤–éƒ¨URLã¯ã‚¹ã‚­ãƒƒãƒ—
                if import_path.startswith(('http://', 'https://', '//')):
                    continue
                
                # ğŸ†• ç›¸å¯¾ãƒ‘ã‚¹ã‚’è§£æ±ºï¼ˆos.path.normpathã‚’ä½¿ç”¨ï¼‰
                normalized_import = import_path.lstrip('./')
                if css_dir and not import_path.startswith('/'):
                    resolved_path = os.path.normpath(os.path.join(css_dir, import_path))
                else:
                    resolved_path = normalized_import
                
                # è¤‡æ•°ã®ãƒ‘ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯
                possible_paths = [
                    resolved_path.lower(),
                    normalized_import.lower(),
                    import_path.lstrip('./').lower(),
                ]
                
                if not any(p in generated_paths_lower for p in possible_paths):
                    issues.append({
                        'type': 'missing_css_import',
                        'file': filepath,
                        'dependency': import_path,
                        'expected_path': resolved_path,
                        'details': f"CSS import '{import_path}' not found"
                    })
                    logger.debug(f"[G-7] Missing CSS import: {import_path} in {filepath}")
        
        # ========================================
        # Check 3: JS fetch API URL â†” Flask Route
        # ========================================
        # Flask routeã‚’åé›†
        flask_routes = set()
        for filepath, content in generated_files.items():
            if not filepath.endswith('.py'):
                continue
            
            # @app.route('/...') ã¾ãŸã¯ @bp.route('/...')
            route_matches = re.findall(r'@\w+\.route\s*\(\s*["\']([^"\']+)["\']', content)
            for route in route_matches:
                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ­£è¦åŒ– (/<int:id> -> /<param>)
                normalized_route = re.sub(r'<\w+:\w+>', '<param>', route)
                normalized_route = re.sub(r'<\w+>', '<param>', normalized_route)
                flask_routes.add(normalized_route)
        
        # JSãƒ•ã‚¡ã‚¤ãƒ«ã®fetch URLã‚’ãƒã‚§ãƒƒã‚¯
        for filepath, content in generated_files.items():
            if not filepath.endswith('.js'):
                continue
            
            # fetch('/api/...') ã‚’æ¤œå‡º
            fetch_urls = re.findall(r'fetch\s*\(\s*["\']([^"\']+)["\']', content)
            # axios.get/postç­‰ã‚‚æ¤œå‡º
            axios_urls = re.findall(r'axios\.\w+\s*\(\s*["\']([^"\']+)["\']', content)
            
            all_api_urls = fetch_urls + axios_urls
            
            for url in all_api_urls:
                # å¤–éƒ¨URLã¯ã‚¹ã‚­ãƒƒãƒ—
                if url.startswith(('http://', 'https://', '//')):
                    continue
                
                # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒªãƒ†ãƒ©ãƒ«ï¼ˆ${...}ï¼‰ã‚’å«ã‚€URLã¯éƒ¨åˆ†ãƒãƒƒãƒã§ç¢ºèª
                if '${' in url or '{' in url:
                    # å‹•çš„éƒ¨åˆ†ã‚’æ­£è¦åŒ–
                    normalized_url = re.sub(r'\$\{[^}]+\}', '<param>', url)
                    normalized_url = re.sub(r'\{[^}]+\}', '<param>', normalized_url)
                else:
                    normalized_url = url
                
                # ãƒ‘ã‚¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ­£è¦åŒ–
                normalized_url = re.sub(r'/\d+', '/<param>', normalized_url)
                
                # routeã«å­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
                route_found = False
                for route in flask_routes:
                    # å®Œå…¨ä¸€è‡´ã¾ãŸã¯å‰æ–¹ä¸€è‡´
                    if normalized_url == route or normalized_url.rstrip('/') == route.rstrip('/'):
                        route_found = True
                        break
                    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ­£è¦åŒ–å¾Œã®æ¯”è¼ƒ
                    if re.sub(r'<param>', '<param>', normalized_url) == re.sub(r'<param>', '<param>', route):
                        route_found = True
                        break
                
                # routeãŒå…¨ããªã„å ´åˆï¼ˆç©ºã®å ´åˆï¼‰ã¯è­¦å‘Šã—ãªã„
                if flask_routes and not route_found and url.startswith('/'):
                    # /auth/, /api/ ãªã©ã®ä¸€èˆ¬çš„ãªãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ãŒã‚ã‚‹ã‹ç¢ºèª
                    url_prefix = '/' + url.split('/')[1] + '/' if len(url.split('/')) > 1 else url
                    prefix_exists = any(r.startswith(url_prefix) for r in flask_routes)
                    
                    if not prefix_exists:
                        issues.append({
                            'type': 'missing_route',
                            'file': filepath,
                            'dependency': url,
                            'details': f"API endpoint '{url}' used in JS but no matching Flask route found"
                        })
                        logger.debug(f"[G-7] Missing route for: {url} in {filepath}")
        
        if issues:
            logger.debug(f"[G-7] Found {len(issues)} frontend dependency issues")
        else:
            logger.debug("[G-7] All frontend dependencies satisfied")
        
        return issues
    
    def _auto_fix_frontend_dependencies(
        self,
        issues: List[Dict[str, Any]],
        generated_files: Dict[str, str]
    ) -> Dict[str, str]:
        """
        ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ä¾å­˜å•é¡Œã‚’LLMã§ä¿®æ­£
        
        Args:
            issues: æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã®ãƒªã‚¹ãƒˆ
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«
        
        Returns:
            æ›´æ–°ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«è¾æ›¸
        """
        if not issues:
            return generated_files
        
        # å•é¡Œã‚¿ã‚¤ãƒ—ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        missing_files = []  # æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆãŒå¿…è¦
        missing_routes = []  # Flaskãƒ«ãƒ¼ãƒˆè¿½åŠ ãŒå¿…è¦
        
        for issue in issues:
            if issue['type'] in ('missing_script', 'missing_css', 'missing_css_import'):
                missing_files.append(issue)
            elif issue['type'] == 'missing_route':
                missing_routes.append(issue)
        
        # ========================================
        # 1. ä¸è¶³ãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆ
        # ========================================
        if missing_files:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            files_to_create = {}
            for issue in missing_files:
                expected_path = issue.get('expected_path', issue['dependency'])
                if expected_path not in files_to_create:
                    files_to_create[expected_path] = issue
            
            for filepath, issue in files_to_create.items():
                # æ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                if filepath in generated_files:
                    continue
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
                if filepath.endswith('.js'):
                    file_type = "JavaScript"
                    template = "// JavaScript file\n"
                elif filepath.endswith('.css'):
                    file_type = "CSS"
                    template = "/* CSS file */\n"
                else:
                    continue
                
                # å‚ç…§å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                referrer_content = generated_files.get(issue['file'], '')
                
                prompt = f"""Generate a {file_type} file that is referenced but missing.

## Missing File: {filepath}
## Referenced from: {issue['file']}

## Context (referrer file excerpt):
```
{referrer_content[:2000]}
```

## Requirements:
1. Create a minimal but functional {file_type} file
2. Include basic structure appropriate for the application
3. For JS: Include any event listeners or API calls that might be expected
4. For CSS: Include basic styling that matches the HTML structure

## Output
Return ONLY the {file_type} code without markdown code blocks.
"""
                
                try:
                    logger.debug(f"[G-7] Generating missing file: {filepath}")
                    
                    response = self._call_llm_api(prompt)
                    
                    if response:
                        # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»
                        import re
                        code = response.strip()
                        code = re.sub(r'^```\w*\n?', '', code)
                        code = re.sub(r'\n?```$', '', code)
                        
                        if code and len(code) > 20:
                            generated_files[filepath] = code
                            logger.debug(f"[G-7] âœ… Created {filepath}")
                        else:
                            logger.debug(f"[G-7] âš  Generated content too short for {filepath}")
                    else:
                        logger.debug(f"[G-7] âš  No response for {filepath}")
                        
                except Exception as e:
                    logger.debug(f"[G-7] âš  Error creating {filepath}: {e}")
        
        # ========================================
        # 2. ä¸è¶³Flaskãƒ«ãƒ¼ãƒˆã®è¿½åŠ 
        # ========================================
        if missing_routes:
            # views/api.py ã¾ãŸã¯é©åˆ‡ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
            api_file = None
            for filepath in generated_files.keys():
                if 'api' in filepath.lower() and filepath.endswith('.py'):
                    api_file = filepath
                    break
                if 'views' in filepath.lower() and filepath.endswith('.py'):
                    api_file = filepath
            
            if api_file and api_file in generated_files:
                current_code = generated_files[api_file]
                
                # ä¸è¶³ãƒ«ãƒ¼ãƒˆã®è©³ç´°
                routes_to_add = []
                for issue in missing_routes:
                    routes_to_add.append(f"- {issue['dependency']}")
                
                routes_text = "\n".join(routes_to_add)
                
                prompt = f"""Add missing API routes to this Flask file.

## File: {api_file}

## Missing Routes (referenced in JavaScript but not defined):
{routes_text}

## Current Code:
```python
{current_code}
```

## Requirements:
1. Add stub routes for each missing endpoint
2. Return appropriate JSON responses
3. Use @login_required if the route seems to need authentication
4. Follow the existing code style and patterns
5. Keep all existing code unchanged

## Output
Return ONLY the complete Python code with the new routes added.
"""
                
                try:
                    logger.debug(f"[G-7] Adding {len(missing_routes)} missing routes to {api_file}")
                    
                    response = self._call_llm_api(prompt)
                    
                    if response:
                        import re
                        code_match = re.search(r'```python\s*\n(.*?)```', response, re.DOTALL)
                        if code_match:
                            fixed_code = code_match.group(1).strip()
                        else:
                            fixed_code = response.strip()
                            if fixed_code.startswith('```'):
                                fixed_code = re.sub(r'^```\w*\n?', '', fixed_code)
                            if fixed_code.endswith('```'):
                                fixed_code = fixed_code[:-3].strip()
                        
                        if fixed_code and len(fixed_code) > len(current_code) * 0.8:
                            generated_files[api_file] = fixed_code
                            logger.debug(f"[G-7] âœ… Added routes to {api_file}")
                        else:
                            logger.debug(f"[G-7] âš  Generated code too short for {api_file}")
                    else:
                        logger.debug(f"[G-7] âš  No response for route additions")
                        
                except Exception as e:
                    logger.debug(f"[G-7] âš  Error adding routes: {e}")
        
        return generated_files

    # G-8: Modelâ†”Schemaæ•´åˆæ€§æ¤œè¨¼
    # =========================================
    
    def _validate_model_schema_consistency(
        self,
        generated_files: Dict[str, str]
    ) -> List[Dict[str, Any]]:
        """
        SQLAlchemyãƒ¢ãƒ‡ãƒ«ã¨Marshmallowã‚¹ã‚­ãƒ¼ãƒã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ•´åˆæ€§ã‚’æ¤œè¨¼
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã®ãƒªã‚¹ãƒˆ [{type, model, schema, field, details}]
        """
        import re
        
        issues = []
        
        # ========================================
        # Step 1: ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã‚«ãƒ©ãƒ å®šç¾©ã‚’æŠ½å‡º
        # ========================================
        models = {}  # {ModelName: {file, columns: [column_names]}}
        
        for filepath, content in generated_files.items():
            if not filepath.endswith('.py'):
                continue
            
            # ã‚¯ãƒ©ã‚¹å®šç¾©ã‚’æ¤œå‡º
            class_matches = re.finditer(
                r'class\s+(\w+)\s*\([^)]*(?:db\.Model|Base)[^)]*\):\s*\n(.*?)(?=\nclass\s|\Z)',
                content,
                re.DOTALL
            )
            
            for match in class_matches:
                model_name = match.group(1)
                class_body = match.group(2)
                
                columns = []
                
                # SQLAlchemy 2.0ã‚¹ã‚¿ã‚¤ãƒ«: field: Mapped[type] = mapped_column(...)
                mapped_columns = re.findall(
                    r'^\s+(\w+):\s*Mapped\[',
                    class_body,
                    re.MULTILINE
                )
                columns.extend(mapped_columns)
                
                # æ—§ã‚¹ã‚¿ã‚¤ãƒ«: field = Column(...)
                old_columns = re.findall(
                    r'^\s+(\w+)\s*=\s*(?:Column|mapped_column)\s*\(',
                    class_body,
                    re.MULTILINE
                )
                columns.extend(old_columns)
                
                # relationship ã¯é™¤å¤–ï¼ˆã‚¹ã‚­ãƒ¼ãƒã«ã¯é€šå¸¸å«ã¾ã‚Œãªã„ï¼‰
                relationships = re.findall(
                    r'^\s+(\w+):\s*Mapped\[.*?\]\s*=\s*relationship\s*\(',
                    class_body,
                    re.MULTILINE
                )
                columns = [c for c in columns if c not in relationships]
                
                # é‡è¤‡ã‚’é™¤å»
                columns = list(set(columns))
                
                if columns:
                    models[model_name] = {
                        'file': filepath,
                        'columns': columns
                    }
        
        logger.debug(f"[G-8] Found {len(models)} models: {list(models.keys())}")
        
        # ========================================
        # Step 2: ã‚¹ã‚­ãƒ¼ãƒã‹ã‚‰ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å®šç¾©ã‚’æŠ½å‡º
        # ========================================
        schemas = {}  # {SchemaName: {file, fields: [field_names], model_name}}
        
        for filepath, content in generated_files.items():
            if not filepath.endswith('.py'):
                continue
            if 'schema' not in filepath.lower():
                continue
            
            # Schemaã‚¯ãƒ©ã‚¹å®šç¾©ã‚’æ¤œå‡º
            schema_matches = re.finditer(
                r'class\s+(\w+Schema)\s*\([^)]*Schema[^)]*\):\s*\n(.*?)(?=\nclass\s|\Z)',
                content,
                re.DOTALL
            )
            
            for match in schema_matches:
                schema_name = match.group(1)
                class_body = match.group(2)
                
                fields = []
                
                # Marshmallowãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰: field = fields.Type(...) ã‚’æŠ½å‡º
                # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã¨å®šç¾©å…¨ä½“ã‚’å–å¾—
                field_pattern = r'^\s+(\w+)\s*=\s*(fields\.[^\n]+)'
                field_matches = re.findall(field_pattern, class_body, re.MULTILINE)
                
                # Nested, Method, Function, Pluckç­‰ã¯é™¤å¤–ï¼ˆãƒ¢ãƒ‡ãƒ«ã‚«ãƒ©ãƒ ã¨å¯¾å¿œã—ãªã„ï¼‰
                # ã“ã‚Œã‚‰ã¯relationshipçµŒç”±ã‚„è¨ˆç®—ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§ã‚ã‚Šã€ãƒ¢ãƒ‡ãƒ«ã«ç›´æ¥å¯¾å¿œã—ãªã„
                EXCLUDE_FIELD_PATTERNS = ['Nested', 'Method', 'Function', 'Pluck', 'List(fields.Nested']
                for field_name, field_def in field_matches:
                    should_exclude = False
                    for exclude_pattern in EXCLUDE_FIELD_PATTERNS:
                        if exclude_pattern in field_def:
                            should_exclude = True
                            logger.debug(f"[G-8] Excluding field '{field_name}' (type contains '{exclude_pattern}')")
                            break
                    if not should_exclude:
                        fields.append(field_name)
                
                # dump_only, load_only ç­‰ã®ãƒ¡ã‚¿æƒ…å ±ã‚‚å«ã‚€
                # class Meta: model = ModelName ã‚’æ¤œå‡º
                model_match = re.search(
                    r'class\s+Meta\s*:.*?model\s*=\s*(\w+)',
                    class_body,
                    re.DOTALL
                )
                
                # ã‚¹ã‚­ãƒ¼ãƒåã‹ã‚‰ãƒ¢ãƒ‡ãƒ«åã‚’æ¨æ¸¬ (UserSchema -> User)
                inferred_model = schema_name.replace('Schema', '').replace('Create', '').replace('Update', '').replace('Response', '')
                model_name = model_match.group(1) if model_match else inferred_model
                
                if fields:
                    schemas[schema_name] = {
                        'file': filepath,
                        'fields': fields,
                        'model_name': model_name
                    }
        
        logger.debug(f"[G-8] Found {len(schemas)} schemas: {list(schemas.keys())}")
        
        # ========================================
        # Step 3: æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        # ========================================
        for schema_name, schema_info in schemas.items():
            model_name = schema_info['model_name']
            
            if model_name not in models:
                # å¯¾å¿œã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„ï¼ˆè­¦å‘Šã®ã¿ï¼‰
                logger.debug(f"[G-8] Schema '{schema_name}' references model '{model_name}' which was not found")
                continue
            
            model_columns = set(models[model_name]['columns'])
            schema_fields = set(schema_info['fields'])
            
            # ã‚¹ã‚­ãƒ¼ãƒã«ã‚ã‚‹ãŒãƒ¢ãƒ‡ãƒ«ã«ãªã„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
            extra_in_schema = schema_fields - model_columns
            # ä¸€èˆ¬çš„ãªè¿½åŠ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼ˆid, created_atç­‰ï¼‰ã¯é™¤å¤–
            common_fields = {'id', 'created_at', 'updated_at', 'password', 'password_hash'}
            extra_in_schema = extra_in_schema - common_fields
            
            for field in extra_in_schema:
                # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åãŒé¡ä¼¼ã—ã¦ã„ã‚‹ã‹ç¢ºèªï¼ˆtypoã®å¯èƒ½æ€§ï¼‰
                similar_columns = [c for c in model_columns if field.lower() in c.lower() or c.lower() in field.lower()]
                
                issues.append({
                    'type': 'schema_field_not_in_model',
                    'schema': schema_name,
                    'schema_file': schema_info['file'],
                    'model': model_name,
                    'model_file': models[model_name]['file'],
                    'field': field,
                    'similar_columns': similar_columns,
                    'details': f"Schema field '{field}' not found in model '{model_name}'"
                })
                logger.debug(f"[G-8] Schema '{schema_name}' has field '{field}' not in model '{model_name}'")
        
        if issues:
            logger.debug(f"[G-8] Found {len(issues)} model-schema consistency issues")
        else:
            logger.debug("[G-8] All model-schema consistencies satisfied")
        
        return issues
    
    def _auto_fix_model_schema_consistency(
        self,
        issues: List[Dict[str, Any]],
        generated_files: Dict[str, str]
    ) -> Dict[str, str]:
        """
        Modelâ†”Schemaæ•´åˆæ€§å•é¡Œã‚’LLMã§ä¿®æ­£
        
        Args:
            issues: æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã®ãƒªã‚¹ãƒˆ
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«
        
        Returns:
            æ›´æ–°ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«è¾æ›¸
        """
        if not issues:
            return generated_files
        
        # ã‚¹ã‚­ãƒ¼ãƒãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        issues_by_schema_file = {}
        for issue in issues:
            schema_file = issue['schema_file']
            if schema_file not in issues_by_schema_file:
                issues_by_schema_file[schema_file] = []
            issues_by_schema_file[schema_file].append(issue)
        
        for schema_file, file_issues in issues_by_schema_file.items():
            if schema_file not in generated_files:
                continue
            
            schema_code = generated_files[schema_file]
            
            # å¯¾å¿œã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’å–å¾—
            model_files_content = {}
            for issue in file_issues:
                model_file = issue['model_file']
                if model_file in generated_files and model_file not in model_files_content:
                    model_files_content[model_file] = generated_files[model_file]
            
            model_context = "\n\n".join([
                f"# {filepath}\n```python\n{content[:3000]}\n```"
                for filepath, content in model_files_content.items()
            ])
            
            # å•é¡Œã®è©³ç´°
            issue_descriptions = []
            for issue in file_issues:
                desc = f"- Field '{issue['field']}' in schema '{issue['schema']}' not found in model '{issue['model']}'"
                if issue['similar_columns']:
                    desc += f" (similar columns: {issue['similar_columns']})"
                issue_descriptions.append(desc)
            
            issues_text = "\n".join(issue_descriptions)
            
            fix_prompt = f"""Fix the Marshmallow schema to match the SQLAlchemy model.

## Schema File: {schema_file}
```python
{schema_code}
```

## Related Model Files:
{model_context}

## Issues Found:
{issues_text}

## Fix Instructions:
1. If the schema field is a typo, correct it to match the model column name
2. If the schema field should not exist (not in model), remove it
3. If the schema field is intentionally different (computed, virtual), keep it but ensure it has dump_only=True or load_only=True as appropriate
4. Do NOT modify the model files - only fix the schema file
5. Keep all existing correct fields unchanged

## Output
Return ONLY the complete fixed schema Python code without markdown code blocks.
"""
            
            try:
                logger.debug(f"[G-8] Fixing schema: {schema_file}")
                
                response = self._call_llm_api(fix_prompt)
                
                if response:
                    import re
                    code = response.strip()
                    code = re.sub(r'^```\w*\n?', '', code)
                    code = re.sub(r'\n?```$', '', code)
                    
                    if code and len(code) > 100:
                        generated_files[schema_file] = code
                        logger.debug(f"[G-8] âœ… Fixed {schema_file}")
                    else:
                        logger.debug(f"[G-8] âš  Generated code too short for {schema_file}")
                else:
                    logger.debug(f"[G-8] âš  No response for {schema_file}")
                    
            except Exception as e:
                logger.debug(f"[G-8] âš  Error fixing {schema_file}: {e}")
        
        return generated_files

    # G-9: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç’°å¢ƒä¾å­˜æ¤œè¨¼
    # =========================================
    
    def _validate_document_environment_commands(
        self,
        generated_files: Dict[str, str]
    ) -> List[Dict[str, Any]]:
        """
        READMEã‚„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…ã®ã‚³ãƒãƒ³ãƒ‰ãŒç’°å¢ƒä¾å­˜ã—ã¦ã„ãªã„ã‹æ¤œè¨¼
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã®ãƒªã‚¹ãƒˆ [{type, file, line, command, details, fix}]
        """
        import re
        
        issues = []
        
        # ç’°å¢ƒä¾å­˜ã‚³ãƒãƒ³ãƒ‰ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
        ENV_DEPENDENT_PATTERNS = [
            # Unixå°‚ç”¨ã‚³ãƒãƒ³ãƒ‰ï¼ˆWindowsã§å‹•ã‹ãªã„ï¼‰
            {
                'pattern': r'^\s*(?:source|chmod|chown|ln\s+-s|tar\s+-[xczf]|grep|sed|awk|curl\s+-[^h])',
                'type': 'unix_only',
                'details': 'Unix-only command, may not work on Windows'
            },
            # Windowsã§ç•°ãªã‚‹ç’°å¢ƒå¤‰æ•°è¨­å®š
            {
                'pattern': r'^\s*export\s+\w+=',
                'type': 'unix_export',
                'details': 'Unix export command. Windows uses "set" or "$env:"',
                'fix': 'Add Windows alternative: set VAR=value (cmd) or $env:VAR="value" (PowerShell)'
            },
            # ãƒ‘ã‚¹åŒºåˆ‡ã‚Šæ–‡å­—ï¼ˆãƒãƒƒã‚¯ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ï¼‰
            {
                'pattern': r'[A-Za-z]:\\\\[A-Za-z]',
                'type': 'windows_path',
                'details': 'Windows-specific path. Unix uses forward slashes',
                'fix': 'Use forward slashes or provide both examples'
            },
            # cp ã‚³ãƒãƒ³ãƒ‰ï¼ˆWindowsã¯copyï¼‰
            {
                'pattern': r'^\s*cp\s+(?!-)',
                'type': 'unix_cp',
                'details': 'Unix cp command. Windows uses "copy"',
                'fix': 'Add Windows alternative or use cross-platform method'
            },
            # rm ã‚³ãƒãƒ³ãƒ‰ï¼ˆWindowsã¯Remove-Itemã¾ãŸã¯delï¼‰
            {
                'pattern': r'^\s*rm\s+',
                'type': 'unix_rm',
                'details': 'Unix rm command. Windows uses "del" or "Remove-Item"'
            },
            # mkdir -pï¼ˆWindowsã¯ mkdir ã®ã¿ã§å¯ï¼‰
            {
                'pattern': r'^\s*mkdir\s+-p\s+',
                'type': 'unix_mkdir_p',
                'details': 'Unix mkdir -p. Windows mkdir creates parent dirs by default'
            },
            # venv activate ãŒç’°å¢ƒæœªæŒ‡å®š
            {
                'pattern': r'^\s*(?:source\s+)?venv[/\\]?(?:bin|Scripts)[/\\]?activate(?!\.(ps1|bat))',
                'type': 'venv_activate_ambiguous',
                'details': 'venv activate command without specifying environment',
                'fix': 'Specify: .\\venv\\Scripts\\Activate.ps1 (PowerShell), venv\\Scripts\\activate.bat (cmd), source venv/bin/activate (Unix)'
            },
        ]
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        doc_files = []
        for filepath in generated_files.keys():
            lower_path = filepath.lower()
            if any(doc in lower_path for doc in ['readme', 'install', 'setup', 'getting_started', 'quickstart', 'contributing']):
                doc_files.append(filepath)
            if lower_path.endswith(('.md', '.rst', '.txt')) and 'doc' in lower_path:
                doc_files.append(filepath)
        
        for filepath in doc_files:
            content = generated_files[filepath]
            lines = content.split('\n')
            
            # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å†…ã®ã‚³ãƒãƒ³ãƒ‰ã‚’æ¤œå‡º
            in_code_block = False
            code_block_lang = ''
            
            for line_num, line in enumerate(lines, 1):
                # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã®é–‹å§‹/çµ‚äº†ã‚’æ¤œå‡º
                if line.strip().startswith('```'):
                    if in_code_block:
                        in_code_block = False
                        code_block_lang = ''
                    else:
                        in_code_block = True
                        # è¨€èªæŒ‡å®šã‚’å–å¾—
                        code_block_lang = line.strip()[3:].lower()
                    continue
                
                # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å¤–ã¯ã‚¹ã‚­ãƒƒãƒ—
                if not in_code_block:
                    continue
                
                # bashã‚„shellã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å†…ã®ã¿ãƒã‚§ãƒƒã‚¯
                if code_block_lang and code_block_lang not in ('bash', 'sh', 'shell', 'console', 'powershell', 'cmd', ''):
                    continue
                
                # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
                for pattern_info in ENV_DEPENDENT_PATTERNS:
                    if re.search(pattern_info['pattern'], line):
                        issues.append({
                            'type': pattern_info['type'],
                            'file': filepath,
                            'line': line_num,
                            'command': line.strip(),
                            'details': pattern_info['details'],
                            'fix': pattern_info.get('fix', 'Provide cross-platform alternatives')
                        })
                        logger.debug(f"[G-9] {filepath}:{line_num} - {pattern_info['type']}: {line.strip()[:50]}")
        
        if issues:
            logger.debug(f"[G-9] Found {len(issues)} environment-dependent commands")
        else:
            logger.debug("[G-9] All document commands are cross-platform compatible")
        
        return issues
    
    def _auto_fix_document_environment_commands(
        self,
        issues: List[Dict[str, Any]],
        generated_files: Dict[str, str]
    ) -> Dict[str, str]:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…ã®ç’°å¢ƒä¾å­˜ã‚³ãƒãƒ³ãƒ‰ã‚’LLMã§ä¿®æ­£
        
        Args:
            issues: æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã®ãƒªã‚¹ãƒˆ
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«
        
        Returns:
            æ›´æ–°ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«è¾æ›¸
        """
        if not issues:
            return generated_files
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        issues_by_file = {}
        for issue in issues:
            filepath = issue['file']
            if filepath not in issues_by_file:
                issues_by_file[filepath] = []
            issues_by_file[filepath].append(issue)
        
        for filepath, file_issues in issues_by_file.items():
            if filepath not in generated_files:
                continue
            
            content = generated_files[filepath]
            
            # å•é¡Œã®è©³ç´°
            issue_descriptions = []
            for issue in file_issues:
                issue_descriptions.append(
                    f"- Line {issue['line']}: `{issue['command'][:60]}...`\n"
                    f"  Problem: {issue['details']}\n"
                    f"  Fix: {issue['fix']}"
                )
            
            issues_text = "\n".join(issue_descriptions)
            
            fix_prompt = f"""Fix the environment-dependent commands in this documentation to be cross-platform compatible.

## File: {filepath}
```markdown
{content}
```

## Issues Found:
{issues_text}

## Fix Instructions:
1. For each command, provide alternatives for different operating systems
2. Use this format for shell commands:
   ```bash
   # Windows (PowerShell)
   .\\venv\\Scripts\\Activate.ps1
   
   # Windows (Command Prompt)
   venv\\Scripts\\activate.bat
   
   # Linux/Mac
   source venv/bin/activate
   ```
3. For simple commands like `cp`, show both Unix and Windows versions
4. Keep the document structure and all other content unchanged
5. Only modify the specific commands identified as issues

## Output
Return the complete fixed markdown document.
"""
            
            try:
                logger.debug(f"[G-9] Fixing document: {filepath}")
                
                response = self._call_llm_api(fix_prompt)
                
                if response:
                    import re
                    # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»ï¼ˆãƒ¬ã‚¹ãƒãƒ³ã‚¹å…¨ä½“ãŒ```ã§å›²ã¾ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
                    fixed_content = response.strip()
                    if fixed_content.startswith('```markdown'):
                        fixed_content = fixed_content[11:]
                    elif fixed_content.startswith('```md'):
                        fixed_content = fixed_content[5:]
                    elif fixed_content.startswith('```'):
                        fixed_content = fixed_content[3:]
                    
                    if fixed_content.endswith('```'):
                        fixed_content = fixed_content[:-3]
                    
                    fixed_content = fixed_content.strip()
                    
                    if fixed_content and len(fixed_content) > len(content) * 0.5:
                        generated_files[filepath] = fixed_content
                        logger.debug(f"[G-9] âœ… Fixed {filepath}")
                    else:
                        logger.debug(f"[G-9] âš  Generated content too short for {filepath}")
                else:
                    logger.debug(f"[G-9] âš  No response for {filepath}")
                    
            except Exception as e:
                logger.debug(f"[G-9] âš  Error fixing {filepath}: {e}")
        
        return generated_files

    # G-10: ã‚¯ãƒ©ã‚¹å®šç¾©å®‰å…¨æ€§æ¤œè¨¼
    # =========================================
    
    def _validate_class_definition_safety(
        self,
        generated_files: Dict[str, str]
    ) -> List[Dict[str, Any]]:
        """
        ã‚¯ãƒ©ã‚¹å®šç¾©æœ¬ä½“ã§ã®å±é™ºãªã‚³ãƒ¼ãƒ‰ã‚’æ¤œå‡º
        
        æ¤œå‡ºå¯¾è±¡:
        - ã‚¯ãƒ©ã‚¹å®šç¾©æœ¬ä½“ã§ã®raiseæ–‡ï¼ˆimportæ™‚ã«å®Ÿè¡Œã•ã‚Œã‚‹ï¼‰
        - ã‚¯ãƒ©ã‚¹å®šç¾©æœ¬ä½“ã§ã®æ¡ä»¶ä»˜ãraiseï¼ˆif not X: raiseï¼‰
        - ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ¬ãƒ™ãƒ«ã§ã®å±é™ºãªå‰¯ä½œç”¨ã‚³ãƒ¼ãƒ‰
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã®ãƒªã‚¹ãƒˆ [{type, file, line, code, details, fix}]
        """
        import re
        import ast
        
        issues = []
        
        for filepath, content in generated_files.items():
            if not filepath.endswith('.py'):
                continue
            
            # ========================================
            # Check 1: ASTè§£æã«ã‚ˆã‚‹ã‚¯ãƒ©ã‚¹å®šç¾©æœ¬ä½“ã®raiseæ¤œå‡º
            # ========================================
            try:
                tree = ast.parse(content)
                
                for node in ast.walk(tree):
                    if isinstance(node, ast.ClassDef):
                        class_name = node.name
                        
                        # ã‚¯ãƒ©ã‚¹å®šç¾©ã®ç›´æ¥ã®å­è¦ç´ ã‚’ãƒã‚§ãƒƒã‚¯
                        for item in node.body:
                            # Ifæ–‡ã®ä¸­ã§raise
                            if isinstance(item, ast.If):
                                for sub_item in ast.walk(item):
                                    if isinstance(sub_item, ast.Raise):
                                        # è¡Œç•ªå·ã‚’å–å¾—
                                        line_no = sub_item.lineno
                                        # è©²å½“è¡Œã®ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—
                                        lines = content.split('\n')
                                        code_line = lines[line_no - 1] if line_no <= len(lines) else ''
                                        
                                        issues.append({
                                            'type': 'class_body_conditional_raise',
                                            'file': filepath,
                                            'line': line_no,
                                            'class': class_name,
                                            'code': code_line.strip(),
                                            'details': f"Conditional raise in class '{class_name}' body executes at import time",
                                            'fix': 'Move validation to __init__ or use @property with lazy evaluation'
                                        })
                                        logger.debug(f"[G-10] {filepath}:{line_no} - Conditional raise in class '{class_name}'")
                            
                            # ç›´æ¥ã®raiseæ–‡
                            elif isinstance(item, ast.Raise):
                                line_no = item.lineno
                                lines = content.split('\n')
                                code_line = lines[line_no - 1] if line_no <= len(lines) else ''
                                
                                issues.append({
                                    'type': 'class_body_raise',
                                    'file': filepath,
                                    'line': line_no,
                                    'class': class_name,
                                    'code': code_line.strip(),
                                    'details': f"Raise statement in class '{class_name}' body executes at import time",
                                    'fix': 'Move to __init__ or use a classmethod/staticmethod'
                                })
                                logger.debug(f"[G-10] {filepath}:{line_no} - Raise in class '{class_name}' body")
                            
                            # ã‚¯ãƒ©ã‚¹æœ¬ä½“ã§ã®assertæ–‡ï¼ˆæœ¬ç•ªç’°å¢ƒã§å•é¡Œã«ãªã‚‹å¯èƒ½æ€§ï¼‰
                            elif isinstance(item, ast.Assert):
                                line_no = item.lineno
                                lines = content.split('\n')
                                code_line = lines[line_no - 1] if line_no <= len(lines) else ''
                                
                                # Configã‚¯ãƒ©ã‚¹ã®å ´åˆã®ã¿è­¦å‘Š
                                if 'Config' in class_name:
                                    issues.append({
                                        'type': 'class_body_assert',
                                        'file': filepath,
                                        'line': line_no,
                                        'class': class_name,
                                        'code': code_line.strip(),
                                        'details': f"Assert in config class '{class_name}' may cause issues with python -O",
                                        'fix': 'Use explicit if/raise instead of assert for config validation'
                                    })
                                    logger.debug(f"[G-10] {filepath}:{line_no} - Assert in config class '{class_name}'")
                
            except SyntaxError as e:
                # æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã¯åˆ¥ã®ãƒã‚§ãƒƒã‚¯ã§æ¤œå‡ºã•ã‚Œã‚‹ã®ã§ã‚¹ã‚­ãƒƒãƒ—
                logger.debug(f"[G-10] Syntax error in {filepath}: {e}")
                continue
            except Exception as e:
                logger.debug(f"[G-10] Error parsing {filepath}: {e}")
                continue
            
            # ========================================
            # Check 2: æ­£è¦è¡¨ç¾ã«ã‚ˆã‚‹è¿½åŠ ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
            # ========================================
            # os.environ.get() ã®çµæœã«å¯¾ã™ã‚‹å³æ™‚raiseãƒ‘ã‚¿ãƒ¼ãƒ³
            # ä¾‹: SECRET_KEY = os.environ.get('KEY') or raise_error()
            pattern = r'=\s*os\.environ\.get\([^)]+\)\s*\n\s*if\s+not\s+\w+:\s*\n\s*raise'
            matches = list(re.finditer(pattern, content, re.MULTILINE))
            
            for match in matches:
                # è¡Œç•ªå·ã‚’è¨ˆç®—
                line_no = content[:match.start()].count('\n') + 1
                
                # æ—¢ã«ASTè§£æã§æ¤œå‡ºã•ã‚Œã¦ã„ãªã„ã‹ç¢ºèª
                already_detected = any(
                    issue['file'] == filepath and 
                    abs(issue['line'] - line_no) <= 2 
                    for issue in issues
                )
                
                if not already_detected:
                    lines = content.split('\n')
                    code_snippet = '\n'.join(lines[max(0, line_no-1):min(len(lines), line_no+2)])
                    
                    issues.append({
                        'type': 'env_var_immediate_raise',
                        'file': filepath,
                        'line': line_no,
                        'code': code_snippet[:100],
                        'details': 'Environment variable check with immediate raise at module/class level',
                        'fix': 'Add default value or move validation to runtime'
                    })
                    logger.debug(f"[G-10] {filepath}:{line_no} - Env var immediate raise pattern")
        
        if issues:
            logger.debug(f"[G-10] Found {len(issues)} class definition safety issues")
        else:
            logger.debug("[G-10] All class definitions are safe")
        
        return issues
    
    def _auto_fix_class_definition_safety(
        self,
        issues: List[Dict[str, Any]],
        generated_files: Dict[str, str]
    ) -> Dict[str, str]:
        """
        ã‚¯ãƒ©ã‚¹å®šç¾©å®‰å…¨æ€§å•é¡Œã‚’LLMã§ä¿®æ­£
        
        Args:
            issues: æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã®ãƒªã‚¹ãƒˆ
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«
        
        Returns:
            æ›´æ–°ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«è¾æ›¸
        """
        if not issues:
            return generated_files
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        issues_by_file = {}
        for issue in issues:
            filepath = issue['file']
            if filepath not in issues_by_file:
                issues_by_file[filepath] = []
            issues_by_file[filepath].append(issue)
        
        for filepath, file_issues in issues_by_file.items():
            if filepath not in generated_files:
                continue
            
            content = generated_files[filepath]
            
            # å•é¡Œã®è©³ç´°
            issue_descriptions = []
            for issue in file_issues:
                issue_descriptions.append(
                    f"- Line {issue['line']}: {issue['type']}\n"
                    f"  Code: `{issue['code'][:60]}...`\n"
                    f"  Problem: {issue['details']}\n"
                    f"  Fix: {issue['fix']}"
                )
            
            issues_text = "\n".join(issue_descriptions)
            
            fix_prompt = f"""Fix the class definition safety issues in this Python file.

## File: {filepath}
```python
{content}
```

## Issues Found:
{issues_text}

## Fix Instructions:
1. For class-level raise statements that check environment variables:
   - Add a default value: `SECRET_KEY = os.environ.get('SECRET_KEY') or 'dev-secret-key-change-in-production'`
   - Or use a property with lazy evaluation
   - Do NOT use `if not X: raise` pattern at class level

2. For config validation that must fail in production:
   - Move the validation to a separate `validate()` classmethod
   - Call this method explicitly after configuration loading, not at import time

3. Keep all other code unchanged

## Example Fix:
Before:
```python
class ProductionConfig(Config):
    SECRET_KEY = os.environ.get('SECRET_KEY')
    if not SECRET_KEY:
        raise ValueError("SECRET_KEY must be set")
```

After:
```python
class ProductionConfig(Config):
    SECRET_KEY = os.environ.get('SECRET_KEY') or 'set-secret-key-in-production'
    
    @classmethod
    def validate(cls):
        if not os.environ.get('SECRET_KEY'):
            raise ValueError("SECRET_KEY must be set in production environment")
```

## Output
Return ONLY the complete fixed Python code without markdown code blocks.
"""
            
            try:
                logger.debug(f"[G-10] Fixing: {filepath}")
                
                response = self._call_llm_api(fix_prompt)
                
                if response:
                    import re
                    fixed_content = response.strip()
                    # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»
                    fixed_content = re.sub(r'^```\w*\n?', '', fixed_content)
                    fixed_content = re.sub(r'\n?```$', '', fixed_content)
                    fixed_content = fixed_content.strip()
                    
                    if fixed_content and len(fixed_content) > len(content) * 0.5:
                        generated_files[filepath] = fixed_content
                        logger.debug(f"[G-10] âœ… Fixed {filepath}")
                    else:
                        logger.debug(f"[G-10] âš  Generated content too short for {filepath}")
                else:
                    logger.debug(f"[G-10] âš  No response for {filepath}")
                    
            except Exception as e:
                logger.debug(f"[G-10] âš  Error fixing {filepath}: {e}")
        
        return generated_files

    # ============================================
    # APIå¥‘ç´„ä¿è­·: å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
    # ============================================
    
    def _extract_frontend_api_field_references(
        self,
        generated_files: Dict[str, str]
    ) -> Dict[str, set]:
        """
        ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‚³ãƒ¼ãƒ‰ã‹ã‚‰APIå‚ç…§ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æŠ½å‡º
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã”ã¨ã®å‚ç…§ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ {endpoint: {field1, field2, ...}}
        """
        import re
        
        api_references = {}  # {endpoint: {fields}}
        response_field_references = set()  # å…¨ä½“ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å‚ç…§
        
        for filepath, content in generated_files.items():
            if not filepath.endswith(('.js', '.jsx', '.ts', '.tsx')):
                continue
            
            # CRLFå¯¾å¿œ
            content = content.replace('\r\n', '\n')
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³1: response.field, result.field, data.field
            # ä¾‹: response.id, result.user_id, data.username
            response_patterns = [
                r'(?:response|result|data|user|item|save|player)\.(\w+)',
                r'(?:response|result|data|user|item|save|player)\[[\'\"](\w+)[\'\"]\]',
            ]
            
            for pattern in response_patterns:
                for match in re.finditer(pattern, content):
                    field = match.group(1)
                    response_field_references.add(field)
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³2: fetchå‘¼ã³å‡ºã—ã¨ãã®å¾Œã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚¢ã‚¯ã‚»ã‚¹
            # ä¾‹: fetch('/api/user').then(r => r.json()).then(data => data.id)
            fetch_pattern = r'fetch\s*\(\s*[\'\"](\/[^\'\"]+)[\'\"]\s*\)'
            for match in re.finditer(fetch_pattern, content):
                endpoint = match.group(1)
                if endpoint not in api_references:
                    api_references[endpoint] = set()
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³3: destructuring
            # ä¾‹: const { id, username } = response;
            destructure_pattern = r'const\s*\{\s*([^}]+)\s*\}\s*=\s*(?:response|result|data|await)'
            for match in re.finditer(destructure_pattern, content):
                fields_str = match.group(1)
                fields = [f.strip().split(':')[0].strip() for f in fields_str.split(',')]
                response_field_references.update(fields)
        
        # å…¨ä½“ã®å‚ç…§ã‚’ 'global' ã‚­ãƒ¼ã«æ ¼ç´
        api_references['_global'] = response_field_references
        
        logger.debug(f"[API Contract] Extracted {len(response_field_references)} frontend field references")
        
        return api_references
    
    def _is_field_referenced_in_frontend(
        self,
        field_name: str,
        generated_files: Dict[str, str],
        api_references: Dict[str, set] = None
    ) -> bool:
        """
        ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã§å‚ç…§ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        
        Args:
            field_name: ãƒã‚§ãƒƒã‚¯ã™ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«
            api_references: äº‹å‰æŠ½å‡ºæ¸ˆã¿ã®APIå‚ç…§ï¼ˆçœç•¥æ™‚ã¯æŠ½å‡ºï¼‰
        
        Returns:
            ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã§å‚ç…§ã•ã‚Œã¦ã„ã‚‹å ´åˆTrue
        """
        if api_references is None:
            api_references = self._extract_frontend_api_field_references(generated_files)
        
        global_refs = api_references.get('_global', set())
        return field_name in global_refs

    # ============================================
    # G-11: ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯äºˆç´„èªãƒã‚§ãƒƒã‚¯
    # ============================================
    
    def _validate_framework_reserved_words(
        self,
        generated_files: Dict[str, str]
    ) -> List[Dict[str, Any]]:
        """
        ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼ˆSQLAlchemy/Flask/Marshmallowï¼‰ã®äºˆç´„èªä½¿ç”¨ã‚’æ¤œå‡º
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã®ãƒªã‚¹ãƒˆ
        """
        import re
        
        issues = []
        
        # SQLAlchemy Declarative API äºˆç´„èª
        SQLALCHEMY_RESERVED = {
            'metadata': 'item_metadata',
            'registry': 'model_registry', 
            'query': 'db_query',
            'query_class': 'custom_query_class',
        }
        
        # Marshmallow äºˆç´„èª
        MARSHMALLOW_RESERVED = {
            'fields': 'schema_fields',
            'dump': 'dump_data',
            'load': 'load_data',
            'validate': 'validate_data',
        }
        
        for filepath, content in generated_files.items():
            if not filepath.endswith('.py'):
                continue
            
            # CRLFå¯¾å¿œï¼ˆWindowsæ”¹è¡Œã‚³ãƒ¼ãƒ‰ï¼‰
            content = content.replace('\r\n', '\n')
            
            # SQLAlchemy ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œå‡º
            is_sqlalchemy_model = bool(re.search(
                r'class\s+\w+\s*\(\s*(db\.Model|Base)\s*\)',
                content
            ))
            
            # Marshmallow ã‚¹ã‚­ãƒ¼ãƒãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œå‡º
            is_marshmallow_schema = bool(re.search(
                r'class\s+\w+\s*\(\s*(ma\.Schema|Schema|SQLAlchemySchema|SQLAlchemyAutoSchema)\s*\)',
                content
            ))
            
            if is_sqlalchemy_model:
                # SQLAlchemyäºˆç´„èªãƒã‚§ãƒƒã‚¯
                for reserved_word, suggested_name in SQLALCHEMY_RESERVED.items():
                    # ã‚«ãƒ©ãƒ å®šç¾©ãƒ‘ã‚¿ãƒ¼ãƒ³: reserved_word: Mapped[...] = ã¾ãŸã¯ reserved_word = Column(...)
                    # [ \t]+ ã‚’ä½¿ç”¨ï¼ˆ\s+ã¯æ”¹è¡Œã‚’å«ã‚€ãŸã‚ä½¿ç”¨ã—ãªã„ï¼‰
                    pattern = rf'^[ \t]+{reserved_word}[ \t]*[:=]'
                    matches = re.finditer(pattern, content, re.MULTILINE)
                    for match in matches:
                        line_no = content[:match.start()].count('\n') + 1
                        line_content = content.split('\n')[line_no - 1].strip()
                        
                        issues.append({
                            'type': 'sqlalchemy_reserved_word',
                            'file': filepath,
                            'line': line_no,
                            'reserved_word': reserved_word,
                            'suggested_name': suggested_name,
                            'code': line_content,
                            'details': f"'{reserved_word}' is reserved in SQLAlchemy Declarative API"
                        })
                        logger.debug(f"[G-11] {filepath}:{line_no} - SQLAlchemy reserved word '{reserved_word}'")
            
            if is_marshmallow_schema:
                # Marshmallowäºˆç´„èªãƒã‚§ãƒƒã‚¯ï¼ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã¨ã—ã¦ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆï¼‰
                for reserved_word, suggested_name in MARSHMALLOW_RESERVED.items():
                    # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å®šç¾©ãƒ‘ã‚¿ãƒ¼ãƒ³: reserved_word = fields.XXX(...)
                    # [ \t]+ ã‚’ä½¿ç”¨ï¼ˆ\s+ã¯æ”¹è¡Œã‚’å«ã‚€ãŸã‚ä½¿ç”¨ã—ãªã„ï¼‰
                    pattern = rf'^[ \t]+{reserved_word}[ \t]*=[ \t]*fields\.'
                    matches = re.finditer(pattern, content, re.MULTILINE)
                    for match in matches:
                        line_no = content[:match.start()].count('\n') + 1
                        line_content = content.split('\n')[line_no - 1].strip()
                        
                        issues.append({
                            'type': 'marshmallow_reserved_word',
                            'file': filepath,
                            'line': line_no,
                            'reserved_word': reserved_word,
                            'suggested_name': suggested_name,
                            'code': line_content,
                            'details': f"'{reserved_word}' conflicts with Marshmallow Schema attribute"
                        })
                        logger.debug(f"[G-11] {filepath}:{line_no} - Marshmallow reserved word '{reserved_word}'")
        
        if issues:
            logger.debug(f"[G-11] Found {len(issues)} framework reserved word issues")
        else:
            logger.debug("[G-11] No framework reserved word issues found")
        
        return issues
    
    def _auto_fix_framework_reserved_words(
        self,
        issues: List[Dict[str, Any]],
        generated_files: Dict[str, str]
    ) -> Dict[str, str]:
        """
        ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯äºˆç´„èªã®å•é¡Œã‚’LLMã§è‡ªå‹•ä¿®æ­£
        """
        if not issues or not self.llm_manager:
            return generated_files
        
        # APIå¥‘ç´„ä¿è­·: ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã§å‚ç…§ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯å¤‰æ›´ã—ãªã„
        api_references = self._extract_frontend_api_field_references(generated_files)
        protected_issues = []
        safe_issues = []
        
        for issue in issues:
            field_name = issue.get('reserved_word', '')
            if self._is_field_referenced_in_frontend(field_name, generated_files, api_references):
                protected_issues.append(issue)
                logger.debug(f"[G-11] âš  Skipping '{field_name}' - referenced in frontend (API contract protection)")
            else:
                safe_issues.append(issue)
        
        if protected_issues:
            logger.debug(f"[G-11] Protected {len(protected_issues)} fields due to frontend references")
        
        # å®‰å…¨ã«å¤‰æ›´ã§ãã‚‹issuesã®ã¿å‡¦ç†
        files_to_fix = {}
        for issue in safe_issues:
            filepath = issue['file']
            if filepath not in files_to_fix:
                files_to_fix[filepath] = []
            files_to_fix[filepath].append(issue)
        
        for filepath, file_issues in files_to_fix.items():
            if filepath not in generated_files:
                continue
            
            original_code = generated_files[filepath]
            
            # ä¿®æ­£æƒ…å ±ã‚’æ§‹ç¯‰
            fixes_info = []
            for issue in file_issues:
                fixes_info.append(
                    f"- Line {issue['line']}: Rename '{issue['reserved_word']}' to '{issue['suggested_name']}'"
                )
            
            # ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§å‚ç…§ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åé›†
            related_files = {}
            reserved_words = [i['reserved_word'] for i in file_issues]
            for other_path, other_content in generated_files.items():
                if other_path != filepath:
                    for rw in reserved_words:
                        if rw in other_content:
                            related_files[other_path] = other_content
                            break
            
            prompt = f"""Fix the following framework reserved word issues in this Python file.

**File: {filepath}**
**Issues:**
{chr(10).join(fixes_info)}

**Current Code:**
```python
{original_code}
```

**Related files that may need reference updates:**
"""
            for rpath, rcontent in list(related_files.items())[:3]:
                prompt += f"\n## {rpath}\n```python\n{rcontent[:2000]}\n```\n"
            
            prompt += """

**Instructions:**
1. Rename ONLY the reserved word column/field names to the suggested names
2. Update ALL references to these renamed attributes within this file
3. Do NOT change any other code or logic
4. Return the complete fixed file

Return ONLY the fixed Python code wrapped in ```python``` blocks.
"""
            
            try:
                logger.debug(f"[G-11] Fixing: {filepath}")
                response = self.llm_manager.generate_response(
                    prompt=prompt,
                    system_prompt="You are a Python code refactoring expert. Rename reserved words while preserving all functionality.",
                    max_tokens=8000
                )
                
                if hasattr(response, 'content'):
                    response_text = response.content
                else:
                    response_text = str(response)
                
                # ã‚³ãƒ¼ãƒ‰æŠ½å‡º
                code_match = re.search(r'```python\s*(.*?)\s*```', response_text, re.DOTALL)
                if code_match:
                    fixed_code = code_match.group(1).strip()
                    if len(fixed_code) > len(original_code) * 0.5:
                        generated_files[filepath] = fixed_code
                        logger.debug(f"[G-11] âœ… Fixed {filepath}")
                        
                        # é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã®å‚ç…§ã‚‚æ›´æ–°
                        for issue in file_issues:
                            old_name = issue['reserved_word']
                            new_name = issue['suggested_name']
                            for other_path in related_files:
                                if other_path in generated_files:
                                    other_content = generated_files[other_path]
                                    
                                    # å±æ€§ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³: .metadata â†’ .item_metadata
                                    other_content = re.sub(
                                        rf'\.{old_name}\b',
                                        f'.{new_name}',
                                        other_content
                                    )
                                    
                                    # Marshmallowã‚¹ã‚­ãƒ¼ãƒã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å®šç¾©ãƒ‘ã‚¿ãƒ¼ãƒ³
                                    # metadata = fields.XXX â†’ item_metadata = fields.XXX
                                    other_content = re.sub(
                                        rf'^([ \t]+){old_name}(\s*=\s*fields\.)',
                                        rf'\1{new_name}\2',
                                        other_content,
                                        flags=re.MULTILINE
                                    )
                                    
                                    # SQLAlchemyAutoSchemaã®å ´åˆã€model_fieldsã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚‚æ›´æ–°
                                    # 'metadata': ... â†’ 'item_metadata': ...
                                    other_content = re.sub(
                                        rf"(['\"]){old_name}(['\"])\s*:",
                                        rf"\1{new_name}\2:",
                                        other_content
                                    )
                                    
                                    generated_files[other_path] = other_content
                                    logger.debug(f"[G-11] Updated references in {other_path}")
                    else:
                        logger.debug(f"[G-11] âš  Generated content too short for {filepath}")
                else:
                    logger.debug(f"[G-11] âš  No code block found for {filepath}")
                    
            except Exception as e:
                logger.debug(f"[G-11] âš  Error fixing {filepath}: {e}")
        
        return generated_files

    # ============================================
    # G-12: Pythonçµ„ã¿è¾¼ã¿åã‚·ãƒ£ãƒ‰ã‚¦ã‚¤ãƒ³ã‚°ãƒã‚§ãƒƒã‚¯
    # ============================================
    
    def _validate_python_builtin_shadowing(
        self,
        generated_files: Dict[str, str]
    ) -> List[Dict[str, Any]]:
        """
        Pythonçµ„ã¿è¾¼ã¿åã®ã‚·ãƒ£ãƒ‰ã‚¦ã‚¤ãƒ³ã‚°ï¼ˆä¸Šæ›¸ãï¼‰ã‚’æ¤œå‡º
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã®ãƒªã‚¹ãƒˆ
        """
        import re
        import builtins
        
        issues = []
        
        # å±é™ºåº¦ã®é«˜ã„çµ„ã¿è¾¼ã¿åï¼ˆSQLAlchemyãƒ¢ãƒ‡ãƒ«ã§ã‚ˆãèª¤ç”¨ã•ã‚Œã‚‹ï¼‰
        HIGH_RISK_BUILTINS = {
            'id': 'item_id',
            'type': 'item_type',
            'list': 'items_list',
            'dict': 'data_dict',
            'str': 'text_str',
            'int': 'number_int',
            'bool': 'flag_bool',
            'set': 'items_set',
            'hash': 'hash_value',
            'input': 'user_input',
            'filter': 'data_filter',
            'format': 'text_format',
            'map': 'data_map',
            'object': 'base_object',
            'property': 'item_property',
            'range': 'value_range',
            'slice': 'data_slice',
            'sum': 'total_sum',
            'max': 'max_value',
            'min': 'min_value',
            'open': 'file_open',
            'file': 'file_obj',
            'dir': 'directory',
            'vars': 'variables',
            'globals': 'global_vars',
            'locals': 'local_vars',
        }
        
        for filepath, content in generated_files.items():
            if not filepath.endswith('.py'):
                continue
            
            # CRLFå¯¾å¿œï¼ˆWindowsæ”¹è¡Œã‚³ãƒ¼ãƒ‰ï¼‰
            content = content.replace('\r\n', '\n')
            
            # ã‚¯ãƒ©ã‚¹å®šç¾©å†…ã®å±æ€§ã‚’ãƒã‚§ãƒƒã‚¯
            class_pattern = r'class\s+(\w+)\s*\(([^)]*)\)\s*:'
            for class_match in re.finditer(class_pattern, content):
                class_name = class_match.group(1)
                class_parents = class_match.group(2)
                class_start = class_match.end()
                
                # Marshmallowã‚¹ã‚­ãƒ¼ãƒã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆAPIãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åãªã®ã§å¤‰æ›´ä¸å¯ï¼‰
                if 'Schema' in class_parents:
                    continue
                
                # ã‚¯ãƒ©ã‚¹æœ¬ä½“ã‚’æŠ½å‡ºï¼ˆæ¬¡ã®ã‚¯ãƒ©ã‚¹ã¾ãŸã¯ãƒ•ã‚¡ã‚¤ãƒ«æœ«å°¾ã¾ã§ï¼‰
                next_class = re.search(r'\nclass\s+\w+', content[class_start:])
                if next_class:
                    class_body = content[class_start:class_start + next_class.start()]
                else:
                    class_body = content[class_start:]
                
                # å±æ€§å®šç¾©ã‚’ãƒã‚§ãƒƒã‚¯
                for builtin_name, suggested_name in HIGH_RISK_BUILTINS.items():
                    # ãƒ‘ã‚¿ãƒ¼ãƒ³: builtin_name: Mapped[...] = ã¾ãŸã¯ builtin_name = Column(...)
                    # [ \t]+ ã‚’ä½¿ç”¨ï¼ˆ\s+ã¯æ”¹è¡Œã‚’å«ã‚€ãŸã‚ä½¿ç”¨ã—ãªã„ï¼‰
                    attr_pattern = rf'^[ \t]+{builtin_name}[ \t]*[:=]'
                    for match in re.finditer(attr_pattern, class_body, re.MULTILINE):
                        # çµ¶å¯¾ä½ç½®ã‚’è¨ˆç®—
                        abs_pos = class_start + match.start()
                        line_no = content[:abs_pos].count('\n') + 1
                        line_content = content.split('\n')[line_no - 1].strip()
                        
                        # 'id'ã¯ä¸»ã‚­ãƒ¼ã¨ã—ã¦ã‚ˆãä½¿ã‚ã‚Œã‚‹ãŸã‚ã€ç‰¹åˆ¥æ‰±ã„
                        if builtin_name == 'id' and 'primary_key' in line_content:
                            continue  # ä¸»ã‚­ãƒ¼ã¨ã—ã¦ã®idã¯è¨±å®¹
                        
                        # Marshmallowãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å®šç¾©ã¯é™¤å¤–ï¼ˆfields.Int, fields.Strç­‰ï¼‰
                        if 'fields.' in line_content:
                            continue
                        
                        issues.append({
                            'type': 'python_builtin_shadowing',
                            'file': filepath,
                            'line': line_no,
                            'class': class_name,
                            'builtin_name': builtin_name,
                            'suggested_name': suggested_name,
                            'code': line_content,
                            'details': f"'{builtin_name}' shadows Python builtin in class '{class_name}'"
                        })
                        logger.debug(f"[G-12] {filepath}:{line_no} - Builtin shadowing '{builtin_name}' in {class_name}")
        
        if issues:
            logger.debug(f"[G-12] Found {len(issues)} Python builtin shadowing issues")
        else:
            logger.debug("[G-12] No Python builtin shadowing issues found")
        
        return issues
    
    def _auto_fix_python_builtin_shadowing(
        self,
        issues: List[Dict[str, Any]],
        generated_files: Dict[str, str]
    ) -> Dict[str, str]:
        """
        Pythonçµ„ã¿è¾¼ã¿åã‚·ãƒ£ãƒ‰ã‚¦ã‚¤ãƒ³ã‚°ã‚’LLMã§è‡ªå‹•ä¿®æ­£
        """
        if not issues or not self.llm_manager:
            return generated_files
        
        # APIå¥‘ç´„ä¿è­·: ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã§å‚ç…§ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯å¤‰æ›´ã—ãªã„
        api_references = self._extract_frontend_api_field_references(generated_files)
        protected_issues = []
        safe_issues = []
        
        for issue in issues:
            field_name = issue.get('builtin_name', '')
            if self._is_field_referenced_in_frontend(field_name, generated_files, api_references):
                protected_issues.append(issue)
                logger.debug(f"[G-12] âš  Skipping '{field_name}' - referenced in frontend (API contract protection)")
            else:
                safe_issues.append(issue)
        
        if protected_issues:
            logger.debug(f"[G-12] Protected {len(protected_issues)} fields due to frontend references")
        
        # å®‰å…¨ã«å¤‰æ›´ã§ãã‚‹issuesã®ã¿å‡¦ç†
        files_to_fix = {}
        for issue in safe_issues:
            filepath = issue['file']
            if filepath not in files_to_fix:
                files_to_fix[filepath] = []
            files_to_fix[filepath].append(issue)
        
        for filepath, file_issues in files_to_fix.items():
            if filepath not in generated_files:
                continue
            
            original_code = generated_files[filepath]
            
            fixes_info = []
            for issue in file_issues:
                fixes_info.append(
                    f"- Line {issue['line']}: Rename '{issue['builtin_name']}' to '{issue['suggested_name']}' in class {issue['class']}"
                )
            
            prompt = f"""Fix Python builtin name shadowing issues in this file.

**File: {filepath}**
**Issues:**
{chr(10).join(fixes_info)}

**Current Code:**
```python
{original_code}
```

**Instructions:**
1. Rename the builtin-shadowing attribute names to the suggested names
2. Update ALL references to these renamed attributes within this file
3. Preserve primary key 'id' fields - do NOT rename those
4. Do NOT change any other code or logic

Return ONLY the fixed Python code wrapped in ```python``` blocks.
"""
            
            try:
                logger.debug(f"[G-12] Fixing: {filepath}")
                response = self.llm_manager.generate_response(
                    prompt=prompt,
                    system_prompt="You are a Python code refactoring expert. Rename builtin-shadowing names while preserving functionality.",
                    max_tokens=8000
                )
                
                if hasattr(response, 'content'):
                    response_text = response.content
                else:
                    response_text = str(response)
                
                code_match = re.search(r'```python\s*(.*?)\s*```', response_text, re.DOTALL)
                if code_match:
                    fixed_code = code_match.group(1).strip()
                    if len(fixed_code) > len(original_code) * 0.5:
                        generated_files[filepath] = fixed_code
                        logger.debug(f"[G-12] âœ… Fixed {filepath}")
                    else:
                        logger.debug(f"[G-12] âš  Generated content too short for {filepath}")
                else:
                    logger.debug(f"[G-12] âš  No code block found for {filepath}")
                    
            except Exception as e:
                logger.debug(f"[G-12] âš  Error fixing {filepath}: {e}")
        
        return generated_files

    # ============================================
    # G-13: Flask Blueprintãƒ«ãƒ¼ãƒˆé‡è¤‡ãƒã‚§ãƒƒã‚¯
    # ============================================
    
    def _validate_flask_route_duplicates(
        self,
        generated_files: Dict[str, str]
    ) -> List[Dict[str, Any]]:
        """
        Flask Blueprinté–“ã®ãƒ«ãƒ¼ãƒˆé‡è¤‡ã‚’æ¤œå‡º
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã®ãƒªã‚¹ãƒˆ
        """
        import re
        
        issues = []
        
        # Step 1: main.py/app.pyã‹ã‚‰register_blueprintã®url_prefixã‚’åé›†
        blueprint_prefixes = {}  # blueprintå¤‰æ•°å -> url_prefix
        
        for filepath, content in generated_files.items():
            if not filepath.endswith('.py'):
                continue
            
            # CRLFå¯¾å¿œ
            content = content.replace('\r\n', '\n')
            
            # register_blueprint(xxx_bp, url_prefix='/xxx') ã‚’æ¤œå‡º
            register_pattern = r'register_blueprint\s*\(\s*(\w+)\s*(?:,\s*url_prefix\s*=\s*[\'"]([^\'"]*)[\'"]\s*)?\)'
            for match in re.finditer(register_pattern, content):
                bp_var = match.group(1)  # blueprintå¤‰æ•°å (ä¾‹: inventory_bp)
                url_prefix = match.group(2) if match.group(2) else ''
                blueprint_prefixes[bp_var] = url_prefix
        
        # Step 2: å„ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰Blueprintå®šç¾©ã¨ãƒ«ãƒ¼ãƒˆã‚’åé›†
        all_routes = []
        
        for filepath, content in generated_files.items():
            if not filepath.endswith('.py'):
                continue
            
            content = content.replace('\r\n', '\n')
            
            # Blueprintå®šç¾©ã‚’æ¤œå‡º: xxx_bp = Blueprint(...)
            bp_var_in_file = None
            bp_def_match = re.search(r'(\w+)\s*=\s*Blueprint\s*\(', content)
            if bp_def_match:
                bp_var_in_file = bp_def_match.group(1)
            
            # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®Blueprintã®url_prefixã‚’å–å¾—
            file_prefix = ''
            if bp_var_in_file and bp_var_in_file in blueprint_prefixes:
                file_prefix = blueprint_prefixes[bp_var_in_file]
            
            # ãƒ«ãƒ¼ãƒˆãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã‚’æ¤œå‡º
            route_pattern = r'@(\w+)\.route\s*\(\s*[\'"]([^\'"]+)[\'"](?:.*?methods\s*=\s*\[([^\]]+)\])?'
            for match in re.finditer(route_pattern, content, re.DOTALL):
                decorator_bp = match.group(1)  # ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã®blueprintå (ä¾‹: inventory_bp)
                route_path = match.group(2)
                methods_str = match.group(3) if match.group(3) else "'GET'"
                
                # ãƒ¡ã‚½ãƒƒãƒ‰ã‚’è§£æ
                methods = re.findall(r"['\"](\w+)['\"]", methods_str)
                if not methods:
                    methods = ['GET']
                
                line_no = content[:match.start()].count('\n') + 1
                
                # url_prefixã‚’æ±ºå®šï¼ˆãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã®bpå > ãƒ•ã‚¡ã‚¤ãƒ«å†…ã®bpå®šç¾©ï¼‰
                route_prefix = ''
                if decorator_bp in blueprint_prefixes:
                    route_prefix = blueprint_prefixes[decorator_bp]
                elif file_prefix:
                    route_prefix = file_prefix
                
                full_route = route_prefix + route_path
                
                for method in methods:
                    all_routes.append({
                        'file': filepath,
                        'line': line_no,
                        'method': method.upper(),
                        'route': full_route,
                        'original_route': route_path,
                        'prefix': route_prefix
                    })
        
        # Step 3: é‡è¤‡ãƒã‚§ãƒƒã‚¯
        route_map = {}  # (method, route) -> [route_info, ...]
        for route_info in all_routes:
            key = (route_info['method'], route_info['route'])
            if key not in route_map:
                route_map[key] = []
            route_map[key].append(route_info)
        
        for (method, route), route_infos in route_map.items():
            if len(route_infos) > 1:
                # åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«å†…ã®é‡è¤‡ã¯ç„¡è¦–ï¼ˆãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã®é‡è¤‡æ¤œå‡ºèª¤ã‚Šé˜²æ­¢ï¼‰
                unique_files = set(r['file'] for r in route_infos)
                if len(unique_files) <= 1:
                    continue
                
                # é‡è¤‡ç™ºè¦‹
                for i, info in enumerate(route_infos):
                    other_files = [r['file'] for j, r in enumerate(route_infos) if j != i]
                    issues.append({
                        'type': 'flask_route_duplicate',
                        'file': info['file'],
                        'line': info['line'],
                        'method': method,
                        'route': route,
                        'original_route': info['original_route'],
                        'conflicting_files': other_files,
                        'details': f"Route '{method} {route}' is duplicated in {', '.join(other_files)}"
                    })
                    logger.debug(f"[G-13] {info['file']}:{info['line']} - Duplicate route '{method} {route}'")
        
        if issues:
            logger.debug(f"[G-13] Found {len(issues)} Flask route duplicate issues")
        else:
            logger.debug("[G-13] No Flask route duplicates found")
        
        return issues
    
    def _auto_fix_flask_route_duplicates(
        self,
        issues: List[Dict[str, Any]],
        generated_files: Dict[str, str]
    ) -> Dict[str, str]:
        """
        Flaskãƒ«ãƒ¼ãƒˆé‡è¤‡ã‚’LLMã§è‡ªå‹•ä¿®æ­£
        """
        if not issues or not self.llm_manager:
            return generated_files
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼ˆæœ€åˆã®ãƒ•ã‚¡ã‚¤ãƒ«ä»¥å¤–ã‚’ä¿®æ­£å¯¾è±¡ã¨ã™ã‚‹ï¼‰
        seen_routes = set()
        files_to_fix = {}
        
        for issue in issues:
            route_key = (issue['method'], issue['route'])
            if route_key in seen_routes:
                # 2ã¤ç›®ä»¥é™ã®é‡è¤‡ãƒ«ãƒ¼ãƒˆã‚’ä¿®æ­£å¯¾è±¡ã«
                filepath = issue['file']
                if filepath not in files_to_fix:
                    files_to_fix[filepath] = []
                files_to_fix[filepath].append(issue)
            else:
                seen_routes.add(route_key)
        
        for filepath, file_issues in files_to_fix.items():
            if filepath not in generated_files:
                continue
            
            original_code = generated_files[filepath]
            
            fixes_info = []
            for issue in file_issues:
                fixes_info.append(
                    f"- Line {issue['line']}: Route '{issue['method']} {issue['route']}' conflicts with {issue['conflicting_files']}"
                )
            
            # å¤‰æ›´å‰ã®ãƒ«ãƒ¼ãƒˆã‚’è¨˜éŒ²
            old_routes = {issue['route'] for issue in file_issues}
            
            prompt = f"""Fix duplicate Flask route issues in this file.

**File: {filepath}**
**Issues:**
{chr(10).join(fixes_info)}

**Current Code:**
```python
{original_code}
```

**Instructions:**
1. Rename the duplicate routes to unique paths (e.g., add a suffix or change the path structure)
2. Update any internal references to these routes
3. Keep the same functionality, just change the route path
4. Do NOT remove routes, only rename them

Return ONLY the fixed Python code wrapped in ```python``` blocks.
"""
            
            try:
                logger.debug(f"[G-13] Fixing: {filepath}")
                response = self.llm_manager.generate_response(
                    prompt=prompt,
                    system_prompt="You are a Flask routing expert. Fix duplicate routes while preserving functionality.",
                    max_tokens=8000
                )
                
                if hasattr(response, 'content'):
                    response_text = response.content
                else:
                    response_text = str(response)
                
                code_match = re.search(r'```python\s*(.*?)\s*```', response_text, re.DOTALL)
                if code_match:
                    fixed_code = code_match.group(1).strip()
                    if len(fixed_code) > len(original_code) * 0.5:
                        generated_files[filepath] = fixed_code
                        logger.debug(f"[G-13] âœ… Fixed {filepath}")
                        
                        # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã®fetch URLã‚‚æ›´æ–°
                        # æ–°ã—ã„ãƒ«ãƒ¼ãƒˆã‚’æŠ½å‡º
                        new_routes = set()
                        route_pattern = r"@\w+\.route\s*\(\s*['\"]([^'\"]+)['\"]"
                        for match in re.finditer(route_pattern, fixed_code):
                            new_routes.add(match.group(1))
                        
                        # å¤ã„ãƒ«ãƒ¼ãƒˆ â†’ æ–°ã—ã„ãƒ«ãƒ¼ãƒˆã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’æ¨æ¸¬
                        # (åŒã˜Blueprintã®url_prefixã‚’è€ƒæ…®)
                        for old_route in old_routes:
                            for new_route in new_routes:
                                # å¤ã„ãƒ«ãƒ¼ãƒˆãŒæ–°ã—ã„ãƒ«ãƒ¼ãƒˆã®éƒ¨åˆ†æ–‡å­—åˆ—ã§ãªã„å ´åˆã€å¤‰æ›´ã•ã‚ŒãŸå¯èƒ½æ€§
                                if old_route != new_route and old_route not in new_routes:
                                    # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
                                    for js_path, js_content in generated_files.items():
                                        if js_path.endswith(('.js', '.jsx', '.ts', '.tsx', '.html')):
                                            # Blueprint url_prefixã‚’å«ã‚€å®Œå…¨ãƒ‘ã‚¹ã‚’æ¨æ¸¬
                                            # fetch('/game/saves') ã®ã‚ˆã†ãªãƒ‘ã‚¿ãƒ¼ãƒ³
                                            updated_content = js_content
                                            
                                            # å˜ç´”ãªãƒ«ãƒ¼ãƒˆãƒ‘ã‚¹ç½®æ›
                                            updated_content = updated_content.replace(
                                                f"'{old_route}'",
                                                f"'{new_route}'"
                                            )
                                            updated_content = updated_content.replace(
                                                f'"{old_route}"',
                                                f'"{new_route}"'
                                            )
                                            updated_content = updated_content.replace(
                                                f"`{old_route}",
                                                f"`{new_route}"
                                            )
                                            
                                            if updated_content != js_content:
                                                generated_files[js_path] = updated_content
                                                logger.debug(f"[G-13] Updated frontend URL in {js_path}: {old_route} â†’ {new_route}")
                    else:
                        logger.debug(f"[G-13] âš  Generated content too short for {filepath}")
                else:
                    logger.debug(f"[G-13] âš  No code block found for {filepath}")
                    
            except Exception as e:
                logger.debug(f"[G-13] âš  Error fixing {filepath}: {e}")
        
        return generated_files

    # ============================================
    # G-14: SQLAlchemyãƒ†ãƒ¼ãƒ–ãƒ«åé‡è¤‡ãƒã‚§ãƒƒã‚¯
    # ============================================
    
    def _validate_sqlalchemy_table_duplicates(
        self,
        generated_files: Dict[str, str]
    ) -> List[Dict[str, Any]]:
        """
        SQLAlchemyãƒ†ãƒ¼ãƒ–ãƒ«åã®é‡è¤‡ã‚’æ¤œå‡º
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã®ãƒªã‚¹ãƒˆ
        """
        import re
        
        issues = []
        
        # å…¨ãƒ†ãƒ¼ãƒ–ãƒ«å®šç¾©ã‚’åé›†
        all_tables = []  # [(filepath, line_no, class_name, table_name)]
        
        for filepath, content in generated_files.items():
            if not filepath.endswith('.py'):
                continue
            
            # CRLFå¯¾å¿œï¼ˆWindowsæ”¹è¡Œã‚³ãƒ¼ãƒ‰ï¼‰
            content = content.replace('\r\n', '\n')
            
            # __tablename__ å®šç¾©ã‚’æ¤œå‡º
            pattern = r'class\s+(\w+)\s*\([^)]*(?:db\.Model|Base)[^)]*\)\s*:.*?__tablename__\s*=\s*[\'"]([^\'"]+)[\'"]'
            for match in re.finditer(pattern, content, re.DOTALL):
                class_name = match.group(1)
                table_name = match.group(2)
                line_no = content[:match.start()].count('\n') + 1
                
                all_tables.append({
                    'file': filepath,
                    'line': line_no,
                    'class': class_name,
                    'table': table_name
                })
        
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
        table_map = {}  # table_name -> [table_info, ...]
        for table_info in all_tables:
            table_name = table_info['table']
            if table_name not in table_map:
                table_map[table_name] = []
            table_map[table_name].append(table_info)
        
        for table_name, table_infos in table_map.items():
            if len(table_infos) > 1:
                for i, info in enumerate(table_infos):
                    other_classes = [f"{t['class']} in {t['file']}" for j, t in enumerate(table_infos) if j != i]
                    issues.append({
                        'type': 'sqlalchemy_table_duplicate',
                        'file': info['file'],
                        'line': info['line'],
                        'class': info['class'],
                        'table': table_name,
                        'conflicting_classes': other_classes,
                        'details': f"Table '{table_name}' is also defined in {', '.join(other_classes)}"
                    })
                    logger.debug(f"[G-14] {info['file']}:{info['line']} - Duplicate table '{table_name}'")
        
        if issues:
            logger.debug(f"[G-14] Found {len(issues)} SQLAlchemy table duplicate issues")
        else:
            logger.debug("[G-14] No SQLAlchemy table duplicates found")
        
        return issues
    
    def _auto_fix_sqlalchemy_table_duplicates(
        self,
        issues: List[Dict[str, Any]],
        generated_files: Dict[str, str]
    ) -> Dict[str, str]:
        """
        SQLAlchemyãƒ†ãƒ¼ãƒ–ãƒ«åé‡è¤‡ã‚’LLMã§è‡ªå‹•ä¿®æ­£
        """
        if not issues or not self.llm_manager:
            return generated_files
        
        # é‡è¤‡ãƒ†ãƒ¼ãƒ–ãƒ«ã®æœ€åˆä»¥å¤–ã‚’ä¿®æ­£
        seen_tables = set()
        files_to_fix = {}
        
        for issue in issues:
            table_name = issue['table']
            if table_name in seen_tables:
                filepath = issue['file']
                if filepath not in files_to_fix:
                    files_to_fix[filepath] = []
                files_to_fix[filepath].append(issue)
            else:
                seen_tables.add(table_name)
        
        for filepath, file_issues in files_to_fix.items():
            if filepath not in generated_files:
                continue
            
            original_code = generated_files[filepath]
            
            fixes_info = []
            for issue in file_issues:
                fixes_info.append(
                    f"- Class '{issue['class']}': Table name '{issue['table']}' conflicts with {issue['conflicting_classes']}"
                )
            
            prompt = f"""Fix duplicate SQLAlchemy table name issues in this file.

**File: {filepath}**
**Issues:**
{chr(10).join(fixes_info)}

**Current Code:**
```python
{original_code}
```

**Instructions:**
1. Rename the __tablename__ to a unique name (e.g., add a prefix or suffix based on context)
2. Update any ForeignKey references that point to this table
3. Keep the same class structure and functionality

Return ONLY the fixed Python code wrapped in ```python``` blocks.
"""
            
            try:
                logger.debug(f"[G-14] Fixing: {filepath}")
                response = self.llm_manager.generate_response(
                    prompt=prompt,
                    system_prompt="You are a SQLAlchemy expert. Fix duplicate table names while preserving relationships.",
                    max_tokens=8000
                )
                
                if hasattr(response, 'content'):
                    response_text = response.content
                else:
                    response_text = str(response)
                
                code_match = re.search(r'```python\s*(.*?)\s*```', response_text, re.DOTALL)
                if code_match:
                    fixed_code = code_match.group(1).strip()
                    if len(fixed_code) > len(original_code) * 0.5:
                        generated_files[filepath] = fixed_code
                        logger.debug(f"[G-14] âœ… Fixed {filepath}")
                    else:
                        logger.debug(f"[G-14] âš  Generated content too short for {filepath}")
                else:
                    logger.debug(f"[G-14] âš  No code block found for {filepath}")
                    
            except Exception as e:
                logger.debug(f"[G-14] âš  Error fixing {filepath}: {e}")
        
        return generated_files

    # ============================================
    # G-15: JavaScriptäºˆç´„èªãƒã‚§ãƒƒã‚¯
    # ============================================
    
    def _validate_javascript_reserved_words(
        self,
        generated_files: Dict[str, str]
    ) -> List[Dict[str, Any]]:
        """
        JavaScriptã®äºˆç´„èªã‚’å¤‰æ•°åã¨ã—ã¦ä½¿ç”¨ã—ã¦ã„ã‚‹ç®‡æ‰€ã‚’æ¤œå‡º
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã®ãƒªã‚¹ãƒˆ
        """
        import re
        
        issues = []
        
        # JavaScriptäºˆç´„èªï¼ˆå¤‰æ•°åã¨ã—ã¦ä½¿ç”¨ä¸å¯ã¾ãŸã¯å±é™ºï¼‰
        JS_RESERVED_WORDS = {
            # å³æ ¼ãƒ¢ãƒ¼ãƒ‰äºˆç´„èª
            'arguments': 'args',
            'eval': 'evalFunc',
            'implements': 'implementsInterface',
            'interface': 'interfaceObj',
            'package': 'packageObj',
            'private': 'privateField',
            'protected': 'protectedField',
            'public': 'publicField',
            'static': 'staticField',
            'yield': 'yieldValue',
            'let': 'letVar',
            'await': 'awaitResult',
            # å°†æ¥ã®äºˆç´„èª
            'enum': 'enumValue',
            # ã‚ˆãèª¤ç”¨ã•ã‚Œã‚‹åå‰
            'class': 'className',
            'function': 'funcObj',
            'delete': 'deleteItem',
            'export': 'exportData',
            'import': 'importData',
            'super': 'superClass',
            'extends': 'extendsClass',
            'default': 'defaultValue',
            'case': 'caseValue',
            'catch': 'catchError',
            'finally': 'finallyBlock',
            'throw': 'throwError',
            'try': 'tryBlock',
            'debugger': 'debuggerTool',
            'with': 'withObj',
        }
        
        for filepath, content in generated_files.items():
            if not filepath.endswith(('.js', '.jsx', '.ts', '.tsx')):
                continue
            
            # CRLFå¯¾å¿œï¼ˆWindowsæ”¹è¡Œã‚³ãƒ¼ãƒ‰ï¼‰
            content = content.replace('\r\n', '\n')
            
            for reserved_word, suggested_name in JS_RESERVED_WORDS.items():
                # å¤‰æ•°å®£è¨€ãƒ‘ã‚¿ãƒ¼ãƒ³: var/let/const reserved_word =
                var_pattern = rf'\b(var|let|const)\s+{reserved_word}\s*='
                for match in re.finditer(var_pattern, content):
                    line_no = content[:match.start()].count('\n') + 1
                    line_content = content.split('\n')[line_no - 1].strip()
                    
                    issues.append({
                        'type': 'javascript_reserved_word',
                        'file': filepath,
                        'line': line_no,
                        'reserved_word': reserved_word,
                        'suggested_name': suggested_name,
                        'code': line_content,
                        'details': f"'{reserved_word}' is a reserved word in JavaScript"
                    })
                    logger.debug(f"[G-15] {filepath}:{line_no} - JS reserved word '{reserved_word}'")
                
                # é–¢æ•°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³: function xxx(reserved_word) ã¾ãŸã¯ (reserved_word) =>
                param_pattern = rf'(?:function\s+\w*\s*\([^)]*\b{reserved_word}\b[^)]*\)|\([^)]*\b{reserved_word}\b[^)]*\)\s*=>)'
                for match in re.finditer(param_pattern, content):
                    line_no = content[:match.start()].count('\n') + 1
                    line_content = content.split('\n')[line_no - 1].strip()
                    
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    existing = [i for i in issues if i['file'] == filepath and i['line'] == line_no and i['reserved_word'] == reserved_word]
                    if not existing:
                        issues.append({
                            'type': 'javascript_reserved_word_param',
                            'file': filepath,
                            'line': line_no,
                            'reserved_word': reserved_word,
                            'suggested_name': suggested_name,
                            'code': line_content,
                            'details': f"'{reserved_word}' used as parameter name"
                        })
                        logger.debug(f"[G-15] {filepath}:{line_no} - JS reserved word as param '{reserved_word}'")
        
        if issues:
            logger.debug(f"[G-15] Found {len(issues)} JavaScript reserved word issues")
        else:
            logger.debug("[G-15] No JavaScript reserved word issues found")
        
        return issues
    
    def _auto_fix_javascript_reserved_words(
        self,
        issues: List[Dict[str, Any]],
        generated_files: Dict[str, str]
    ) -> Dict[str, str]:
        """
        JavaScriptäºˆç´„èªã®å•é¡Œã‚’LLMã§è‡ªå‹•ä¿®æ­£
        """
        if not issues or not self.llm_manager:
            return generated_files
        
        files_to_fix = {}
        for issue in issues:
            filepath = issue['file']
            if filepath not in files_to_fix:
                files_to_fix[filepath] = []
            files_to_fix[filepath].append(issue)
        
        for filepath, file_issues in files_to_fix.items():
            if filepath not in generated_files:
                continue
            
            original_code = generated_files[filepath]
            
            fixes_info = []
            for issue in file_issues:
                fixes_info.append(
                    f"- Line {issue['line']}: Rename '{issue['reserved_word']}' to '{issue['suggested_name']}'"
                )
            
            prompt = f"""Fix JavaScript reserved word issues in this file.

**File: {filepath}**
**Issues:**
{chr(10).join(fixes_info)}

**Current Code:**
```javascript
{original_code}
```

**Instructions:**
1. Rename the reserved word variables/parameters to the suggested names
2. Update ALL references to these renamed variables within this file
3. Do NOT change any other code or logic

Return ONLY the fixed JavaScript code wrapped in ```javascript``` blocks.
"""
            
            try:
                logger.debug(f"[G-15] Fixing: {filepath}")
                response = self.llm_manager.generate_response(
                    prompt=prompt,
                    system_prompt="You are a JavaScript code refactoring expert. Rename reserved words while preserving functionality.",
                    max_tokens=8000
                )
                
                if hasattr(response, 'content'):
                    response_text = response.content
                else:
                    response_text = str(response)
                
                code_match = re.search(r'```(?:javascript|js)\s*(.*?)\s*```', response_text, re.DOTALL)
                if code_match:
                    fixed_code = code_match.group(1).strip()
                    if len(fixed_code) > len(original_code) * 0.5:
                        generated_files[filepath] = fixed_code
                        logger.debug(f"[G-15] âœ… Fixed {filepath}")
                    else:
                        logger.debug(f"[G-15] âš  Generated content too short for {filepath}")
                else:
                    logger.debug(f"[G-15] âš  No code block found for {filepath}")
                    
            except Exception as e:
                logger.debug(f"[G-15] âš  Error fixing {filepath}: {e}")
        
        return generated_files

    # ============================================
    # G-16: SQLAlchemy relationshipå‚ç…§å…ˆæ¤œè¨¼
    # ============================================
    
    def _validate_relationship_references(
        self,
        generated_files: Dict[str, str]
    ) -> List[Dict[str, Any]]:
        """
        SQLAlchemy relationship()ã®å‚ç…§å…ˆãŒæœ‰åŠ¹ãªModelã‚¯ãƒ©ã‚¹ã‹æ¤œè¨¼
        
        æ¤œå‡ºå¯¾è±¡:
        - relationshipå‚ç…§å…ˆãŒå­˜åœ¨ã—ãªã„ã‚¯ãƒ©ã‚¹
        - relationshipå‚ç…§å…ˆãŒEnumãªã©éModelã‚¯ãƒ©ã‚¹
        - back_populatesã®ä¸æ•´åˆ
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã®ãƒªã‚¹ãƒˆ
        """
        import re
        
        issues = []
        
        # Step 1: å…¨Modelã‚¯ãƒ©ã‚¹ã‚’åé›†
        model_classes = set()
        enum_classes = set()
        
        for filepath, content in generated_files.items():
            if not filepath.endswith('.py'):
                continue
            
            # CRLFå¯¾å¿œ
            content = content.replace('\r\n', '\n')
            
            # db.Model ã¾ãŸã¯ Base ã‚’ç¶™æ‰¿ã—ãŸã‚¯ãƒ©ã‚¹ã‚’æ¤œå‡º
            model_pattern = r'class\s+(\w+)\s*\([^)]*(?:db\.Model|Base)[^)]*\)\s*:'
            for match in re.finditer(model_pattern, content):
                model_classes.add(match.group(1))
            
            # Enumã‚¯ãƒ©ã‚¹ã‚’æ¤œå‡ºï¼ˆfrom enum import Enum ã¾ãŸã¯ PyEnumï¼‰
            enum_pattern = r'class\s+(\w+)\s*\([^)]*(?:Enum|PyEnum|IntEnum|StrEnum)[^)]*\)\s*:'
            for match in re.finditer(enum_pattern, content):
                enum_classes.add(match.group(1))
        
        logger.debug(f"[G-16] Found Model classes: {model_classes}")
        logger.debug(f"[G-16] Found Enum classes: {enum_classes}")
        
        # Step 2: relationship()ã®å‚ç…§å…ˆã‚’æ¤œè¨¼
        for filepath, content in generated_files.items():
            if not filepath.endswith('.py'):
                continue
            
            # CRLFå¯¾å¿œ
            content = content.replace('\r\n', '\n')
            
            # relationship('ClassName') ã¾ãŸã¯ relationship("ClassName") ã‚’æ¤œå‡º
            rel_pattern = r'relationship\s*\(\s*[\'"](\w+)[\'"]'
            for match in re.finditer(rel_pattern, content):
                ref_class = match.group(1)
                line_no = content[:match.start()].count('\n') + 1
                line_content = content.split('\n')[line_no - 1].strip()
                
                # å‚ç…§å…ˆãŒEnumã®å ´åˆ
                if ref_class in enum_classes:
                    issues.append({
                        'type': 'relationship_references_enum',
                        'file': filepath,
                        'line': line_no,
                        'referenced_class': ref_class,
                        'code': line_content,
                        'details': f"relationship() references Enum '{ref_class}', not a Model class",
                        'suggested_fix': f"Change to the correct Model class (e.g., if this is for quests, use 'Quest' instead of '{ref_class}')"
                    })
                    logger.debug(f"[G-16] {filepath}:{line_no} - relationship references Enum '{ref_class}'")
                
                # å‚ç…§å…ˆãŒModelã§ã‚‚Enumã§ã‚‚ãªã„ï¼ˆå­˜åœ¨ã—ãªã„ã‚¯ãƒ©ã‚¹ï¼‰
                elif ref_class not in model_classes and ref_class not in enum_classes:
                    # TYPE_CHECKINGãƒ–ãƒ­ãƒƒã‚¯å†…ã®å‰æ–¹å‚ç…§ã¯è¨±å®¹
                    # ãŸã ã—ã€Modelã‚¯ãƒ©ã‚¹ãƒªã‚¹ãƒˆã«ãªã„ã‚‚ã®ã¯è­¦å‘Š
                    issues.append({
                        'type': 'relationship_references_unknown',
                        'file': filepath,
                        'line': line_no,
                        'referenced_class': ref_class,
                        'code': line_content,
                        'details': f"relationship() references unknown class '{ref_class}'",
                        'suggested_fix': f"Verify that '{ref_class}' is a valid Model class and is imported correctly"
                    })
                    logger.debug(f"[G-16] {filepath}:{line_no} - relationship references unknown class '{ref_class}'")
        
        # Step 3: back_populatesã®æ•´åˆæ€§æ¤œè¨¼
        back_populates_map = {}  # {(class_name, attr_name): (filepath, line_no, target_class, target_attr)}
        
        for filepath, content in generated_files.items():
            if not filepath.endswith('.py'):
                continue
            
            content = content.replace('\r\n', '\n')
            
            # ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹åã‚’è¿½è·¡
            current_class = None
            for i, line in enumerate(content.split('\n'), 1):
                class_match = re.match(r'class\s+(\w+)\s*\(', line)
                if class_match:
                    current_class = class_match.group(1)
                
                # relationship with back_populates
                # ãƒ‘ã‚¿ãƒ¼ãƒ³1: type annotationå½¢å¼ (attr: Mapped[...] = relationship(...))
                bp_match = re.search(r'(\w+)\s*:.*=\s*relationship\s*\(\s*[\'"](\w+)[\'"].*back_populates\s*=\s*[\'"](\w+)[\'"]', line)
                if not bp_match:
                    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ç›´æ¥ä»£å…¥å½¢å¼ (attr = relationship(...))
                    bp_match = re.search(r'(\w+)\s*=\s*relationship\s*\(\s*[\'"](\w+)[\'"].*back_populates\s*=\s*[\'"](\w+)[\'"]', line)
                if bp_match and current_class:
                    attr_name = bp_match.group(1)
                    target_class = bp_match.group(2)
                    target_attr = bp_match.group(3)
                    back_populates_map[(current_class, attr_name)] = (filepath, i, target_class, target_attr)
        
        # back_populatesã®ç›¸äº’å‚ç…§ã‚’æ¤œè¨¼
        # å„ã‚¯ãƒ©ã‚¹ã®å±æ€§ã‚’åé›†ï¼ˆrelationshipä»¥å¤–ã‚‚å«ã‚€ï¼‰
        class_attributes = {}  # {class_name: {attr_names}}
        for filepath, content in generated_files.items():
            if not filepath.endswith('.py'):
                continue
            content = content.replace('\r\n', '\n')
            current_class = None
            for line in content.split('\n'):
                class_match = re.match(r'class\s+(\w+)\s*\(', line)
                if class_match:
                    current_class = class_match.group(1)
                    if current_class not in class_attributes:
                        class_attributes[current_class] = set()
                
                # å±æ€§å®šç¾©ã‚’æ¤œå‡ºï¼ˆMapped[...] = ã¾ãŸã¯ = relationship/Columnç­‰ï¼‰
                # ãƒ‘ã‚¿ãƒ¼ãƒ³1: type annotationå½¢å¼ (attr: Mapped[...] = ...)
                if current_class:
                    attr_match = re.match(r'^\s+(\w+)\s*:', line)
                    if attr_match and not attr_match.group(1).startswith('_'):
                        class_attributes[current_class].add(attr_match.group(1))
                    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ç›´æ¥ä»£å…¥å½¢å¼ (attr = ...)
                    else:
                        attr_match2 = re.match(r'^\s+(\w+)\s*=', line)
                        if attr_match2 and not attr_match2.group(1).startswith('_'):
                            class_attributes[current_class].add(attr_match2.group(1))
        
        for (class_name, attr_name), (filepath, line_no, target_class, target_attr) in back_populates_map.items():
            # ç›¸æ‰‹å´ã®å®šç¾©ã‚’æ¢ã™
            expected_key = (target_class, target_attr)
            if expected_key in back_populates_map:
                reverse_info = back_populates_map[expected_key]
                reverse_target_class = reverse_info[2]
                reverse_target_attr = reverse_info[3]
                
                # ç›¸æ‰‹å´ãŒè‡ªåˆ†ã‚’æŒ‡ã—ã¦ã„ãªã„å ´åˆ
                if reverse_target_class != class_name or reverse_target_attr != attr_name:
                    issues.append({
                        'type': 'back_populates_mismatch',
                        'file': filepath,
                        'line': line_no,
                        'class': class_name,
                        'attr': attr_name,
                        'target_class': target_class,
                        'target_attr': target_attr,
                        'details': f"back_populates mismatch: {class_name}.{attr_name} -> {target_class}.{target_attr}, but {target_class}.{target_attr} -> {reverse_target_class}.{reverse_target_attr}",
                        'suggested_fix': f"Ensure {target_class}.{target_attr} has back_populates='{attr_name}'"
                    })
                    logger.debug(f"[G-16] {filepath}:{line_no} - back_populates mismatch")
            else:
                # ç›¸æ‰‹å´ã«back_populatesãŒãªã„å ´åˆã€å±æ€§è‡ªä½“ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                target_attrs = class_attributes.get(target_class, set())
                if target_attr not in target_attrs:
                    issues.append({
                        'type': 'back_populates_target_missing',
                        'file': filepath,
                        'line': line_no,
                        'class': class_name,
                        'attr': attr_name,
                        'target_class': target_class,
                        'target_attr': target_attr,
                        'details': f"back_populates target missing: {class_name}.{attr_name} references {target_class}.{target_attr}, but '{target_attr}' does not exist in {target_class}",
                        'suggested_fix': f"Add 'player_inventories: Mapped[List[\"{class_name}\"]] = relationship(\"{class_name}\", back_populates=\"{attr_name}\")' to {target_class}"
                    })
                    logger.debug(f"[G-16] {filepath}:{line_no} - back_populates target '{target_attr}' missing in {target_class}")
        
        if issues:
            logger.debug(f"[G-16] Found {len(issues)} relationship reference issues")
        else:
            logger.debug("[G-16] No relationship reference issues found")
        
        return issues
    
    def _auto_fix_relationship_references(
        self,
        issues: List[Dict[str, Any]],
        generated_files: Dict[str, str]
    ) -> Dict[str, str]:
        """
        relationshipå‚ç…§å…ˆã®å•é¡Œã‚’LLMã§è‡ªå‹•ä¿®æ­£
        """
        if not issues or not self.llm_manager:
            return generated_files
        
        # ã‚¯ãƒ©ã‚¹åã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ç‰¹å®šã™ã‚‹ãƒãƒƒãƒ—ã‚’æ§‹ç¯‰
        class_to_file = {}
        for filepath, content in generated_files.items():
            if filepath.endswith('.py'):
                content = content.replace('\r\n', '\n')
                model_pattern = r'class\s+(\w+)\s*\([^)]*(?:db\.Model|Base)[^)]*\)\s*:'
                for match in re.finditer(model_pattern, content):
                    class_to_file[match.group(1)] = filepath
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼ˆback_populates_target_missingã¯ä¿®æ­£å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¤‰æ›´ï¼‰
        files_to_fix = {}
        for issue in issues:
            if issue['type'] == 'back_populates_target_missing':
                # ä¿®æ­£å¯¾è±¡ã¯ target_class ã®ãƒ•ã‚¡ã‚¤ãƒ«
                target_class = issue.get('target_class', '')
                target_filepath = class_to_file.get(target_class)
                if target_filepath:
                    if target_filepath not in files_to_fix:
                        files_to_fix[target_filepath] = []
                    files_to_fix[target_filepath].append(issue)
            else:
                filepath = issue['file']
                if filepath not in files_to_fix:
                    files_to_fix[filepath] = []
                files_to_fix[filepath].append(issue)
        
        # å…¨Modelã‚¯ãƒ©ã‚¹æƒ…å ±ã‚’åé›†
        model_info = []
        for filepath, content in generated_files.items():
            if filepath.endswith('.py'):
                content = content.replace('\r\n', '\n')
                model_pattern = r'class\s+(\w+)\s*\([^)]*(?:db\.Model|Base)[^)]*\)\s*:'
                for match in re.finditer(model_pattern, content):
                    model_info.append(f"- {match.group(1)} (in {filepath})")
        
        for filepath, file_issues in files_to_fix.items():
            if filepath not in generated_files:
                continue
            
            original_code = generated_files[filepath]
            
            fixes_info = []
            for issue in file_issues:
                if issue['type'] == 'relationship_references_enum':
                    fixes_info.append(
                        f"- Line {issue['line']}: relationship('{issue['referenced_class']}') references an Enum, not a Model. "
                        f"Change to the correct Model class."
                    )
                elif issue['type'] == 'relationship_references_unknown':
                    fixes_info.append(
                        f"- Line {issue['line']}: relationship('{issue['referenced_class']}') references unknown class. "
                        f"Verify and fix the class name."
                    )
                elif issue['type'] == 'back_populates_mismatch':
                    fixes_info.append(
                        f"- Line {issue['line']}: back_populates mismatch for {issue['class']}.{issue['attr']}. "
                        f"{issue['details']}"
                    )
                elif issue['type'] == 'back_populates_target_missing':
                    fixes_info.append(
                        f"- {issue['target_class']} is missing the '{issue['target_attr']}' relationship property. "
                        f"Add: {issue['target_attr']}: Mapped[List[\"{issue['class']}\"]] = relationship(\"{issue['class']}\", back_populates=\"{issue['attr']}\")"
                    )
            
            prompt = f"""Fix the SQLAlchemy relationship reference issues in this Python file.

**File: {filepath}**
**Issues:**
{chr(10).join(fixes_info)}

**Available Model classes in this project:**
{chr(10).join(model_info)}

**Current Code:**
```python
{original_code}
```

**Instructions:**
1. Fix relationship() to reference the correct Model class (not Enum)
2. For example, if a Player has quests, the relationship should be relationship('Quest'), not relationship('QuestStatus')
3. Ensure back_populates attributes match on both sides of the relationship
4. Update the Mapped type hints to match (e.g., Mapped[list["Quest"]] not Mapped[list["QuestStatus"]])
5. Do NOT change any other code or logic

Return ONLY the fixed Python code wrapped in ```python``` blocks.
"""
            
            try:
                logger.debug(f"[G-16] Fixing: {filepath}")
                response = self.llm_manager.generate_response(
                    prompt=prompt,
                    system_prompt="You are a SQLAlchemy expert. Fix relationship references to point to valid Model classes.",
                    max_tokens=8000
                )
                
                if hasattr(response, 'content'):
                    response_text = response.content
                else:
                    response_text = str(response)
                
                code_match = re.search(r'```python\s*(.*?)\s*```', response_text, re.DOTALL)
                if code_match:
                    fixed_code = code_match.group(1).strip()
                    if len(fixed_code) > len(original_code) * 0.5:
                        generated_files[filepath] = fixed_code
                        logger.debug(f"[G-16] âœ… Fixed {filepath}")
                    else:
                        logger.debug(f"[G-16] âš  Generated content too short for {filepath}")
                else:
                    logger.debug(f"[G-16] âš  No code block found for {filepath}")
                    
            except Exception as e:
                logger.debug(f"[G-16] âš  Error fixing {filepath}: {e}")
        
        return generated_files

    # ============================================
    # G-17: ForeignKeyå‚ç…§å…ˆæ¤œè¨¼
    # ============================================
    
    def _validate_foreignkey_references(
        self,
        generated_files: Dict[str, str]
    ) -> List[Dict[str, Any]]:
        """
        ForeignKey()ã®å‚ç…§å…ˆãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹æ¤œè¨¼
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã®ãƒªã‚¹ãƒˆ
        """
        import re
        
        issues = []
        
        # Step 1: å…¨ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’åé›†
        table_names = set()
        
        for filepath, content in generated_files.items():
            if not filepath.endswith('.py'):
                continue
            
            content = content.replace('\r\n', '\n')
            
            # __tablename__ ã‚’æ¤œå‡º
            tablename_pattern = r"__tablename__\s*=\s*['\"]([^'\"]+)['\"]"
            for match in re.finditer(tablename_pattern, content):
                table_names.add(match.group(1))
        
        logger.debug(f"[G-17] Found tables: {table_names}")
        
        # Step 2: ForeignKeyå‚ç…§å…ˆã‚’æ¤œè¨¼
        for filepath, content in generated_files.items():
            if not filepath.endswith('.py'):
                continue
            
            content = content.replace('\r\n', '\n')
            
            # ForeignKey('table.column') ã¾ãŸã¯ ForeignKey("table.column") ã‚’æ¤œå‡º
            fk_pattern = r"ForeignKey\s*\(\s*['\"]([^'\"]+)['\"]"
            for match in re.finditer(fk_pattern, content):
                fk_ref = match.group(1)
                # table.column å½¢å¼ã‹ã‚‰ table éƒ¨åˆ†ã‚’æŠ½å‡º
                if '.' in fk_ref:
                    ref_table = fk_ref.split('.')[0]
                else:
                    ref_table = fk_ref
                
                line_no = content[:match.start()].count('\n') + 1
                line_content = content.split('\n')[line_no - 1].strip()
                
                if ref_table not in table_names:
                    issues.append({
                        'type': 'foreignkey_references_unknown_table',
                        'file': filepath,
                        'line': line_no,
                        'referenced_table': ref_table,
                        'full_reference': fk_ref,
                        'code': line_content,
                        'details': f"ForeignKey references unknown table '{ref_table}'",
                        'available_tables': list(table_names)
                    })
                    logger.debug(f"[G-17] {filepath}:{line_no} - ForeignKey references unknown table '{ref_table}'")
        
        if issues:
            logger.debug(f"[G-17] Found {len(issues)} ForeignKey reference issues")
        else:
            logger.debug("[G-17] No ForeignKey reference issues found")
        
        return issues
    
    def _auto_fix_foreignkey_references(
        self,
        issues: List[Dict[str, Any]],
        generated_files: Dict[str, str]
    ) -> Dict[str, str]:
        """
        ForeignKeyå‚ç…§å…ˆã®å•é¡Œã‚’LLMã§è‡ªå‹•ä¿®æ­£
        """
        if not issues or not self.llm_manager:
            return generated_files
        
        files_to_fix = {}
        for issue in issues:
            filepath = issue['file']
            if filepath not in files_to_fix:
                files_to_fix[filepath] = []
            files_to_fix[filepath].append(issue)
        
        for filepath, file_issues in files_to_fix.items():
            if filepath not in generated_files:
                continue
            
            original_code = generated_files[filepath]
            available_tables = file_issues[0].get('available_tables', [])
            
            fixes_info = []
            for issue in file_issues:
                fixes_info.append(
                    f"- Line {issue['line']}: ForeignKey('{issue['full_reference']}') references unknown table '{issue['referenced_table']}'"
                )
            
            prompt = f"""Fix the ForeignKey reference issues in this Python file.

**File: {filepath}**
**Issues:**
{chr(10).join(fixes_info)}

**Available tables in this project:**
{', '.join(available_tables)}

**Current Code:**
```python
{original_code}
```

**Instructions:**
1. Fix ForeignKey() to reference an existing table
2. Ensure the column name after the dot is correct (usually 'id')
3. Do NOT change any other code or logic

Return ONLY the fixed Python code wrapped in ```python``` blocks.
"""
            
            try:
                logger.debug(f"[G-17] Fixing: {filepath}")
                response = self.llm_manager.generate_response(
                    prompt=prompt,
                    system_prompt="You are a SQLAlchemy expert. Fix ForeignKey references to point to valid tables.",
                    max_tokens=8000
                )
                
                if hasattr(response, 'content'):
                    response_text = response.content
                else:
                    response_text = str(response)
                
                code_match = re.search(r'```python\s*(.*?)\s*```', response_text, re.DOTALL)
                if code_match:
                    fixed_code = code_match.group(1).strip()
                    if len(fixed_code) > len(original_code) * 0.5:
                        generated_files[filepath] = fixed_code
                        logger.debug(f"[G-17] âœ… Fixed {filepath}")
                    else:
                        logger.debug(f"[G-17] âš  Generated content too short for {filepath}")
                else:
                    logger.debug(f"[G-17] âš  No code block found for {filepath}")
                    
            except Exception as e:
                logger.debug(f"[G-17] âš  Error fixing {filepath}: {e}")
        
        return generated_files

    # ============================================
    # G-18: SQLAlchemy Enumå®šç¾©æ¤œè¨¼
    # ============================================
    
    def _validate_sqlalchemy_enum_usage(
        self,
        generated_files: Dict[str, str]
    ) -> List[Dict[str, Any]]:
        """
        SQLAlchemy Enumã®ä½¿ç”¨æ–¹æ³•ãŒæ­£ã—ã„ã‹æ¤œè¨¼
        
        æ¤œå‡ºå¯¾è±¡:
        - Enum()ã«æ¸¡ã•ã‚ŒãŸã‚¯ãƒ©ã‚¹ãŒPython Enumã‚’ç¶™æ‰¿ã—ã¦ã„ãªã„
        - Enumã‚«ãƒ©ãƒ ã®å®šç¾©ãƒŸã‚¹
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã®ãƒªã‚¹ãƒˆ
        """
        import re
        
        issues = []
        
        # Step 1: å…¨Enumã‚¯ãƒ©ã‚¹ã‚’åé›†
        enum_classes = set()
        
        for filepath, content in generated_files.items():
            if not filepath.endswith('.py'):
                continue
            
            content = content.replace('\r\n', '\n')
            
            # Python Enumã‚’ç¶™æ‰¿ã—ãŸã‚¯ãƒ©ã‚¹ã‚’æ¤œå‡º
            enum_pattern = r'class\s+(\w+)\s*\([^)]*(?:Enum|PyEnum|IntEnum|StrEnum)[^)]*\)\s*:'
            for match in re.finditer(enum_pattern, content):
                enum_classes.add(match.group(1))
        
        logger.debug(f"[G-18] Found Enum classes: {enum_classes}")
        
        # Step 2: Enum()ã®ä½¿ç”¨ã‚’æ¤œè¨¼
        for filepath, content in generated_files.items():
            if not filepath.endswith('.py'):
                continue
            
            content = content.replace('\r\n', '\n')
            
            # Enum(ClassName) ã‚’æ¤œå‡ºï¼ˆSQLAlchemy Enumå‹ï¼‰
            # ãƒ‘ã‚¿ãƒ¼ãƒ³: Enum(ClassName) ã¾ãŸã¯ Enum('value1', 'value2')
            enum_usage_pattern = r'Enum\s*\(\s*(\w+)\s*[,)]'
            for match in re.finditer(enum_usage_pattern, content):
                ref_class = match.group(1)
                line_no = content[:match.start()].count('\n') + 1
                line_content = content.split('\n')[line_no - 1].strip()
                
                # æ–‡å­—åˆ—ãƒªãƒ†ãƒ©ãƒ«ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆEnum('active', 'inactive')å½¢å¼ï¼‰
                if ref_class.startswith("'") or ref_class.startswith('"'):
                    continue
                
                # å‚ç…§å…ˆãŒEnumã‚¯ãƒ©ã‚¹ã§ãªã„å ´åˆ
                if ref_class not in enum_classes:
                    # ä¸€èˆ¬çš„ãªPythonå‹åã¯ã‚¹ã‚­ãƒƒãƒ—
                    if ref_class in ['str', 'int', 'float', 'bool', 'String', 'Integer']:
                        continue
                    
                    issues.append({
                        'type': 'enum_references_non_enum',
                        'file': filepath,
                        'line': line_no,
                        'referenced_class': ref_class,
                        'code': line_content,
                        'details': f"Enum() references '{ref_class}' which is not a Python Enum class",
                        'available_enums': list(enum_classes)
                    })
                    logger.debug(f"[G-18] {filepath}:{line_no} - Enum references non-Enum class '{ref_class}'")
        
        if issues:
            logger.debug(f"[G-18] Found {len(issues)} Enum usage issues")
        else:
            logger.debug("[G-18] No Enum usage issues found")
        
        return issues
    
    def _auto_fix_sqlalchemy_enum_usage(
        self,
        issues: List[Dict[str, Any]],
        generated_files: Dict[str, str]
    ) -> Dict[str, str]:
        """
        Enumä½¿ç”¨ã®å•é¡Œã‚’LLMã§è‡ªå‹•ä¿®æ­£
        """
        if not issues or not self.llm_manager:
            return generated_files
        
        files_to_fix = {}
        for issue in issues:
            filepath = issue['file']
            if filepath not in files_to_fix:
                files_to_fix[filepath] = []
            files_to_fix[filepath].append(issue)
        
        for filepath, file_issues in files_to_fix.items():
            if filepath not in generated_files:
                continue
            
            original_code = generated_files[filepath]
            available_enums = file_issues[0].get('available_enums', [])
            
            fixes_info = []
            for issue in file_issues:
                fixes_info.append(
                    f"- Line {issue['line']}: Enum({issue['referenced_class']}) references a non-Enum class"
                )
            
            prompt = f"""Fix the SQLAlchemy Enum usage issues in this Python file.

**File: {filepath}**
**Issues:**
{chr(10).join(fixes_info)}

**Available Python Enum classes in this project:**
{', '.join(available_enums)}

**Current Code:**
```python
{original_code}
```

**Instructions:**
1. Fix Enum() to reference a valid Python Enum class
2. Or create a new Enum class if needed
3. Ensure proper import of the Enum class
4. Do NOT change any other code or logic

Return ONLY the fixed Python code wrapped in ```python``` blocks.
"""
            
            try:
                logger.debug(f"[G-18] Fixing: {filepath}")
                response = self.llm_manager.generate_response(
                    prompt=prompt,
                    system_prompt="You are a SQLAlchemy expert. Fix Enum usage to reference valid Python Enum classes.",
                    max_tokens=8000
                )
                
                if hasattr(response, 'content'):
                    response_text = response.content
                else:
                    response_text = str(response)
                
                code_match = re.search(r'```python\s*(.*?)\s*```', response_text, re.DOTALL)
                if code_match:
                    fixed_code = code_match.group(1).strip()
                    if len(fixed_code) > len(original_code) * 0.5:
                        generated_files[filepath] = fixed_code
                        logger.debug(f"[G-18] âœ… Fixed {filepath}")
                    else:
                        logger.debug(f"[G-18] âš  Generated content too short for {filepath}")
                else:
                    logger.debug(f"[G-18] âš  No code block found for {filepath}")
                    
            except Exception as e:
                logger.debug(f"[G-18] âš  Error fixing {filepath}: {e}")
        
        return generated_files

    # ============================================
    # G-19: Mappedå‹ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ•´åˆæ€§æ¤œè¨¼
    # ============================================
    
    def _validate_mapped_type_consistency(
        self,
        generated_files: Dict[str, str]
    ) -> List[Dict[str, Any]]:
        """
        Mappedå‹ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã¨mapped_column/Columnå®šç¾©ã®æ•´åˆæ€§ã‚’æ¤œè¨¼
        
        æ¤œå‡ºå¯¾è±¡:
        - Mapped[int]ãªã®ã«Stringå‹ã‚«ãƒ©ãƒ 
        - Mapped[str]ãªã®ã«Integerå‹ã‚«ãƒ©ãƒ 
        - å‹ã®ä¸ä¸€è‡´
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã®ãƒªã‚¹ãƒˆ
        """
        import re
        
        issues = []
        
        # å‹ãƒãƒƒãƒ”ãƒ³ã‚°: Pythonå‹ -> è¨±å®¹ã•ã‚Œã‚‹SQLAlchemyå‹
        TYPE_MAPPING = {
            'int': ['Integer', 'BigInteger', 'SmallInteger', 'primary_key=True'],
            'str': ['String', 'Text', 'VARCHAR', 'CHAR'],
            'float': ['Float', 'Numeric', 'Decimal', 'REAL'],
            'bool': ['Boolean', 'BOOLEAN'],
            'datetime': ['DateTime', 'DATETIME', 'TIMESTAMP'],
            'date': ['Date', 'DATE'],
            'time': ['Time', 'TIME'],
            'bytes': ['LargeBinary', 'BLOB', 'BINARY'],
        }
        
        for filepath, content in generated_files.items():
            if not filepath.endswith('.py'):
                continue
            
            content = content.replace('\r\n', '\n')
            
            # Mapped[Type] = mapped_column(...) ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
            # ä¾‹: name: Mapped[str] = mapped_column(Integer, ...)
            pattern = r'(\w+)\s*:\s*Mapped\[([^\]]+)\]\s*=\s*mapped_column\s*\(([^)]+)\)'
            
            for match in re.finditer(pattern, content):
                attr_name = match.group(1)
                mapped_type = match.group(2).strip()
                column_def = match.group(3).strip()
                
                line_no = content[:match.start()].count('\n') + 1
                line_content = content.split('\n')[line_no - 1].strip()
                
                # Optional[Type] ã‚„ list[Type] ã‹ã‚‰åŸºæœ¬å‹ã‚’æŠ½å‡º
                base_type = mapped_type
                if 'Optional[' in mapped_type:
                    base_type = re.search(r'Optional\[(\w+)\]', mapped_type)
                    base_type = base_type.group(1) if base_type else mapped_type
                elif 'list[' in mapped_type.lower():
                    continue  # ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ãƒƒãƒ—ãªã®ã§ã‚¹ã‚­ãƒƒãƒ—
                
                # å‹ãƒã‚§ãƒƒã‚¯
                base_type_lower = base_type.lower()
                if base_type_lower in TYPE_MAPPING:
                    allowed_types = TYPE_MAPPING[base_type_lower]
                    
                    # ã‚«ãƒ©ãƒ å®šç¾©ã«è¨±å®¹ã•ã‚Œã‚‹å‹ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    type_found = False
                    for allowed_type in allowed_types:
                        if allowed_type in column_def:
                            type_found = True
                            break
                    
                    if not type_found:
                        # ä¸ä¸€è‡´ã®å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯
                        for py_type, sql_types in TYPE_MAPPING.items():
                            if py_type != base_type_lower:
                                for sql_type in sql_types:
                                    if sql_type in column_def and sql_type != 'primary_key=True':
                                        issues.append({
                                            'type': 'mapped_type_mismatch',
                                            'file': filepath,
                                            'line': line_no,
                                            'attribute': attr_name,
                                            'mapped_type': mapped_type,
                                            'column_type': sql_type,
                                            'code': line_content,
                                            'details': f"Mapped[{mapped_type}] but column uses {sql_type}",
                                            'suggested_mapped_type': py_type
                                        })
                                        logger.debug(f"[G-19] {filepath}:{line_no} - Type mismatch: Mapped[{mapped_type}] vs {sql_type}")
                                        break
        
        if issues:
            logger.debug(f"[G-19] Found {len(issues)} Mapped type consistency issues")
        else:
            logger.debug("[G-19] No Mapped type consistency issues found")
        
        return issues
    
    def _auto_fix_mapped_type_consistency(
        self,
        issues: List[Dict[str, Any]],
        generated_files: Dict[str, str]
    ) -> Dict[str, str]:
        """
        Mappedå‹ã®ä¸æ•´åˆã‚’LLMã§è‡ªå‹•ä¿®æ­£
        """
        if not issues or not self.llm_manager:
            return generated_files
        
        files_to_fix = {}
        for issue in issues:
            filepath = issue['file']
            if filepath not in files_to_fix:
                files_to_fix[filepath] = []
            files_to_fix[filepath].append(issue)
        
        for filepath, file_issues in files_to_fix.items():
            if filepath not in generated_files:
                continue
            
            original_code = generated_files[filepath]
            
            fixes_info = []
            for issue in file_issues:
                fixes_info.append(
                    f"- Line {issue['line']}: {issue['attribute']} has Mapped[{issue['mapped_type']}] but column uses {issue['column_type']}. "
                    f"Either change to Mapped[{issue['suggested_mapped_type']}] or change column type."
                )
            
            prompt = f"""Fix the Mapped type annotation inconsistencies in this Python file.

**File: {filepath}**
**Issues:**
{chr(10).join(fixes_info)}

**Current Code:**
```python
{original_code}
```

**Instructions:**
1. Fix the type mismatch by either:
   a. Changing the Mapped[Type] annotation to match the column type, OR
   b. Changing the column type to match the Mapped annotation
2. Choose the option that makes more semantic sense for the attribute name
3. Do NOT change any other code or logic

Return ONLY the fixed Python code wrapped in ```python``` blocks.
"""
            
            try:
                logger.debug(f"[G-19] Fixing: {filepath}")
                response = self.llm_manager.generate_response(
                    prompt=prompt,
                    system_prompt="You are a SQLAlchemy expert. Fix type annotation mismatches between Mapped[] and column definitions.",
                    max_tokens=8000
                )
                
                if hasattr(response, 'content'):
                    response_text = response.content
                else:
                    response_text = str(response)
                
                code_match = re.search(r'```python\s*(.*?)\s*```', response_text, re.DOTALL)
                if code_match:
                    fixed_code = code_match.group(1).strip()
                    if len(fixed_code) > len(original_code) * 0.5:
                        generated_files[filepath] = fixed_code
                        logger.debug(f"[G-19] âœ… Fixed {filepath}")
                    else:
                        logger.debug(f"[G-19] âš  Generated content too short for {filepath}")
                else:
                    logger.debug(f"[G-19] âš  No code block found for {filepath}")
                    
            except Exception as e:
                logger.debug(f"[G-19] âš  Error fixing {filepath}: {e}")
        
        return generated_files

    # ==========================================
    # G-20: Flaské™çš„ãƒ‘ã‚¹æ¤œè¨¼
    # ==========================================
    def _validate_flask_static_paths(
        self,
        generated_files: Dict[str, str]
    ) -> List[Dict[str, Any]]:
        """
        Flaské™çš„ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®æ•´åˆæ€§ã‚’æ¤œè¨¼
        
        Flaskãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã€HTMLå†…ã®é™çš„ãƒ•ã‚¡ã‚¤ãƒ«å‚ç…§ãŒ
        /static/ ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã‹æ¤œè¨¼ã™ã‚‹ã€‚
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«è¾æ›¸ {filepath: content}
        
        Returns:
            å•é¡Œã®ãƒªã‚¹ãƒˆ [{file, line, issue, current_path, suggested_path}]
        """
        issues = []
        
        # Flaskãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¤å®š
        is_flask_project = False
        for filepath, content in generated_files.items():
            if filepath.endswith('.py'):
                if 'from flask import' in content or 'import flask' in content.lower():
                    is_flask_project = True
                    break
                if 'Flask(' in content or 'flask.Flask' in content:
                    is_flask_project = True
                    break
        
        if not is_flask_project:
            logger.debug("[G-20] Not a Flask project, skipping static path validation")
            return issues
        
        logger.debug("[G-20] Flask project detected, validating static paths")
        
        # HTML/JSãƒ•ã‚¡ã‚¤ãƒ«ã®é™çš„ãƒ‘ã‚¹å‚ç…§ã‚’ãƒã‚§ãƒƒã‚¯
        for filepath, content in generated_files.items():
            if not filepath.endswith(('.html', '.htm')):
                continue
            
            lines = content.split('\n')
            for line_num, line in enumerate(lines, 1):
                # CSSå‚ç…§: href="..."
                css_matches = re.findall(
                    r'<link[^>]+href=["\']([^"\']+\.css)["\']',
                    line, re.IGNORECASE
                )
                for css_path in css_matches:
                    # æ—¢ã«/static/ã§å§‹ã¾ã‚‹å ´åˆã¯OK
                    if css_path.startswith('/static/') or css_path.startswith('http'):
                        continue
                    # {{ url_for(...) }} ã®å ´åˆã¯OK
                    if '{{' in css_path or '{%' in css_path:
                        continue
                    # ç›¸å¯¾ãƒ‘ã‚¹ â†’ å•é¡Œ
                    issues.append({
                        'file': filepath,
                        'line': line_num,
                        'issue': 'CSS relative path in Flask project',
                        'current_path': css_path,
                        'suggested_path': f'/static/{css_path.lstrip("./")}'
                    })
                
                # JSå‚ç…§: src="..."
                js_matches = re.findall(
                    r'<script[^>]+src=["\']([^"\']+\.js)["\']',
                    line, re.IGNORECASE
                )
                for js_path in js_matches:
                    if js_path.startswith('/static/') or js_path.startswith('http'):
                        continue
                    if '{{' in js_path or '{%' in js_path:
                        continue
                    issues.append({
                        'file': filepath,
                        'line': line_num,
                        'issue': 'JS relative path in Flask project',
                        'current_path': js_path,
                        'suggested_path': f'/static/{js_path.lstrip("./")}'
                    })
                
                # ç”»åƒå‚ç…§: src="..."
                img_matches = re.findall(
                    r'<img[^>]+src=["\']([^"\']+\.(png|jpg|jpeg|gif|svg|webp))["\']',
                    line, re.IGNORECASE
                )
                for img_match in img_matches:
                    img_path = img_match[0] if isinstance(img_match, tuple) else img_match
                    if img_path.startswith('/static/') or img_path.startswith('http') or img_path.startswith('data:'):
                        continue
                    if '{{' in img_path or '{%' in img_path:
                        continue
                    issues.append({
                        'file': filepath,
                        'line': line_num,
                        'issue': 'Image relative path in Flask project',
                        'current_path': img_path,
                        'suggested_path': f'/static/{img_path.lstrip("./")}'
                    })
        
        if issues:
            logger.debug(f"[G-20] Found {len(issues)} Flask static path issues")
        else:
            logger.debug("[G-20] No Flask static path issues found")
        
        return issues
    
    def _auto_fix_flask_static_paths(
        self,
        issues: List[Dict[str, Any]],
        generated_files: Dict[str, str]
    ) -> Dict[str, str]:
        """
        Flaské™çš„ãƒ‘ã‚¹ã®å•é¡Œã‚’è‡ªå‹•ä¿®æ­£
        
        ç›¸å¯¾ãƒ‘ã‚¹ã‚’/static/ä»˜ãã®çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›ã™ã‚‹ã€‚
        
        Args:
            issues: æ¤œå‡ºã•ã‚ŒãŸå•é¡Œãƒªã‚¹ãƒˆ
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«è¾æ›¸
        
        Returns:
            ä¿®æ­£å¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«è¾æ›¸
        """
        if not issues:
            return generated_files
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        issues_by_file = {}
        for issue in issues:
            filepath = issue['file']
            if filepath not in issues_by_file:
                issues_by_file[filepath] = []
            issues_by_file[filepath].append(issue)
        
        # å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿®æ­£
        for filepath, file_issues in issues_by_file.items():
            if filepath not in generated_files:
                continue
            
            content = generated_files[filepath]
            
            for issue in file_issues:
                current_path = issue['current_path']
                suggested_path = issue['suggested_path']
                
                # ãƒ‘ã‚¹ã‚’ç½®æ›ï¼ˆã‚¯ã‚©ãƒ¼ãƒˆä»˜ãã§æ­£ç¢ºã«ç½®æ›ï¼‰
                content = content.replace(f'"{current_path}"', f'"{suggested_path}"')
                content = content.replace(f"'{current_path}'", f"'{suggested_path}'")
                
                logger.debug(f"[G-20] Fixed {filepath}: {current_path} â†’ {suggested_path}")
            
            generated_files[filepath] = content
        
        return generated_files

    # ==========================================
    # G-21: HTMLå‚ç…§æ•´åˆæ€§æ¤œè¨¼
    # ==========================================
    def _validate_html_asset_references(
        self,
        generated_files: Dict[str, str]
    ) -> List[Dict[str, Any]]:
        """
        HTMLå†…ã®CSS/JSå‚ç…§ãŒå®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¨ä¸€è‡´ã™ã‚‹ã‹æ¤œè¨¼
        
        HTMLã§å‚ç…§ã•ã‚Œã¦ã„ã‚‹CSS/JSãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã€‚
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«è¾æ›¸ {filepath: content}
        
        Returns:
            å•é¡Œã®ãƒªã‚¹ãƒˆ [{html_file, line, ref_type, ref_path, issue}]
        """
        issues = []
        
        # ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ã‚»ãƒƒãƒˆï¼ˆæ­£è¦åŒ–ï¼‰
        generated_paths = set()
        for filepath in generated_files.keys():
            # è¤‡æ•°ã®å½¢å¼ã§ç™»éŒ²ï¼ˆãƒãƒƒãƒã—ã‚„ã™ãã™ã‚‹ãŸã‚ï¼‰
            generated_paths.add(filepath)
            generated_paths.add(filepath.lstrip('./'))
            generated_paths.add('/' + filepath.lstrip('./'))
            # static/ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ä»˜ãã‚‚è¿½åŠ 
            if not filepath.startswith('static/'):
                generated_paths.add('static/' + filepath.lstrip('./'))
                generated_paths.add('/static/' + filepath.lstrip('./'))
        
        # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®å‚ç…§ã‚’ãƒã‚§ãƒƒã‚¯
        for filepath, content in generated_files.items():
            if not filepath.endswith(('.html', '.htm')):
                continue
            
            lines = content.split('\n')
            for line_num, line in enumerate(lines, 1):
                # CSSå‚ç…§
                css_matches = re.findall(
                    r'<link[^>]+href=["\']([^"\']+)["\'][^>]*>',
                    line, re.IGNORECASE
                )
                for href in css_matches:
                    # stylesheetã®ã¿ãƒã‚§ãƒƒã‚¯
                    if 'rel=' in line and 'stylesheet' not in line.lower():
                        continue
                    # å¤–éƒ¨URLã¯ã‚¹ã‚­ãƒƒãƒ—
                    if href.startswith('http') or href.startswith('//'):
                        continue
                    # Jinja2ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¯ã‚¹ã‚­ãƒƒãƒ—
                    if '{{' in href or '{%' in href:
                        continue
                    # CDNã¯ã‚¹ã‚­ãƒƒãƒ—
                    if 'cdn' in href.lower() or 'googleapis' in href.lower():
                        continue
                    
                    # ãƒ‘ã‚¹ã‚’æ­£è¦åŒ–ã—ã¦ãƒã‚§ãƒƒã‚¯
                    normalized_href = href.lstrip('./')
                    if not self._is_asset_generated(normalized_href, generated_paths):
                        issues.append({
                            'html_file': filepath,
                            'line': line_num,
                            'ref_type': 'CSS',
                            'ref_path': href,
                            'issue': f'Referenced CSS file not generated: {href}'
                        })
                
                # JSå‚ç…§
                js_matches = re.findall(
                    r'<script[^>]+src=["\']([^"\']+)["\'][^>]*>',
                    line, re.IGNORECASE
                )
                for src in js_matches:
                    # å¤–éƒ¨URLã¯ã‚¹ã‚­ãƒƒãƒ—
                    if src.startswith('http') or src.startswith('//'):
                        continue
                    # Jinja2ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¯ã‚¹ã‚­ãƒƒãƒ—
                    if '{{' in src or '{%' in src:
                        continue
                    # CDNã¯ã‚¹ã‚­ãƒƒãƒ—
                    if 'cdn' in src.lower() or 'googleapis' in src.lower() or 'unpkg' in src.lower():
                        continue
                    
                    normalized_src = src.lstrip('./')
                    if not self._is_asset_generated(normalized_src, generated_paths):
                        issues.append({
                            'html_file': filepath,
                            'line': line_num,
                            'ref_type': 'JS',
                            'ref_path': src,
                            'issue': f'Referenced JS file not generated: {src}'
                        })
        
        if issues:
            logger.debug(f"[G-21] Found {len(issues)} missing asset references")
        else:
            logger.debug("[G-21] All asset references are valid")
        
        return issues
    
    def _is_asset_generated(self, ref_path: str, generated_paths: set) -> bool:
        """å‚ç…§ãƒ‘ã‚¹ãŒç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ã«å«ã¾ã‚Œã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        # ç›´æ¥ãƒãƒƒãƒ
        if ref_path in generated_paths:
            return True
        
        # /static/ã‚’é™¤å»ã—ã¦ãƒã‚§ãƒƒã‚¯
        if ref_path.startswith('/static/'):
            without_static = ref_path[8:]  # len('/static/') = 8
            if without_static in generated_paths:
                return True
        
        # static/ã‚’é™¤å»ã—ã¦ãƒã‚§ãƒƒã‚¯
        if ref_path.startswith('static/'):
            without_static = ref_path[7:]  # len('static/') = 7
            if without_static in generated_paths:
                return True
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿ã§ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€å¾Œã®æ‰‹æ®µï¼‰
        ref_filename = ref_path.split('/')[-1]
        for path in generated_paths:
            if path.endswith('/' + ref_filename) or path == ref_filename:
                return True
        
        return False
    
    def _auto_fix_html_asset_references(
        self,
        issues: List[Dict[str, Any]],
        generated_files: Dict[str, str]
    ) -> Dict[str, str]:
        """
        HTMLå‚ç…§æ•´åˆæ€§ã®å•é¡Œã‚’è‡ªå‹•ä¿®æ­£
        
        æ¬ è½ã—ã¦ã„ã‚‹CSS/JSãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆã‚’LLMã«ä¾é ¼ã™ã‚‹ã€‚
        
        Args:
            issues: æ¤œå‡ºã•ã‚ŒãŸå•é¡Œãƒªã‚¹ãƒˆ
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«è¾æ›¸
        
        Returns:
            ä¿®æ­£å¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«è¾æ›¸ï¼ˆè¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ«å«ã‚€ï¼‰
        """
        if not issues:
            return generated_files
        
        # æ¬ è½ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        missing_files = {}
        for issue in issues:
            ref_path = issue['ref_path']
            ref_type = issue['ref_type']
            
            # ãƒ‘ã‚¹ã‚’æ­£è¦åŒ–
            normalized_path = ref_path.lstrip('./')
            if normalized_path.startswith('/static/'):
                normalized_path = normalized_path[1:]  # å…ˆé ­ã®/ã‚’é™¤å»
            elif normalized_path.startswith('static/'):
                pass  # ãã®ã¾ã¾
            else:
                normalized_path = 'static/' + normalized_path
            
            if normalized_path not in missing_files:
                missing_files[normalized_path] = ref_type
        
        # å„æ¬ è½ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
        for filepath, file_type in missing_files.items():
            if filepath in generated_files:
                continue
            
            logger.debug(f"[G-21] Generating missing {file_type} file: {filepath}")
            
            try:
                # é–¢é€£ã™ã‚‹HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’å–å¾—ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”¨ï¼‰
                html_context = ""
                for html_file, content in generated_files.items():
                    if html_file.endswith('.html'):
                        html_context = content[:2000]  # æœ€åˆã®2000æ–‡å­—
                        break
                
                prompt = f"""Generate the missing {file_type} file for a web application.

File to generate: {filepath}

HTML context (for reference):
```html
{html_context}
```

Requirements:
1. Generate a complete, functional {file_type} file
2. The file should work with the HTML shown above
3. Use modern best practices
4. Include appropriate comments

Return ONLY the {file_type} code, no explanations.
"""
                
                response = self.llm_manager.generate_response(
                    prompt=prompt,
                    system_prompt=f"You are a {file_type} expert. Generate clean, professional code.",
                    max_tokens=4000
                )
                
                if hasattr(response, 'content'):
                    response_text = response.content
                else:
                    response_text = str(response)
                
                # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡º
                code_pattern = r'```(?:css|javascript|js)?\s*(.*?)\s*```'
                code_match = re.search(code_pattern, response_text, re.DOTALL)
                if code_match:
                    generated_code = code_match.group(1).strip()
                else:
                    # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ãŒãªã„å ´åˆã¯å…¨ä½“ã‚’ä½¿ç”¨
                    generated_code = response_text.strip()
                
                if len(generated_code) > 50:  # æœ€å°é™ã®å†…å®¹ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    generated_files[filepath] = generated_code
                    logger.debug(f"[G-21] âœ… Generated missing file: {filepath}")
                else:
                    logger.debug(f"[G-21] âš  Generated content too short for {filepath}")
                
            except Exception as e:
                logger.debug(f"[G-21] âš  Error generating {filepath}: {e}")
        
        return generated_files

    # ============================================
    # G-22: JavaScriptã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ—ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œè¨¼
    # ============================================
    
    def _validate_js_animation_loop(
        self,
        generated_files: Dict[str, str]
    ) -> List[Dict[str, Any]]:
        """
        JavaScriptã®ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œè¨¼
        gameLoopç­‰ã®ãƒ¡ã‚½ãƒƒãƒ‰ãŒrequestAnimationFrameçµŒç”±ã§æ­£ã—ãå‘¼ã°ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        
        æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³:
          1. animation loopé–¢æ•°ãŒå¼•æ•°ãªã—ã§ç›´æ¥å‘¼ã³å‡ºã•ã‚Œã¦ã„ã‚‹
          2. requestAnimationFrameã®callbackå†…ã§Date.now()ã¨timestampå¼•æ•°ãŒæ··åœ¨ã—ã¦ã„ã‚‹
             ï¼ˆæ™‚é–“åŸºæº–ä¸ä¸€è‡´ãƒã‚°: DOMHighResTimeStamp vs Unix epoch msï¼‰
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã®ãƒªã‚¹ãƒˆ
        """
        import re
        
        issues = []
        
        # ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ—é–¢é€£ã®ãƒ¡ã‚½ãƒƒãƒ‰åãƒ‘ã‚¿ãƒ¼ãƒ³
        LOOP_METHOD_NAMES = ['gameLoop', 'animationLoop', 'renderLoop', 'mainLoop', 'update', 'animate', 'loop', 'frame', 'tick']
        
        for filepath, content in generated_files.items():
            # .htmlãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã€<script>ã‚¿ã‚°å†…ã®JSã‚’æŠ½å‡º
            if filepath.endswith('.html'):
                script_blocks = re.findall(r'<script[^>]*>(.*?)</script>', content, re.DOTALL)
                if not script_blocks:
                    continue
                js_content = '\n'.join(script_blocks)
            elif filepath.endswith(('.js', '.jsx', '.ts', '.tsx')):
                js_content = content
            else:
                continue
            
            # CRLFå¯¾å¿œ
            js_content = js_content.replace('\r\n', '\n')
            lines = js_content.split('\n')
            
            for method_name in LOOP_METHOD_NAMES:
                # ãƒ¡ã‚½ãƒƒãƒ‰å®šç¾©ã‚’æ¤œå‡º: gameLoop(timestamp) or function gameLoop(currentTime) ç­‰
                method_def_pattern = rf'(?:function\s+)?{method_name}\s*\(\s*(\w+)\s*\)\s*{{'
                method_def_match = re.search(method_def_pattern, js_content)
                
                if not method_def_match:
                    continue
                
                param_name = method_def_match.group(1)
                
                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒæ™‚é–“é–¢é€£ã®åå‰ã‹ãƒã‚§ãƒƒã‚¯
                time_param_names = ['time', 'timestamp', 'currentTime', 'now', 't', 'dt', 'deltaTime']
                is_time_param = any(tp.lower() in param_name.lower() for tp in time_param_names)
                
                if not is_time_param:
                    continue
                
                # requestAnimationFrameã§å‘¼ã°ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
                raf_call_pattern = rf'requestAnimationFrame\s*\([^)]*{method_name}'
                is_raf_callback = re.search(raf_call_pattern, js_content) is not None
                
                if not is_raf_callback:
                    continue
                
                # ===== æ¤œå‡º1: å¼•æ•°ãªã—ã®ç›´æ¥å‘¼ã³å‡ºã— =====
                bad_call_pattern = rf'this\.{method_name}\s*\(\s*\)'
                good_call_pattern = rf'requestAnimationFrame\s*\([^)]*{method_name}'
                
                for line_no, line in enumerate(lines, 1):
                    stripped = line.strip()
                    if stripped.startswith('//') or stripped.startswith('/*'):
                        continue
                    if re.search(good_call_pattern, line):
                        continue
                    bad_match = re.search(bad_call_pattern, line)
                    if bad_match:
                        if 'requestAnimationFrame' not in line:
                            issues.append({
                                'type': 'animation_loop_bad_call',
                                'file': filepath,
                                'line': line_no,
                                'method_name': method_name,
                                'param_name': param_name,
                                'code': stripped,
                                'details': f"'{method_name}()' called without argument. Should use: requestAnimationFrame((t) => this.{method_name}(t))"
                            })
                            logger.debug(f"[G-22] {filepath}:{line_no} - Animation loop '{method_name}' called without timestamp argument")
                
                # ===== æ¤œå‡º2: æ™‚é–“åŸºæº–ä¸ä¸€è‡´ï¼ˆDate.now() vs rAF timestampï¼‰ =====
                # requestAnimationFrameã®callbackå¼•æ•°(DOMHighResTimeStamp)ã‚’å—ã‘å–ã‚‹é–¢æ•°å†…ã§
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã¨æ¯”è¼ƒã•ã‚Œã‚‹å¤‰æ•°ãŒDate.now()ã§ä»£å…¥ã•ã‚Œã¦ã„ã‚‹å ´åˆã‚’æ¤œå‡º
                
                # é–¢æ•°æœ¬ä½“ã‚’æŠ½å‡ºï¼ˆãƒ–ãƒ¬ãƒ¼ã‚¹å¯¾å¿œï¼‰
                func_start = method_def_match.start()
                brace_count = 0
                func_body_start = js_content.index('{', func_start)
                func_body_end = func_body_start
                for ci in range(func_body_start, len(js_content)):
                    if js_content[ci] == '{':
                        brace_count += 1
                    elif js_content[ci] == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            func_body_end = ci + 1
                            break
                
                func_body = js_content[func_body_start:func_body_end]
                
                # Step A: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å¼•æ•°ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚’ç‰¹å®š
                # ä¾‹: const now = timestamp; â†’ nowã¯timestampã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹
                aliases = {param_name}
                alias_pattern = rf'(?:const|let|var)\s+(\w+)\s*=\s*(?:{"|".join(re.escape(a) for a in aliases)})\b'
                for alias_match in re.finditer(alias_pattern, func_body):
                    aliases.add(alias_match.group(1))
                
                # Step B: ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã¨å·®åˆ†æ¯”è¼ƒã•ã‚Œã‚‹å¤‰æ•°ã‚’ç‰¹å®š
                # ä¾‹: now - lastDropTime â†’ lastDropTimeãŒæ¯”è¼ƒå¯¾è±¡
                comparison_vars = set()
                for alias in aliases:
                    for m in re.finditer(rf'{re.escape(alias)}\s*-\s*(\w+)', func_body):
                        var = m.group(1)
                        if var not in aliases and not var.isdigit():
                            comparison_vars.add(var)
                    for m in re.finditer(rf'(\w+)\s*-\s*{re.escape(alias)}', func_body):
                        var = m.group(1)
                        if var not in aliases and not var.isdigit():
                            comparison_vars.add(var)
                
                # Step C: ã“ã‚Œã‚‰ã®å¤‰æ•°ãŒãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã§Date.now()ã§ä»£å…¥ã•ã‚Œã¦ã„ã‚‹ã‹æ¤œå‡º
                for var in comparison_vars:
                    date_now_assigns = list(re.finditer(
                        rf'{re.escape(var)}\s*=\s*Date\.now\(\)',
                        js_content
                    ))
                    if date_now_assigns:
                        for dna in date_now_assigns:
                            dna_line = js_content[:dna.start()].count('\n') + 1
                            dna_code = js_content.split('\n')[dna_line - 1].strip()
                            # ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã¯ã‚¹ã‚­ãƒƒãƒ—
                            if dna_code.startswith('//') or dna_code.startswith('/*'):
                                continue
                            issues.append({
                                'type': 'timestamp_origin_mismatch',
                                'file': filepath,
                                'line': dna_line,
                                'method_name': method_name,
                                'param_name': param_name,
                                'var_name': var,
                                'code': dna_code,
                                'details': (
                                    f"'{var}' is assigned Date.now() (Unix epoch ms) but compared with "
                                    f"rAF timestamp (DOMHighResTimeStamp) in '{method_name}({param_name})'. "
                                    f"Replace Date.now() with performance.now(), or use timestamp "
                                    f"parameter consistently."
                                )
                            })
                            logger.debug(
                                f"[G-22] {filepath}:{dna_line} - Time origin mismatch: "
                                f"'{var} = Date.now()' but compared with rAF timestamp in {method_name}()"
                            )
        
        if issues:
            logger.debug(f"[G-22] Found {len(issues)} JavaScript animation loop issues")
        else:
            logger.debug("[G-22] No JavaScript animation loop issues found")
        
        return issues
    
    def _auto_fix_js_animation_loop(
        self,
        issues: List[Dict[str, Any]],
        generated_files: Dict[str, str]
    ) -> Dict[str, str]:
        """
        JavaScriptã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ—ã®å•é¡Œã‚’è‡ªå‹•ä¿®æ­£
        
        ä¿®æ­£ãƒ‘ã‚¿ãƒ¼ãƒ³:
          1. animation_loop_bad_call: this.gameLoop() â†’ requestAnimationFrame((t) => this.gameLoop(t))
          2. timestamp_origin_mismatch: Date.now() â†’ performance.now() (rAF callbackå†…)
        """
        if not issues:
            return generated_files
        
        import re
        
        for issue in issues:
            filepath = issue['file']
            if filepath not in generated_files:
                continue
            
            content = generated_files[filepath]
            method_name = issue['method_name']
            param_name = issue['param_name']
            
            if issue['type'] == 'animation_loop_bad_call':
                # å•é¡Œãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ­£ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ç½®æ›
                # this.gameLoop(); â†’ requestAnimationFrame((t) => this.gameLoop(t));
                bad_pattern = rf'(\s*)this\.{method_name}\s*\(\s*\)\s*;'
                good_replacement = rf'\1requestAnimationFrame((t) => this.{method_name}(t));'
                
                new_content = re.sub(bad_pattern, good_replacement, content)
                
                if new_content != content:
                    generated_files[filepath] = new_content
                    logger.debug(f"[G-22] âœ… Fixed animation loop call in {filepath}")
            
            elif issue['type'] == 'timestamp_origin_mismatch':
                # rAFã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã¨æ¯”è¼ƒã•ã‚Œã‚‹å¤‰æ•°ã¸ã®Date.now()ä»£å…¥ã‚’performance.now()ã«ç½®æ›
                var_name = issue.get('var_name', '')
                if not var_name:
                    continue
                
                # è©²å½“å¤‰æ•°ã¸ã®Date.now()ä»£å…¥ã‚’ç½®æ›
                pattern = rf'({re.escape(var_name)}\s*=\s*)Date\.now\(\)'
                replacement = rf'\1performance.now()'
                new_content = re.sub(pattern, replacement, content)
                
                if new_content != content:
                    generated_files[filepath] = new_content
                    content = new_content  # æ¬¡ã®issueå‡¦ç†ç”¨ã«æ›´æ–°
                    count = content.count('performance.now()') - content.count('performance.now()')
                    logger.debug(f"[G-22] âœ… Fixed '{var_name} = Date.now()' â†’ '{var_name} = performance.now()' in {filepath}")
        
        return generated_files
        
        return generated_files

    # ============================================
    # G-23: APIå¥‘ç´„æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
    # ============================================
    
    def _validate_api_contract_consistency(
        self,
        generated_files: Dict[str, str]
    ) -> List[Dict[str, Any]]:
        """
        ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰â†”ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰é–“ã®APIå¥‘ç´„æ•´åˆæ€§ã‚’æ¤œè¨¼
        
        ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã§å‚ç…§ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®ã‚¹ã‚­ãƒ¼ãƒã«å­˜åœ¨ã™ã‚‹ã‹ã€
        ã¾ãŸé€†ã«ã‚¹ã‚­ãƒ¼ãƒã§å®šç¾©ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã§æ­£ã—ãå‚ç…§ã•ã‚Œã¦ã„ã‚‹ã‹ã‚’ãƒã‚§ãƒƒã‚¯
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã®ãƒªã‚¹ãƒˆ
        """
        import re
        
        issues = []
        
        # 1. ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‹ã‚‰APIå‚ç…§ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æŠ½å‡º
        api_references = self._extract_frontend_api_field_references(generated_files)
        frontend_fields = api_references.get('_global', set())
        
        if not frontend_fields:
            logger.debug("[G-23] No frontend API references found")
            return issues
        
        # 2. ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®ã‚¹ã‚­ãƒ¼ãƒãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æŠ½å‡º
        schema_fields = {}  # {schema_name: {fields}}
        
        for filepath, content in generated_files.items():
            if not filepath.endswith('.py'):
                continue
            if 'schema' not in filepath.lower() and 'Schema' not in content:
                continue
            
            content = content.replace('\r\n', '\n')
            
            # Marshmallowã‚¹ã‚­ãƒ¼ãƒã‚¯ãƒ©ã‚¹ã‚’æ¤œå‡º
            class_pattern = r'class\s+(\w+Schema)\s*\([^)]*\)\s*:'
            for class_match in re.finditer(class_pattern, content):
                schema_name = class_match.group(1)
                class_start = class_match.end()
                
                # æ¬¡ã®ã‚¯ãƒ©ã‚¹ã¾ãŸã¯ãƒ•ã‚¡ã‚¤ãƒ«æœ«å°¾ã¾ã§ã‚’å–å¾—
                next_class = re.search(r'\nclass\s+\w+', content[class_start:])
                if next_class:
                    class_body = content[class_start:class_start + next_class.start()]
                else:
                    class_body = content[class_start:]
                
                # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å®šç¾©ã‚’æŠ½å‡º
                field_pattern = r'^\s+(\w+)\s*=\s*(?:fields\.|ma\.)'
                fields = set()
                for line in class_body.split('\n'):
                    field_match = re.match(field_pattern, line)
                    if field_match:
                        fields.add(field_match.group(1))
                
                # SQLAlchemyAutoSchemaã®å ´åˆã€Metaã‚¯ãƒ©ã‚¹ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
                if 'SQLAlchemyAutoSchema' in content or 'SQLAlchemySchema' in content:
                    meta_pattern = r'class\s+Meta\s*:.*?model\s*=\s*(\w+)'
                    meta_match = re.search(meta_pattern, class_body, re.DOTALL)
                    if meta_match:
                        model_name = meta_match.group(1)
                        # ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚‚è¿½åŠ ï¼ˆç°¡æ˜“çš„ã«ï¼‰
                        fields.add('id')  # é€šå¸¸idã¯å­˜åœ¨ã™ã‚‹
                
                if fields:
                    schema_fields[schema_name] = fields
        
        # 3. æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        all_backend_fields = set()
        for fields in schema_fields.values():
            all_backend_fields.update(fields)
        
        # ä¸€èˆ¬çš„ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åï¼ˆèª¤æ¤œå‡ºã‚’é¿ã‘ã‚‹ãŸã‚é™¤å¤–ï¼‰
        common_fields = {'id', 'name', 'value', 'data', 'type', 'status', 'error', 'message', 
                        'success', 'result', 'items', 'count', 'total', 'page', 'limit',
                        'x', 'y', 'width', 'height', 'key', 'index', 'length', 'size'}
        
        # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã§å‚ç…§ã•ã‚Œã¦ã„ã‚‹ãŒãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã«å­˜åœ¨ã—ãªã„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
        for field in frontend_fields:
            if field in common_fields:
                continue
            if field not in all_backend_fields and len(field) > 2:
                # ã‚¹ã‚­ãƒ¼ãƒã§å®šç¾©ã•ã‚Œã¦ã„ãªã„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¸ã®å‚ç…§
                # ãŸã ã—ã€ã“ã‚Œã¯å¿…ãšã—ã‚‚ã‚¨ãƒ©ãƒ¼ã§ã¯ãªã„ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«å¤‰æ•°ã®å¯èƒ½æ€§ï¼‰
                # é‡è¦ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼ˆ_id, _at ã§çµ‚ã‚ã‚‹ã‚‚ã®ï¼‰ã®ã¿ãƒã‚§ãƒƒã‚¯
                if field.endswith('_id') or field.endswith('_at') or field.endswith('_name'):
                    issues.append({
                        'type': 'frontend_references_missing_field',
                        'field': field,
                        'severity': 'warning',
                        'details': f"Frontend references '{field}' but no matching schema field found"
                    })
        
        if issues:
            logger.debug(f"[G-23] Found {len(issues)} API contract consistency issues")
        else:
            logger.debug("[G-23] API contract consistency check passed")
        
        return issues
    
    def _auto_fix_api_contract_consistency(
        self,
        issues: List[Dict[str, Any]],
        generated_files: Dict[str, str]
    ) -> Dict[str, str]:
        """
        APIå¥‘ç´„æ•´åˆæ€§ã®å•é¡Œã‚’å ±å‘Šï¼ˆè‡ªå‹•ä¿®æ­£ã¯å±é™ºãªãŸã‚è¡Œã‚ãªã„ï¼‰
        
        ã“ã®ãƒã‚§ãƒƒã‚¯ã¯ä¸»ã«è­¦å‘Šã¨ã—ã¦æ©Ÿèƒ½ã—ã€é–‹ç™ºè€…ã«å•é¡Œã‚’é€šçŸ¥ã™ã‚‹
        """
        if not issues:
            return generated_files
        
        # è‡ªå‹•ä¿®æ­£ã¯è¡Œã‚ãšã€è­¦å‘Šã®ã¿
        for issue in issues:
            logger.debug(f"[G-23] âš  {issue['details']}")
        
        return generated_files

    # ============================================
    # G-24: back_populatesç›¸äº’å‚ç…§æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
    # ============================================
    
    def _validate_back_populates_consistency(
        self,
        generated_files: Dict[str, str]
    ) -> List[Dict[str, Any]]:
        """
        SQLAlchemy relationship ã® back_populates ç›¸äº’å‚ç…§æ•´åˆæ€§ã‚’æ¤œè¨¼
        
        back_populates ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€åŒæ–¹ã®ãƒ¢ãƒ‡ãƒ«ã«å¯¾å¿œã™ã‚‹ relationship ãŒå¿…è¦
        ç‰‡æ–¹ã«ã—ã‹å®šç¾©ãŒãªã„å ´åˆã€SQLAlchemy ãŒ InvalidRequestError ã‚’ç™ºç”Ÿã•ã›ã‚‹
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã®ãƒªã‚¹ãƒˆ
        """
        import re
        
        issues = []
        
        # 1. å…¨ãƒ¢ãƒ‡ãƒ«ã® relationship ã¨ back_populates ã‚’åé›†
        relationships = {}  # {(model_name, attr_name): (target_model, back_populates_value, filepath, line)}
        
        for filepath, content in generated_files.items():
            if not filepath.endswith('.py'):
                continue
            
            content = content.replace('\r\n', '\n')
            lines = content.split('\n')
            
            # ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹åã‚’è¿½è·¡
            current_class = None
            
            for line_num, line in enumerate(lines, 1):
                # ã‚¯ãƒ©ã‚¹å®šç¾©ã‚’æ¤œå‡º
                class_match = re.match(r'^class\s+(\w+)\s*\(', line)
                if class_match:
                    current_class = class_match.group(1)
                    continue
                
                if not current_class:
                    continue
                
                # relationshipå®šç¾©ã‚’æ¤œå‡º
                # ãƒ‘ã‚¿ãƒ¼ãƒ³1: attr: Mapped[...] = relationship(..., back_populates="...")
                # ãƒ‘ã‚¿ãƒ¼ãƒ³2: attr = relationship(..., back_populates="...")
                rel_patterns = [
                    r'(\w+)\s*:\s*Mapped\[.*?\]\s*=\s*relationship\s*\(\s*["\'](\w+)["\'].*?back_populates\s*=\s*["\'](\w+)["\']',
                    r'(\w+)\s*=\s*relationship\s*\(\s*["\'](\w+)["\'].*?back_populates\s*=\s*["\'](\w+)["\']',
                    r'(\w+)\s*:\s*Mapped\[.*?\]\s*=\s*relationship\s*\(\s*(\w+)\s*,.*?back_populates\s*=\s*["\'](\w+)["\']',
                ]
                
                for pattern in rel_patterns:
                    match = re.search(pattern, line)
                    if match:
                        attr_name = match.group(1)
                        target_model = match.group(2)
                        back_populates = match.group(3)
                        
                        relationships[(current_class, attr_name)] = (
                            target_model,
                            back_populates,
                            filepath,
                            line_num
                        )
                        break
        
        # 2. ç›¸äº’å‚ç…§ã‚’æ¤œè¨¼
        for (model_name, attr_name), (target_model, back_populates, filepath, line_num) in relationships.items():
            # target_model ã« back_populates ã§æŒ‡å®šã•ã‚ŒãŸå±æ€§ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
            expected_key = (target_model, back_populates)
            
            if expected_key not in relationships:
                # å¯¾å¿œã™ã‚‹ relationship ãŒå­˜åœ¨ã—ãªã„
                issues.append({
                    'type': 'missing_back_populates_target',
                    'model': model_name,
                    'attribute': attr_name,
                    'target_model': target_model,
                    'expected_attribute': back_populates,
                    'file': filepath,
                    'line': line_num,
                    'severity': 'error',
                    'details': f"'{model_name}.{attr_name}' has back_populates='{back_populates}', but '{target_model}' has no '{back_populates}' attribute"
                })
            else:
                # å­˜åœ¨ã™ã‚‹å ´åˆã€ç›¸äº’å‚ç…§ãŒæ­£ã—ã„ã‹ç¢ºèª
                target_info = relationships[expected_key]
                target_back_populates = target_info[1]
                
                if target_back_populates != attr_name:
                    issues.append({
                        'type': 'back_populates_mismatch',
                        'model': model_name,
                        'attribute': attr_name,
                        'target_model': target_model,
                        'expected_attribute': back_populates,
                        'actual_back_populates': target_back_populates,
                        'file': filepath,
                        'line': line_num,
                        'severity': 'error',
                        'details': f"'{model_name}.{attr_name}' expects '{target_model}.{back_populates}' to back_populates='{attr_name}', but it back_populates='{target_back_populates}'"
                    })
        
        if issues:
            logger.debug(f"[G-24] Found {len(issues)} back_populates consistency issues")
        else:
            logger.debug("[G-24] All back_populates references are consistent")
        
        return issues
    
    def _auto_fix_back_populates_consistency(
        self,
        issues: List[Dict[str, Any]],
        generated_files: Dict[str, str]
    ) -> Dict[str, str]:
        """
        back_populatesæ•´åˆæ€§ã®å•é¡Œã‚’è‡ªå‹•ä¿®æ­£
        
        æ¬ è½ã—ã¦ã„ã‚‹relationshipå®šç¾©ã‚’è¿½åŠ ã™ã‚‹
        """
        import re
        
        if not issues or not self.llm_manager:
            return generated_files
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        files_to_fix = {}
        for issue in issues:
            if issue['type'] != 'missing_back_populates_target':
                continue
            
            target_model = issue['target_model']
            
            # target_modelãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
            for filepath, content in generated_files.items():
                if not filepath.endswith('.py'):
                    continue
                if f"class {target_model}" in content:
                    if filepath not in files_to_fix:
                        files_to_fix[filepath] = []
                    files_to_fix[filepath].append(issue)
                    break
        
        for filepath, file_issues in files_to_fix.items():
            if filepath not in generated_files:
                continue
            
            original_code = generated_files[filepath]
            
            fixes_info = []
            for issue in file_issues:
                fixes_info.append(
                    f"- Add '{issue['expected_attribute']}' relationship to '{issue['target_model']}' "
                    f"with back_populates='{issue['attribute']}' targeting '{issue['model']}'"
                )
            
            prompt = f"""Fix missing back_populates relationship definitions in this Python file.

**File: {filepath}**
**Issues:**
{chr(10).join(fixes_info)}

**Current Code:**
```python
{original_code}
```

**Instructions:**
1. Add the missing relationship() definitions to the target models
2. Ensure back_populates values match on both sides
3. Use proper Mapped[] type annotations for SQLAlchemy 2.0
4. Add necessary imports (relationship, Mapped, List) if not present
5. Use TYPE_CHECKING for forward references if needed

Return ONLY the fixed Python code wrapped in ```python``` blocks.
"""
            
            try:
                logger.debug(f"[G-24] Fixing: {filepath}")
                response = self.llm_manager.generate_response(
                    prompt=prompt,
                    system_prompt="You are a SQLAlchemy ORM expert. Add missing relationship definitions with correct back_populates.",
                    max_tokens=8000
                )
                
                if hasattr(response, 'content'):
                    response_text = response.content
                elif isinstance(response, dict) and 'content' in response:
                    response_text = response['content']
                else:
                    response_text = str(response)
                
                # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡º
                code_match = re.search(r'```python\s*(.*?)\s*```', response_text, re.DOTALL)
                if code_match:
                    fixed_code = code_match.group(1).strip()
                    
                    # åŸºæœ¬çš„ãªæ¤œè¨¼
                    if len(fixed_code) > len(original_code) * 0.5:
                        generated_files[filepath] = fixed_code
                        logger.debug(f"[G-24] âœ… Fixed {filepath}")
                    else:
                        logger.debug(f"[G-24] âš  Fixed code too short, skipping")
                else:
                    logger.debug(f"[G-24] âš  No code block in response")
                    
            except Exception as e:
                logger.debug(f"[G-24] âš  Error fixing {filepath}: {e}")
        
        return generated_files

    # ============================================
    # G-25: ForeignKeyå‹æ³¨é‡ˆæ¬ å¦‚ãƒã‚§ãƒƒã‚¯
    # ============================================
    
    def _validate_foreignkey_type_annotation(
        self,
        generated_files: Dict[str, str]
    ) -> List[Dict[str, Any]]:
        """
        SQLAlchemy 2.0 ã§ ForeignKey ã«å‹æ³¨é‡ˆãŒãªã„å ´åˆã‚’æ¤œå‡º
        
        SQLAlchemy 2.0 ã§ã¯ Mapped[] å‹æ³¨é‡ˆãŒæ¨å¥¨ã•ã‚Œã‚‹
        å‹æ³¨é‡ˆãŒãªã„ã¨å‹æ¨è«–ãŒåŠ¹ã‹ãšã€IDEè£œå®Œã‚„mypyãƒã‚§ãƒƒã‚¯ãŒæ©Ÿèƒ½ã—ãªã„
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã®ãƒªã‚¹ãƒˆ
        """
        import re
        
        issues = []
        
        for filepath, content in generated_files.items():
            if not filepath.endswith('.py'):
                continue
            
            content = content.replace('\r\n', '\n')
            lines = content.split('\n')
            
            current_class = None
            
            for line_num, line in enumerate(lines, 1):
                # ã‚¯ãƒ©ã‚¹å®šç¾©ã‚’æ¤œå‡º
                class_match = re.match(r'^class\s+(\w+)\s*\(', line)
                if class_match:
                    current_class = class_match.group(1)
                    continue
                
                if not current_class:
                    continue
                
                # ForeignKeyã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãŒå‹æ³¨é‡ˆãŒãªã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
                # âŒ user_id = Column(ForeignKey("user.id"))
                # âŒ user_id = mapped_column(ForeignKey("user.id"))
                # âœ… user_id: Mapped[int] = mapped_column(ForeignKey("user.id"))
                
                # å‹æ³¨é‡ˆãªã—ã®ForeignKey
                bad_patterns = [
                    r'^\s+(\w+)\s*=\s*(?:Column|mapped_column)\s*\(\s*(?:Integer,\s*)?ForeignKey',
                    r'^\s+(\w+)\s*=\s*(?:Column|mapped_column)\s*\(\s*ForeignKey',
                ]
                
                for pattern in bad_patterns:
                    match = re.search(pattern, line)
                    if match:
                        attr_name = match.group(1)
                        
                        # åŒã˜è¡Œã«å‹æ³¨é‡ˆãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                        if not re.search(rf'{attr_name}\s*:\s*Mapped\[', line):
                            issues.append({
                                'type': 'missing_foreignkey_type_annotation',
                                'class': current_class,
                                'attribute': attr_name,
                                'file': filepath,
                                'line': line_num,
                                'severity': 'warning',
                                'details': f"'{current_class}.{attr_name}' uses ForeignKey without Mapped[] type annotation"
                            })
                        break
        
        if issues:
            logger.debug(f"[G-25] Found {len(issues)} ForeignKey type annotation issues")
        else:
            logger.debug("[G-25] All ForeignKey columns have proper type annotations")
        
        return issues
    
    def _auto_fix_foreignkey_type_annotation(
        self,
        issues: List[Dict[str, Any]],
        generated_files: Dict[str, str]
    ) -> Dict[str, str]:
        """
        ForeignKeyå‹æ³¨é‡ˆæ¬ å¦‚ã‚’è‡ªå‹•ä¿®æ­£
        """
        import re
        
        if not issues:
            return generated_files
        
        for issue in issues:
            filepath = issue['file']
            if filepath not in generated_files:
                continue
            
            content = generated_files[filepath]
            attr_name = issue['attribute']
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³: attr = mapped_column(ForeignKey(...)) â†’ attr: Mapped[int] = mapped_column(ForeignKey(...))
            pattern = rf'^(\s+){attr_name}\s*=\s*(mapped_column\s*\(\s*ForeignKey)'
            replacement = rf'\1{attr_name}: Mapped[int] = \2'
            
            new_content = re.sub(pattern, replacement, content, flags=re.MULTILINE)
            
            # Columnãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚å¯¾å¿œ
            pattern2 = rf'^(\s+){attr_name}\s*=\s*(Column\s*\(\s*(?:Integer,\s*)?ForeignKey)'
            replacement2 = rf'\1{attr_name}: Mapped[int] = mapped_column(\2'
            
            new_content = re.sub(pattern2, replacement2, new_content, flags=re.MULTILINE)
            
            if new_content != content:
                generated_files[filepath] = new_content
                logger.debug(f"[G-25] âœ… Fixed {attr_name} in {filepath}")
        
        return generated_files

    # ============================================
    # G-26: Mappedå‹ã¨SQLAlchemyå‹ã®ä¸ä¸€è‡´ãƒã‚§ãƒƒã‚¯
    # ============================================
    
    def _validate_mapped_sqlalchemy_type_mismatch(
        self,
        generated_files: Dict[str, str]
    ) -> List[Dict[str, Any]]:
        """
        Mapped[]å‹æ³¨é‡ˆã¨mapped_column()ã®SQLAlchemyå‹ã®ä¸ä¸€è‡´ã‚’æ¤œå‡º
        
        ä¾‹: Mapped[int] = mapped_column(String) â† å‹ä¸ä¸€è‡´
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã®ãƒªã‚¹ãƒˆ
        """
        import re
        
        issues = []
        
        # Pythonå‹ã¨SQLAlchemyå‹ã®ãƒãƒƒãƒ”ãƒ³ã‚°
        type_mapping = {
            'int': ['Integer', 'BigInteger', 'SmallInteger'],
            'str': ['String', 'Text', 'VARCHAR', 'CHAR'],
            'float': ['Float', 'Numeric', 'DECIMAL'],
            'bool': ['Boolean'],
            'datetime': ['DateTime'],
            'date': ['Date'],
            'time': ['Time'],
            'bytes': ['LargeBinary', 'BLOB'],
        }
        
        # é€†ãƒãƒƒãƒ”ãƒ³ã‚°
        sqlalchemy_to_python = {}
        for py_type, sa_types in type_mapping.items():
            for sa_type in sa_types:
                sqlalchemy_to_python[sa_type.lower()] = py_type
        
        for filepath, content in generated_files.items():
            if not filepath.endswith('.py'):
                continue
            
            content = content.replace('\r\n', '\n')
            lines = content.split('\n')
            
            current_class = None
            
            for line_num, line in enumerate(lines, 1):
                class_match = re.match(r'^class\s+(\w+)\s*\(', line)
                if class_match:
                    current_class = class_match.group(1)
                    continue
                
                if not current_class:
                    continue
                
                # ãƒ‘ã‚¿ãƒ¼ãƒ³: attr: Mapped[PythonType] = mapped_column(SQLAlchemyType, ...)
                match = re.search(
                    r'(\w+)\s*:\s*Mapped\[(\w+)\]\s*=\s*mapped_column\s*\(\s*(\w+)',
                    line
                )
                
                if match:
                    attr_name = match.group(1)
                    python_type = match.group(2).lower()
                    sqlalchemy_type = match.group(3).lower()
                    
                    # ForeignKeyã‚„primary_keyã¯ç‰¹åˆ¥æ‰±ã„
                    if sqlalchemy_type in ['foreignkey', 'primary_key']:
                        continue
                    
                    # å‹ãƒã‚§ãƒƒã‚¯
                    expected_python = sqlalchemy_to_python.get(sqlalchemy_type)
                    if expected_python and expected_python != python_type:
                        # Optional[]ã‚„List[]ãªã©ã®è¤‡åˆå‹ã¯ç„¡è¦–
                        if python_type not in ['optional', 'list', 'dict', 'any']:
                            issues.append({
                                'type': 'mapped_type_mismatch',
                                'class': current_class,
                                'attribute': attr_name,
                                'python_type': python_type,
                                'sqlalchemy_type': sqlalchemy_type,
                                'expected_python_type': expected_python,
                                'file': filepath,
                                'line': line_num,
                                'severity': 'warning',
                                'details': f"'{current_class}.{attr_name}': Mapped[{python_type}] with {sqlalchemy_type} (expected Mapped[{expected_python}])"
                            })
        
        if issues:
            logger.debug(f"[G-26] Found {len(issues)} Mapped/SQLAlchemy type mismatch issues")
        else:
            logger.debug("[G-26] All Mapped types match SQLAlchemy types")
        
        return issues
    
    def _auto_fix_mapped_sqlalchemy_type_mismatch(
        self,
        issues: List[Dict[str, Any]],
        generated_files: Dict[str, str]
    ) -> Dict[str, str]:
        """
        Mappedå‹ã¨SQLAlchemyå‹ã®ä¸ä¸€è‡´ã‚’è‡ªå‹•ä¿®æ­£
        """
        import re
        
        if not issues:
            return generated_files
        
        for issue in issues:
            filepath = issue['file']
            if filepath not in generated_files:
                continue
            
            content = generated_files[filepath]
            attr_name = issue['attribute']
            old_type = issue['python_type']
            new_type = issue['expected_python_type']
            
            # Mapped[old_type] â†’ Mapped[new_type]
            pattern = rf'({attr_name}\s*:\s*Mapped\[){old_type}(\])'
            replacement = rf'\g<1>{new_type}\g<2>'
            
            new_content = re.sub(pattern, replacement, content, flags=re.IGNORECASE)
            
            if new_content != content:
                generated_files[filepath] = new_content
                logger.debug(f"[G-26] âœ… Fixed {attr_name} type: {old_type} â†’ {new_type}")
        
        return generated_files

    # ============================================
    # G-27: relationship overlapsæ¤œå‡º
    # ============================================
    
    def _validate_relationship_overlaps(
        self,
        generated_files: Dict[str, str]
    ) -> List[Dict[str, Any]]:
        """
        åŒã˜ã‚«ãƒ©ãƒ ã‚’è¤‡æ•°ã®relationshipãŒå‚ç…§ã™ã‚‹å ´åˆã‚’æ¤œå‡º
        
        SQLAlchemyã¯è­¦å‘Šã‚’å‡ºã™ãŒã€å®Ÿè¡Œæ™‚ã‚¨ãƒ©ãƒ¼ã«ã¯ãªã‚‰ãªã„
        ã—ã‹ã—ã€æ„å›³ã—ãªã„å‹•ä½œã‚’å¼•ãèµ·ã“ã™å¯èƒ½æ€§ãŒã‚ã‚‹
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã®ãƒªã‚¹ãƒˆ
        """
        import re
        
        issues = []
        
        # ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®relationshipã‚’åé›†
        # {model_name: [(attr_name, target_model, has_back_populates, filepath, line)]}
        model_relationships = {}
        
        for filepath, content in generated_files.items():
            if not filepath.endswith('.py'):
                continue
            
            content = content.replace('\r\n', '\n')
            lines = content.split('\n')
            
            current_class = None
            
            for line_num, line in enumerate(lines, 1):
                class_match = re.match(r'^class\s+(\w+)\s*\(', line)
                if class_match:
                    current_class = class_match.group(1)
                    if current_class not in model_relationships:
                        model_relationships[current_class] = []
                    continue
                
                if not current_class:
                    continue
                
                # relationshipå®šç¾©ã‚’æ¤œå‡º
                rel_match = re.search(
                    r'(\w+)\s*(?::\s*Mapped\[.*?\])?\s*=\s*relationship\s*\(\s*["\']?(\w+)["\']?',
                    line
                )
                
                if rel_match:
                    attr_name = rel_match.group(1)
                    target_model = rel_match.group(2)
                    has_back_populates = 'back_populates' in line
                    
                    model_relationships[current_class].append({
                        'attr_name': attr_name,
                        'target_model': target_model,
                        'has_back_populates': has_back_populates,
                        'filepath': filepath,
                        'line': line_num
                    })
        
        # åŒã˜ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¸ã®è¤‡æ•°relationshipã‚’ãƒã‚§ãƒƒã‚¯
        for model_name, relationships in model_relationships.items():
            target_counts = {}
            for rel in relationships:
                target = rel['target_model']
                if target not in target_counts:
                    target_counts[target] = []
                target_counts[target].append(rel)
            
            for target, rels in target_counts.items():
                if len(rels) > 1:
                    # è¤‡æ•°ã®relationshipãŒåŒã˜ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’å‚ç…§
                    # back_populatesãŒãªã„å ´åˆã¯è­¦å‘Š
                    missing_back_populates = [r for r in rels if not r['has_back_populates']]
                    
                    if missing_back_populates:
                        for rel in missing_back_populates:
                            issues.append({
                                'type': 'relationship_overlap',
                                'model': model_name,
                                'attribute': rel['attr_name'],
                                'target_model': target,
                                'file': rel['filepath'],
                                'line': rel['line'],
                                'severity': 'warning',
                                'details': f"'{model_name}' has multiple relationships to '{target}' without back_populates"
                            })
        
        if issues:
            logger.debug(f"[G-27] Found {len(issues)} relationship overlap issues")
        else:
            logger.debug("[G-27] No relationship overlap issues found")
        
        return issues
    
    def _auto_fix_relationship_overlaps(
        self,
        issues: List[Dict[str, Any]],
        generated_files: Dict[str, str]
    ) -> Dict[str, str]:
        """
        relationship overlapsã®å•é¡Œã¯è­¦å‘Šã®ã¿ï¼ˆè‡ªå‹•ä¿®æ­£ã¯è¤‡é›‘ãªãŸã‚ï¼‰
        """
        if issues:
            for issue in issues:
                logger.debug(f"[G-27] âš  {issue['details']}")
        
        return generated_files

    # ============================================
    # G-28: å¾ªç’°ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ¤œå‡º
    # ============================================
    
    def _validate_circular_imports(
        self,
        generated_files: Dict[str, str]
    ) -> List[Dict[str, Any]]:
        """
        Python/Flaskã®å¾ªç’°ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’æ¤œå‡º
        
        å¾ªç’°ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¯ImportErrorã‚„AttributeErrorã‚’å¼•ãèµ·ã“ã™
        ç‰¹ã«Flaskã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§é »ç™ºã™ã‚‹å•é¡Œ
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã®ãƒªã‚¹ãƒˆ
        """
        import re
        from pathlib import Path
        
        issues = []
        
        # ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰
        # {module_name: [imported_modules]}
        import_graph = {}
        
        for filepath, content in generated_files.items():
            if not filepath.endswith('.py'):
                continue
            
            # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åã‚’æŠ½å‡ºï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰ï¼‰
            module_name = Path(filepath).stem
            if module_name == '__init__':
                module_name = Path(filepath).parent.name
            
            content = content.replace('\r\n', '\n')
            
            imports = set()
            
            # from X import Y ãƒ‘ã‚¿ãƒ¼ãƒ³
            for match in re.finditer(r'^from\s+([\w.]+)\s+import', content, re.MULTILINE):
                imported = match.group(1).split('.')[0]  # ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
                imports.add(imported)
            
            # import X ãƒ‘ã‚¿ãƒ¼ãƒ³
            for match in re.finditer(r'^import\s+([\w.]+)', content, re.MULTILINE):
                imported = match.group(1).split('.')[0]
                imports.add(imported)
            
            import_graph[module_name] = list(imports)
        
        # å¾ªç’°ã‚’æ¤œå‡ºï¼ˆDFSï¼‰
        def find_cycle(node, visited, path):
            if node in path:
                cycle_start = path.index(node)
                return path[cycle_start:] + [node]
            
            if node in visited:
                return None
            
            visited.add(node)
            path.append(node)
            
            for neighbor in import_graph.get(node, []):
                if neighbor in import_graph:  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã¿
                    cycle = find_cycle(neighbor, visited, path)
                    if cycle:
                        return cycle
            
            path.pop()
            return None
        
        visited = set()
        for module in import_graph:
            if module not in visited:
                cycle = find_cycle(module, visited, [])
                if cycle:
                    cycle_str = ' -> '.join(cycle)
                    issues.append({
                        'type': 'circular_import',
                        'cycle': cycle,
                        'severity': 'error',
                        'details': f"Circular import detected: {cycle_str}"
                    })
        
        if issues:
            logger.debug(f"[G-28] Found {len(issues)} circular import issues")
        else:
            logger.debug("[G-28] No circular imports detected")
        
        return issues
    
    def _auto_fix_circular_imports(
        self,
        issues: List[Dict[str, Any]],
        generated_files: Dict[str, str]
    ) -> Dict[str, str]:
        """
        å¾ªç’°ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®å•é¡Œã¯è­¦å‘Šã®ã¿ï¼ˆè‡ªå‹•ä¿®æ­£ã¯è¤‡é›‘ãªãŸã‚ï¼‰
        
        æ¨å¥¨è§£æ±ºç­–ã‚’ãƒ­ã‚°ã«å‡ºåŠ›
        """
        if issues:
            for issue in issues:
                logger.debug(f"[G-28] âš  {issue['details']}")
                logger.debug("[G-28] æ¨å¥¨è§£æ±ºç­–:")
                logger.debug("  1. TYPE_CHECKING ãƒ–ãƒ­ãƒƒã‚¯ã‚’ä½¿ç”¨")
                logger.debug("  2. é…å»¶ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆé–¢æ•°å†…importï¼‰")
                logger.debug("  3. å…±é€šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«dbã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’åˆ†é›¢")
        
        return generated_files

    # ============================================
    # G-29: Marshmallow Nestedã‚¹ã‚­ãƒ¼ãƒå¾ªç’°å‚ç…§æ¤œå‡º
    # ============================================
    
    def _validate_marshmallow_nested_circular(
        self,
        generated_files: Dict[str, str]
    ) -> List[Dict[str, Any]]:
        """
        Marshmallow Nestedã‚¹ã‚­ãƒ¼ãƒã®å¾ªç’°å‚ç…§ã‚’æ¤œå‡º
        
        ã‚¹ã‚­ãƒ¼ãƒAãŒã‚¹ã‚­ãƒ¼ãƒBã‚’Nested()ã§å‚ç…§ã—ã€
        ã‚¹ã‚­ãƒ¼ãƒBãŒã‚¹ã‚­ãƒ¼ãƒAã‚’Nested()ã§å‚ç…§ã™ã‚‹å ´åˆã‚’æ¤œå‡º
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã®ãƒªã‚¹ãƒˆ
        """
        import re
        
        issues = []
        
        # ã‚¹ã‚­ãƒ¼ãƒã®Nestedå‚ç…§ã‚’åé›†
        # {schema_name: [nested_schema_names]}
        schema_graph = {}
        schema_files = {}  # {schema_name: (filepath, line)}
        
        for filepath, content in generated_files.items():
            if not filepath.endswith('.py'):
                continue
            if 'schema' not in filepath.lower() and 'Schema' not in content:
                continue
            
            content = content.replace('\r\n', '\n')
            lines = content.split('\n')
            
            current_schema = None
            
            for line_num, line in enumerate(lines, 1):
                # ã‚¹ã‚­ãƒ¼ãƒã‚¯ãƒ©ã‚¹å®šç¾©ã‚’æ¤œå‡º
                class_match = re.match(r'^class\s+(\w+Schema)\s*\(', line)
                if class_match:
                    current_schema = class_match.group(1)
                    if current_schema not in schema_graph:
                        schema_graph[current_schema] = []
                        schema_files[current_schema] = (filepath, line_num)
                    continue
                
                if not current_schema:
                    continue
                
                # Nested()ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æ¤œå‡º
                nested_match = re.search(r'fields\.Nested\s*\(\s*["\']?(\w+)["\']?', line)
                if nested_match:
                    nested_schema = nested_match.group(1)
                    schema_graph[current_schema].append(nested_schema)
        
        # å¾ªç’°ã‚’æ¤œå‡º
        def find_schema_cycle(schema, visited, path):
            if schema in path:
                cycle_start = path.index(schema)
                return path[cycle_start:] + [schema]
            
            if schema in visited:
                return None
            
            visited.add(schema)
            path.append(schema)
            
            for nested in schema_graph.get(schema, []):
                if nested in schema_graph:
                    cycle = find_schema_cycle(nested, visited, path)
                    if cycle:
                        return cycle
            
            path.pop()
            return None
        
        visited = set()
        for schema in schema_graph:
            if schema not in visited:
                cycle = find_schema_cycle(schema, visited, [])
                if cycle:
                    cycle_str = ' -> '.join(cycle)
                    filepath, line = schema_files.get(cycle[0], ('unknown', 0))
                    issues.append({
                        'type': 'marshmallow_nested_circular',
                        'cycle': cycle,
                        'file': filepath,
                        'line': line,
                        'severity': 'warning',
                        'details': f"Marshmallow Nested circular reference: {cycle_str}"
                    })
        
        if issues:
            logger.debug(f"[G-29] Found {len(issues)} Marshmallow Nested circular issues")
        else:
            logger.debug("[G-29] No Marshmallow Nested circular references detected")
        
        return issues
    
    def _auto_fix_marshmallow_nested_circular(
        self,
        issues: List[Dict[str, Any]],
        generated_files: Dict[str, str]
    ) -> Dict[str, str]:
        """
        Marshmallow Nestedå¾ªç’°å‚ç…§ã¯è­¦å‘Šã®ã¿
        
        æ¨å¥¨è§£æ±ºç­–ã‚’ãƒ­ã‚°ã«å‡ºåŠ›
        """
        if issues:
            for issue in issues:
                logger.debug(f"[G-29] âš  {issue['details']}")
                logger.debug("[G-29] æ¨å¥¨è§£æ±ºç­–:")
                logger.debug("  1. æ–‡å­—åˆ—å‚ç…§ã‚’ä½¿ç”¨: fields.Nested('SchemaName')")
                logger.debug("  2. exclude ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å¾ªç’°ã‚’æ–­ã¡åˆ‡ã‚‹")
                logger.debug("  3. only ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å¿…è¦ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã¿å«ã‚ã‚‹")
        
        return generated_files

    # ============================================
    # G-31: Marshmallow APIèª¤ç”¨æ¤œå‡º
    # ============================================
    
    def _validate_marshmallow_api_usage(
        self,
        generated_files: Dict[str, str]
    ) -> List[Dict[str, Any]]:
        """
        Marshmallowå›ºæœ‰ã®APIèª¤ç”¨ã‚’é™çš„ã«æ¤œå‡º
        
        æ¤œå‡ºé …ç›®:
        - required=True ã¨ load_default ã®åŒæ™‚ä½¿ç”¨ï¼ˆValueErrorï¼‰
        - required=True ã¨ missing ã®åŒæ™‚ä½¿ç”¨ï¼ˆdeprecatedã€ValueErrorï¼‰
        - dump_only=True ã¨ required=True ã®åŒæ™‚ä½¿ç”¨ï¼ˆè«–ç†çŸ›ç›¾ï¼‰
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã®ãƒªã‚¹ãƒˆ
        """
        import re
        
        issues = []
        
        for filepath, content in generated_files.items():
            if not filepath.endswith('.py'):
                continue
            
            # ã‚¹ã‚­ãƒ¼ãƒãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯Marshmallowã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å¯¾è±¡
            if 'schema' not in filepath.lower() and 'Schema' not in content:
                continue
            if 'marshmallow' not in content and 'fields.' not in content:
                continue
            
            content = content.replace('\r\n', '\n')
            
            # å…¨ã¦ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å®šç¾©é–‹å§‹ä½ç½®ã‚’è¦‹ã¤ã‘ã‚‹
            # ãƒ‘ã‚¿ãƒ¼ãƒ³: field_name = fields.Type(
            field_starts = list(re.finditer(r'(\w+)\s*=\s*fields\.(\w+)\s*\(', content))
            
            for match in field_starts:
                field_name = match.group(1)
                field_type = match.group(2)
                start_pos = match.end() - 1  # '(' ã®ä½ç½®
                
                # æ‹¬å¼§ã®ãƒã‚¹ãƒˆã‚’è€ƒæ…®ã—ã¦é–‰ã˜æ‹¬å¼§ã‚’è¦‹ã¤ã‘ã‚‹
                paren_count = 1
                pos = start_pos + 1
                while pos < len(content) and paren_count > 0:
                    if content[pos] == '(':
                        paren_count += 1
                    elif content[pos] == ')':
                        paren_count -= 1
                    pos += 1
                
                # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å®šç¾©å…¨ä½“ã‚’æŠ½å‡º
                field_def = content[start_pos:pos]
                
                # è¡Œç•ªå·ã‚’è¨ˆç®—
                line_num = content[:match.start()].count('\n') + 1
                
                # â˜… æ–‡å­—åˆ—ãƒªãƒ†ãƒ©ãƒ«ã‚’é™¤å¤–ã—ã¦ã‹ã‚‰æ¤œå‡ºï¼ˆèª¤æ¤œå‡ºé˜²æ­¢ï¼‰
                # æ–‡å­—åˆ—å†…ã® "required=True" ç­‰ã‚’èª¤æ¤œå‡ºã—ãªã„ãŸã‚
                field_def_no_strings = re.sub(r'""".*?"""', '', field_def, flags=re.DOTALL)
                field_def_no_strings = re.sub(r"'''.*?'''", '', field_def_no_strings, flags=re.DOTALL)
                field_def_no_strings = re.sub(r'"[^"\\]*(?:\\.[^"\\]*)*"', '', field_def_no_strings)
                field_def_no_strings = re.sub(r"'[^'\\]*(?:\\.[^'\\]*)*'", '', field_def_no_strings)
                
                # ãƒ‘ã‚¿ãƒ¼ãƒ³1: required=True ã¨ load_default ã®åŒæ™‚ä½¿ç”¨
                has_required_true = re.search(r'required\s*=\s*True', field_def_no_strings)
                has_load_default = re.search(r'load_default\s*=', field_def_no_strings)
                
                if has_required_true and has_load_default:
                    issues.append({
                        'type': 'marshmallow_required_load_default',
                        'file': filepath,
                        'line': line_num,
                        'field': field_name,
                        'details': f"Marshmallow 3.x: 'required=True' and 'load_default' cannot be used together in field '{field_name}'. This will raise ValueError at import time.",
                        'fix': "Remove 'required=True' (use required=False with load_default) or remove 'load_default' (keep required=True for mandatory fields)"
                    })
                    logger.debug(f"[G-31] {filepath}:{line_num} - required=True + load_default in '{field_name}'")
                
                # ãƒ‘ã‚¿ãƒ¼ãƒ³2: required=True ã¨ missing ã®åŒæ™‚ä½¿ç”¨ï¼ˆdeprecatedï¼‰
                has_missing = re.search(r'missing\s*=', field_def_no_strings)
                
                if has_required_true and has_missing:
                    issues.append({
                        'type': 'marshmallow_required_missing',
                        'file': filepath,
                        'line': line_num,
                        'field': field_name,
                        'details': f"Marshmallow 3.x: 'required=True' and 'missing' cannot be used together in field '{field_name}'. 'missing' is deprecated, use 'load_default' instead.",
                        'fix': "Remove 'required=True' and replace 'missing' with 'load_default'"
                    })
                    logger.debug(f"[G-31] {filepath}:{line_num} - required=True + missing in '{field_name}'")
                
                # ãƒ‘ã‚¿ãƒ¼ãƒ³3: dump_only=True ã¨ required=True ã®åŒæ™‚ä½¿ç”¨ï¼ˆè«–ç†çŸ›ç›¾ï¼‰
                has_dump_only = re.search(r'dump_only\s*=\s*True', field_def_no_strings)
                
                if has_dump_only and has_required_true:
                    issues.append({
                        'type': 'marshmallow_dump_only_required',
                        'file': filepath,
                        'line': line_num,
                        'field': field_name,
                        'details': f"Marshmallow: 'dump_only=True' and 'required=True' is a logical contradiction in field '{field_name}'. dump_only fields are ignored during load, so required has no effect.",
                        'fix': "Remove 'required=True' from dump_only field"
                    })
                    logger.debug(f"[G-31] {filepath}:{line_num} - dump_only=True + required=True in '{field_name}'")
        
        if issues:
            logger.debug(f"[G-31] Found {len(issues)} Marshmallow API usage issues")
        else:
            logger.debug("[G-31] No Marshmallow API usage issues detected")
        
        return issues
    
    def _auto_fix_marshmallow_api_usage(
        self,
        issues: List[Dict[str, Any]],
        generated_files: Dict[str, str]
    ) -> Dict[str, str]:
        """
        Marshmallow APIèª¤ç”¨ã‚’LLMã§ä¿®æ­£
        
        Args:
            issues: æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã®ãƒªã‚¹ãƒˆ
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«
        
        Returns:
            æ›´æ–°ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«è¾æ›¸
        """
        if not issues:
            return generated_files
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        issues_by_file = {}
        for issue in issues:
            filepath = issue['file']
            if filepath not in issues_by_file:
                issues_by_file[filepath] = []
            issues_by_file[filepath].append(issue)
        
        for filepath, file_issues in issues_by_file.items():
            if filepath not in generated_files:
                continue
            
            code = generated_files[filepath]
            
            # å•é¡Œã®è©³ç´°
            issue_descriptions = []
            for issue in file_issues:
                desc = f"- Line {issue['line']}: Field '{issue['field']}' - {issue['details']}"
                desc += f"\n  Suggested fix: {issue['fix']}"
                issue_descriptions.append(desc)
            
            issues_text = "\n".join(issue_descriptions)
            
            fix_prompt = f"""Fix the Marshmallow schema API usage errors.

## File: {filepath}
```python
{code}
```

## Issues Found:
{issues_text}

## Fix Instructions:
1. For 'required=True' + 'load_default' conflict:
   - If the field should have a default value when not provided: Remove 'required=True', keep 'load_default'
   - If the field is truly mandatory: Remove 'load_default', keep 'required=True'
   - Usually, if there's a sensible default (like 'Anonymous' for player_name), use required=False with load_default

2. For 'required=True' + 'missing' conflict:
   - Replace 'missing' with 'load_default' (missing is deprecated in Marshmallow 3.x)
   - Remove 'required=True'

3. For 'dump_only=True' + 'required=True' conflict:
   - Remove 'required=True' (dump_only fields are only for serialization, not deserialization)

## Output
Return ONLY the complete fixed Python code without markdown code blocks.
"""
            
            try:
                logger.debug(f"[G-31] Fixing: {filepath}")
                
                response = self._call_llm_api(fix_prompt)
                
                if response:
                    import re
                    fixed_code = response.strip()
                    fixed_code = re.sub(r'^```\w*\n?', '', fixed_code)
                    fixed_code = re.sub(r'\n?```$', '', fixed_code)
                    
                    if fixed_code and len(fixed_code) > 100:
                        generated_files[filepath] = fixed_code
                        logger.debug(f"[G-31] âœ… Fixed {filepath}")
                    else:
                        logger.debug(f"[G-31] âš  Generated code too short for {filepath}")
                else:
                    logger.debug(f"[G-31] âš  No response for {filepath}")
                    
            except Exception as e:
                logger.debug(f"[G-31] âš  Error fixing {filepath}: {e}")
        
        return generated_files

    # ============================================
    # G-30: JavaScript thiså‚ç…§å–ªå¤±ãƒã‚§ãƒƒã‚¯
    # ============================================
    
    def _validate_js_this_context_loss(
        self,
        generated_files: Dict[str, str]
    ) -> List[Dict[str, Any]]:
        """
        JavaScriptã§thiså‚ç…§ãŒå–ªå¤±ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
        
        ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°ã§this.methodã‚’ç›´æ¥æ¸¡ã™ã¨ã€
        thisã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒå–ªå¤±ã—ã¦undefinedã«ãªã‚‹
        
        æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³:
        - requestAnimationFrame(this.method)
        - setTimeout(this.method, ...)
        - setInterval(this.method, ...)
        - addEventListener(..., this.method)
        - .then(this.method)
        - .catch(this.method)
        - .forEach(this.method)
        - .map(this.method)
        - .filter(this.method)
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã®ãƒªã‚¹ãƒˆ
        """
        import re
        
        issues = []
        
        # thiså–ªå¤±ã‚’å¼•ãèµ·ã“ã™ãƒ‘ã‚¿ãƒ¼ãƒ³
        dangerous_patterns = [
            (r'requestAnimationFrame\s*\(\s*this\.(\w+)\s*\)', 'requestAnimationFrame'),
            (r'setTimeout\s*\(\s*this\.(\w+)\s*,', 'setTimeout'),
            (r'setInterval\s*\(\s*this\.(\w+)\s*,', 'setInterval'),
            (r'addEventListener\s*\([^,]+,\s*this\.(\w+)\s*\)', 'addEventListener'),
            (r'\.then\s*\(\s*this\.(\w+)\s*\)', '.then()'),
            (r'\.catch\s*\(\s*this\.(\w+)\s*\)', '.catch()'),
            (r'\.forEach\s*\(\s*this\.(\w+)\s*\)', '.forEach()'),
            (r'\.map\s*\(\s*this\.(\w+)\s*\)', '.map()'),
            (r'\.filter\s*\(\s*this\.(\w+)\s*\)', '.filter()'),
        ]
        
        for filepath, content in generated_files.items():
            if not filepath.endswith(('.js', '.jsx', '.ts', '.tsx')):
                continue
            
            content = content.replace('\r\n', '\n')
            lines = content.split('\n')
            
            for line_num, line in enumerate(lines, 1):
                for pattern, context in dangerous_patterns:
                    match = re.search(pattern, line)
                    if match:
                        method_name = match.group(1)
                        
                        # ã‚¢ãƒ­ãƒ¼é–¢æ•°ã‚„bind()ãŒä½¿ã‚ã‚Œã¦ã„ãªã„ã‹ãƒã‚§ãƒƒã‚¯
                        # æ­£ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³: requestAnimationFrame(() => this.method())
                        # æ­£ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³: requestAnimationFrame(this.method.bind(this))
                        if '=>' not in line and '.bind(' not in line:
                            issues.append({
                                'type': 'this_context_loss',
                                'method': method_name,
                                'context': context,
                                'file': filepath,
                                'line': line_num,
                                'severity': 'error',
                                'details': f"'{context}' with 'this.{method_name}' will lose 'this' context"
                            })
        
        if issues:
            logger.debug(f"[G-30] Found {len(issues)} JavaScript this context loss issues")
        else:
            logger.debug("[G-30] No JavaScript this context loss issues found")
        
        return issues
    
    def _auto_fix_js_this_context_loss(
        self,
        issues: List[Dict[str, Any]],
        generated_files: Dict[str, str]
    ) -> Dict[str, str]:
        """
        JavaScript thiså‚ç…§å–ªå¤±ã‚’è‡ªå‹•ä¿®æ­£
        
        this.method â†’ () => this.method() ã¾ãŸã¯ this.method.bind(this)
        """
        import re
        
        if not issues:
            return generated_files
        
        for issue in issues:
            filepath = issue['file']
            if filepath not in generated_files:
                continue
            
            content = generated_files[filepath]
            method_name = issue['method']
            context = issue['context']
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ã”ã¨ã«ä¿®æ­£
            if context == 'requestAnimationFrame':
                # requestAnimationFrame(this.method) â†’ requestAnimationFrame((t) => this.method(t))
                pattern = rf'requestAnimationFrame\s*\(\s*this\.{method_name}\s*\)'
                replacement = f'requestAnimationFrame((t) => this.{method_name}(t))'
                content = re.sub(pattern, replacement, content)
            
            elif context in ['setTimeout', 'setInterval']:
                # setTimeout(this.method, 100) â†’ setTimeout(() => this.method(), 100)
                pattern = rf'{context}\s*\(\s*this\.{method_name}\s*,'
                replacement = f'{context}(() => this.{method_name}(),'
                content = re.sub(pattern, replacement, content)
            
            elif context == 'addEventListener':
                # addEventListener('click', this.method) â†’ addEventListener('click', (e) => this.method(e))
                pattern = rf"addEventListener\s*\(\s*(['\"][^'\"]+['\"])\s*,\s*this\.{method_name}\s*\)"
                replacement = rf"addEventListener(\1, (e) => this.{method_name}(e))"
                content = re.sub(pattern, replacement, content)
            
            elif context in ['.then()', '.catch()']:
                # .then(this.method) â†’ .then((x) => this.method(x))
                dot_method = context.replace('()', '')
                pattern = rf'{re.escape(dot_method)}\s*\(\s*this\.{method_name}\s*\)'
                replacement = f'{dot_method}((x) => this.{method_name}(x))'
                content = re.sub(pattern, replacement, content)
            
            elif context in ['.forEach()', '.map()', '.filter()']:
                # .forEach(this.method) â†’ .forEach((item) => this.method(item))
                dot_method = context.replace('()', '')
                pattern = rf'{re.escape(dot_method)}\s*\(\s*this\.{method_name}\s*\)'
                replacement = f'{dot_method}((item) => this.{method_name}(item))'
                content = re.sub(pattern, replacement, content)
            
            generated_files[filepath] = content
            logger.debug(f"[G-30] âœ… Fixed this.{method_name} in {context}")
        
        return generated_files

    # ============================================
    # ESLintçµ±åˆ: JavaScriptå“è³ªãƒã‚§ãƒƒã‚¯
    # ============================================
    
    def _check_eslint_available(self) -> bool:
        """
        ESLintãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
        
        Returns:
            ESLintãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆTrue
        """
        import subprocess
        import shutil
        import platform
        
        is_windows = platform.system() == 'Windows'
        
        # npxãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
        # Windows: npx.cmd, Unix: npx
        npx_cmd = shutil.which('npx.cmd') if is_windows else shutil.which('npx')
        if not npx_cmd and is_windows:
            npx_cmd = shutil.which('npx')  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        
        if npx_cmd:
            try:
                result = subprocess.run(
                    [npx_cmd, 'eslint', '--version'],
                    capture_output=True,
                    timeout=30,
                    shell=is_windows,  # Windowsã§ã¯shell=TrueãŒå¿…è¦ãªå ´åˆãŒã‚ã‚‹
                    encoding='utf-8',
                    errors='replace'
                )
                if result.returncode == 0:
                    version = result.stdout.strip()
                    logger.debug(f"[ESLint] Available via npx: {version}")
                    self._eslint_cmd = [npx_cmd, 'eslint']
                    return True
            except Exception as e:
                logger.debug(f"[ESLint] npx check failed: {e}")
        
        # ç›´æ¥eslintã‚³ãƒãƒ³ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
        # Windows: eslint.cmd, Unix: eslint
        eslint_cmd = shutil.which('eslint.cmd') if is_windows else shutil.which('eslint')
        if not eslint_cmd and is_windows:
            eslint_cmd = shutil.which('eslint')  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        
        if eslint_cmd:
            try:
                result = subprocess.run(
                    [eslint_cmd, '--version'],
                    capture_output=True,
                    timeout=10,
                    shell=is_windows,
                    encoding='utf-8',
                    errors='replace'
                )
                if result.returncode == 0:
                    version = result.stdout.strip()
                    logger.debug(f"[ESLint] Available directly: {version}")
                    self._eslint_cmd = [eslint_cmd]
                    return True
            except Exception as e:
                logger.debug(f"[ESLint] direct check failed: {e}")
        
        logger.debug("[ESLint] Not available")
        return False
    
    def _run_eslint_on_files(
        self,
        generated_files: Dict[str, str]
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆã•ã‚ŒãŸJavaScriptãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾ã—ã¦ESLintã‚’å®Ÿè¡Œ
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            ESLintçµæœ {
                'has_errors': bool,
                'errors': List[Dict],
                'warnings': List[Dict],
                'files_checked': int
            }
        """
        import subprocess
        import tempfile
        import os
        import json
        
        result = {
            'has_errors': False,
            'errors': [],
            'warnings': [],
            'files_checked': 0
        }
        
        # JavaScriptãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŠ½å‡º
        js_files = {
            fp: content for fp, content in generated_files.items()
            if fp.endswith(('.js', '.jsx', '.mjs'))
        }
        
        if not js_files:
            logger.debug("[ESLint] No JavaScript files to check")
            return result
        
        result['files_checked'] = len(js_files)
        
        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›¸ãå‡ºã—ã¦ESLintå®Ÿè¡Œ
        with tempfile.TemporaryDirectory() as tmpdir:
            # ESLint v9 Flat Configå½¢å¼ï¼ˆeslint.config.jsï¼‰
            eslint_config_js = '''export default [
    {
        languageOptions: {
            ecmaVersion: 2021,
            sourceType: "module",
            globals: {
                // Browser globals
                window: "readonly",
                document: "readonly",
                console: "readonly",
                fetch: "readonly",
                requestAnimationFrame: "readonly",
                cancelAnimationFrame: "readonly",
                setTimeout: "readonly",
                setInterval: "readonly",
                clearTimeout: "readonly",
                clearInterval: "readonly",
                localStorage: "readonly",
                sessionStorage: "readonly",
                alert: "readonly",
                confirm: "readonly",
                prompt: "readonly",
                performance: "readonly",
                navigator: "readonly",
                location: "readonly",
                history: "readonly",
                Image: "readonly",
                Audio: "readonly",
                Event: "readonly",
                CustomEvent: "readonly",
                FormData: "readonly",
                URLSearchParams: "readonly",
                URL: "readonly",
                Blob: "readonly",
                FileReader: "readonly",
                WebSocket: "readonly",
                Worker: "readonly",
                HTMLElement: "readonly",
                HTMLCanvasElement: "readonly",
                CanvasRenderingContext2D: "readonly",
                KeyboardEvent: "readonly",
                MouseEvent: "readonly",
                Headers: "readonly",
                Request: "readonly",
                Response: "readonly"
            }
        },
        rules: {
            // Errors (critical issues)
            "no-undef": "error",
            "no-unused-vars": ["error", {"args": "none", "varsIgnorePattern": "^_"}],
            "no-unreachable": "error",
            "no-dupe-keys": "error",
            "no-duplicate-case": "error",
            "no-empty": "error",
            "no-func-assign": "error",
            "no-invalid-regexp": "error",
            "no-obj-calls": "error",
            "no-sparse-arrays": "error",
            "use-isnan": "error",
            "valid-typeof": "error",
            // Warnings (quality improvements)
            "no-constant-condition": "warn",
            "no-debugger": "warn",
            "no-extra-semi": "warn",
            "no-redeclare": "warn"
        }
    }
];
'''
            
            config_path = os.path.join(tmpdir, 'eslint.config.js')
            with open(config_path, 'w', encoding='utf-8') as f:
                f.write(eslint_config_js)
            
            # JavaScriptãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«æ›¸ãå‡ºã—
            file_paths = []
            filepath_mapping = {}  # tmp_path -> original_path
            
            for filepath, content in js_files.items():
                # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ç¶­æŒ
                safe_path = filepath.replace('/', '_').replace('\\', '_')
                tmp_path = os.path.join(tmpdir, safe_path)
                
                with open(tmp_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                file_paths.append(tmp_path)
                filepath_mapping[tmp_path] = filepath
            
            # ESLintå®Ÿè¡Œï¼ˆv9 Flat Configï¼‰
            try:
                import platform
                is_windows = platform.system() == 'Windows'
                
                # _check_eslint_availableã§è¨­å®šã•ã‚ŒãŸã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨
                if hasattr(self, '_eslint_cmd') and self._eslint_cmd:
                    base_cmd = self._eslint_cmd
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    base_cmd = ['npx', 'eslint']
                
                cmd = base_cmd + ['--format', 'json'] + file_paths
                
                proc_result = subprocess.run(
                    cmd,
                    capture_output=True,
                    timeout=60,
                    cwd=tmpdir,
                    shell=is_windows,
                    encoding='utf-8',
                    errors='replace'
                )
                
                # ESLintã¯ã‚¨ãƒ©ãƒ¼ãŒã‚ã£ã¦ã‚‚çµ‚äº†ã‚³ãƒ¼ãƒ‰1ã‚’è¿”ã™
                if proc_result.stdout:
                    try:
                        eslint_output = json.loads(proc_result.stdout)
                        
                        for file_result in eslint_output:
                            tmp_path = file_result.get('filePath', '')
                            original_path = filepath_mapping.get(tmp_path, tmp_path)
                            
                            for msg in file_result.get('messages', []):
                                issue = {
                                    'file': original_path,
                                    'line': msg.get('line', 0),
                                    'column': msg.get('column', 0),
                                    'rule': msg.get('ruleId', 'unknown'),
                                    'message': msg.get('message', ''),
                                    'severity': msg.get('severity', 1)  # 1=warning, 2=error
                                }
                                
                                if msg.get('severity', 1) == 2:
                                    result['errors'].append(issue)
                                else:
                                    result['warnings'].append(issue)
                        
                        result['has_errors'] = len(result['errors']) > 0
                        
                    except json.JSONDecodeError as e:
                        logger.debug(f"[ESLint] Failed to parse output: {e}")
                        logger.debug(f"[ESLint] stdout: {proc_result.stdout[:500]}")
                
                if proc_result.stderr and 'error' in proc_result.stderr.lower():
                    logger.debug(f"[ESLint] stderr: {proc_result.stderr[:500]}")
                    
            except subprocess.TimeoutExpired:
                logger.debug("[ESLint] Timeout expired")
            except Exception as e:
                logger.debug(f"[ESLint] Execution failed: {e}")
        
        logger.debug(f"[ESLint] Checked {result['files_checked']} files: {len(result['errors'])} errors, {len(result['warnings'])} warnings")
        
        return result
    
    def _fix_eslint_errors_with_llm(
        self,
        generated_files: Dict[str, str],
        eslint_errors: List[Dict[str, Any]]
    ) -> Dict[str, str]:
        """
        ESLintã‚¨ãƒ©ãƒ¼ã‚’LLMã§è‡ªå‹•ä¿®æ­£
        
        Args:
            generated_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«
            eslint_errors: ESLintã‚¨ãƒ©ãƒ¼ãƒªã‚¹ãƒˆ
        
        Returns:
            ä¿®æ­£å¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«è¾æ›¸
        """
        if not eslint_errors or not self.llm_manager:
            return generated_files
        
        import re
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ã‚¨ãƒ©ãƒ¼ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        errors_by_file = {}
        for error in eslint_errors:
            filepath = error['file']
            if filepath not in errors_by_file:
                errors_by_file[filepath] = []
            errors_by_file[filepath].append(error)
        
        for filepath, file_errors in errors_by_file.items():
            if filepath not in generated_files:
                continue
            
            original_code = generated_files[filepath]
            
            # ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’æ•´å½¢
            error_details = []
            for err in file_errors:
                error_details.append(
                    f"- Line {err['line']}, Col {err['column']}: [{err['rule']}] {err['message']}"
                )
            
            prompt = f"""Fix the following ESLint errors in this JavaScript file.

**File: {filepath}**

**ESLint Errors:**
{chr(10).join(error_details)}

**Current Code:**
```javascript
{original_code}
```

**Instructions:**
1. Fix ALL the listed ESLint errors
2. For 'no-undef' errors: Either define the variable, import it, or add it as a parameter
3. For 'no-unused-vars' errors: Remove unused variables or use them appropriately
4. For other errors: Follow ESLint best practices to resolve them
5. Do NOT change the overall logic or functionality
6. Preserve all existing features and behavior

Return ONLY the fixed JavaScript code wrapped in ```javascript``` blocks.
"""
            
            try:
                logger.debug(f"[ESLint] Fixing {len(file_errors)} errors in {filepath}")
                
                response = self.llm_manager.generate_response(
                    prompt=prompt,
                    system_prompt="You are a JavaScript expert. Fix ESLint errors while preserving functionality.",
                    max_tokens=16000
                )
                
                if hasattr(response, 'content'):
                    response_text = response.content
                else:
                    response_text = str(response)
                
                # ã‚³ãƒ¼ãƒ‰æŠ½å‡º
                code_match = re.search(r'```(?:javascript|js)?\s*(.*?)\s*```', response_text, re.DOTALL)
                if code_match:
                    fixed_code = code_match.group(1).strip()
                    if len(fixed_code) > len(original_code) * 0.5:
                        generated_files[filepath] = fixed_code
                        logger.debug(f"[ESLint] âœ… Fixed {filepath}")
                    else:
                        logger.debug(f"[ESLint] âš  Generated content too short for {filepath}")
                else:
                    logger.debug(f"[ESLint] âš  No code block found for {filepath}")
                    
            except Exception as e:
                logger.debug(f"[ESLint] âš  Error fixing {filepath}: {e}")
        
        return generated_files

    # ============================================
    # ç·åˆãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆComprehensive Reviewï¼‰
    # ============================================
    
    def _final_comprehensive_review(
        self,
        all_files: Dict[str, str],
        goal: str
    ) -> Dict[str, str]:
        """
        ç·åˆãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆ16é …ç›®ãƒã‚§ãƒƒã‚¯ï¼†è‡ªå‹•ä¿®æ­£ï¼‰
        
        ã‚·ãƒ³ã‚°ãƒ«ã‚¹ãƒ†ãƒ¼ã‚¸/ãƒãƒ«ãƒã‚¹ãƒ†ãƒ¼ã‚¸å…±é€šã§æœ€å¾Œã«å¿…ãšå®Ÿè¡Œã€‚
        ã€Œä¸€ç™ºã§å‹•ãã€ã‚’ä¿è¨¼ã™ã‚‹ãŸã‚ã®æœ€çµ‚ãƒã‚§ãƒƒã‚¯ã€‚
        
        Args:
            all_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ä¸€å¼
            goal: å…ƒã®ã‚´ãƒ¼ãƒ«ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç†è§£ç”¨ï¼‰
        
        Returns:
            ä¿®æ­£å¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«ä¸€å¼
        """
        MAX_REVIEW_ATTEMPTS = 2  # ğŸ”§ æ™‚é–“çŸ­ç¸®: 4â†’2 (åŠ¹æœ: 50%å‰Šæ¸›)
        
        logger.debug("[Comprehensive Review] Starting final comprehensive review...")
        
        previous_issue_count = float('inf')
        no_improvement_count = 0
        
        # ğŸ†• ä¿®æ­£å›æ•°ãƒˆãƒ©ãƒƒã‚«ãƒ¼ï¼ˆWhack-a-moleé˜²æ­¢ï¼‰
        attempt_tracker = self.FixAttemptTracker(max_attempts=2)
        
        # ğŸ†• D1: P4/P5ã§å´ä¸‹ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¨˜éŒ²ï¼ˆä»¥é™ã®ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã‚¹ã‚­ãƒƒãƒ—ï¼‰
        rejected_files = set()
        
        for attempt in range(MAX_REVIEW_ATTEMPTS):
            # ============================================================
            # ğŸ†• æ”¹ä¿®: æ¤œå‡º2ãƒ¬ã‚¤ãƒ¤åŒ–
            # Layer 1: Blocking issuesï¼ˆãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚¨ãƒ©ãƒ¼ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ï¼‰ã®ã¿ä¿®æ­£å¯¾è±¡
            # Layer 2: Quality issues ã¯æœ€çµ‚å ±å‘Šã®ã¿ï¼ˆè‡ªå‹•ä¿®æ­£ã—ãªã„ï¼‰
            # ============================================================
            detection_result = self._detect_blocking_issues(all_files, goal)
            
            # ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã¯å‡¦ç†ã‚’ä¸­æ–­ï¼ˆèª¤åˆ¤å®šé˜²æ­¢ï¼‰
            if detection_result.get('error', False):
                error_msg = detection_result.get('error_message', 'Unknown error')
                logger.debug(f"[Blocking Detection] âš  Detection failed: {error_msg}")
                logger.debug(f"[Blocking Detection] Stopping review to prevent false success")
                break
            
            issues = detection_result.get('issues', [])
            current_issue_count = len(issues)
            
            if not issues:
                logger.debug(f"[Blocking Detection] âœ… No blocking issues found (attempt {attempt + 1})")
                break
            
            # â­ Zen HUD: åˆæœŸReview issueæ•°ã‚’è¨˜éŒ²ï¼ˆæœ€åˆã®ãƒ«ãƒ¼ãƒ—ã®ã¿ï¼‰
            if attempt == 0:
                self._zen_summary["review"]["initial"] = current_issue_count
                
                # ============================================
                # å·®åˆ†ã‚¹ã‚­ãƒ£ãƒ³: åˆå›æ¤œå‡ºçµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                # ============================================
                if self._differential_scan_enabled:
                    self._issue_cache = self._cache_issues(issues, all_files)
                    logger.debug(f"[Differential Scan] Initial cache created with {current_issue_count} issue(s)")
            
            # ğŸ†• åæŸåˆ¤å®š: issueæ•°ãŒæ¸›å°‘ã—ã¦ã„ãªã„å ´åˆ
            if current_issue_count >= previous_issue_count:
                no_improvement_count += 1
                logger.debug(f"[Comprehensive Review] No improvement ({current_issue_count} >= {previous_issue_count}), count={no_improvement_count}")
                
                # ğŸ”§ æ™‚é–“çŸ­ç¸®: 1å›ã§ã‚‚æ”¹å–„ãªã—ãªã‚‰å³çµ‚äº† (4â†’1)
                if no_improvement_count >= 1:
                    logger.debug(f"[Comprehensive Review] âš  No improvement detected, stopping early")
                    break
            else:
                no_improvement_count = 0
                improvement = previous_issue_count - current_issue_count
                if previous_issue_count != float('inf'):
                    logger.debug(f"[Comprehensive Review] Improved by {improvement} issue(s) ({previous_issue_count} -> {current_issue_count})")
            
            previous_issue_count = current_issue_count
            
            logger.debug(f"[Comprehensive Review] Found {current_issue_count} issue(s) (attempt {attempt + 1}/{MAX_REVIEW_ATTEMPTS})")
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
            logger.debug(f"    {Icon.WARNING.value} Found {current_issue_count} issue(s) in comprehensive review")
            
            # å•é¡Œã®æ¦‚è¦ã‚’è¡¨ç¤ºï¼ˆæœ€å¤§5ä»¶ï¼‰
            for i, issue in enumerate(issues[:5]):
                issue_file = issue.get('file', 'unknown')
                issue_type = issue.get('type', '?')
                issue_desc = issue.get('description', 'No description')[:60]
                logger.debug(f"      [{issue_type}] {issue_file}: {issue_desc}...")
            
            if len(issues) > 5:
                logger.debug(f"      ... and {len(issues) - 5} more issue(s)")
            
            # 2. å•é¡ŒãŒã‚ã‚Œã°ä¿®æ­£
            logger.debug(f"    {Icon.GEAR.value} Attempting to fix issues...")
            fixed_files = self._fix_comprehensive_issues(all_files, issues, goal, attempt_tracker, rejected_files, round_number=attempt)
            
            if fixed_files:
                # ============================================
                # Phase B: å·®åˆ†ã‚¹ã‚­ãƒ£ãƒ³ã«ã‚ˆã‚‹æ‚ªåŒ–æ¤œçŸ¥ï¼ˆv2.0ï¼‰
                # ç›®çš„: å¤‰æ›´ç¯„å›²ã®ã¿ã‚’å†æ¤œå‡ºã—ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨ãƒãƒ¼ã‚¸ã—ã¦åˆ¤å®š
                # åŠ¹æœ: LLMæ¤œå‡ºã®ã°ã‚‰ã¤ãã‚’å±€æ‰€åŒ–ã€ã€ŒãŠåŒ–ã‘Issueã€ã‚’é˜²æ­¢
                # ============================================
                REGRESSION_TOLERANCE = 0  # ğŸ”§ Issueå¢—åŠ ã¯è¨±å®¹ã—ãªã„ï¼ˆ2â†’0ã«å¤‰æ›´ï¼‰
                
                logger.debug(f"[Regression Check] Checking for regression...")
                
                apply_fix = True  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯é©ç”¨
                post_fix_count = current_issue_count  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                
                # å·®åˆ†ã‚¹ã‚­ãƒ£ãƒ³ãŒæœ‰åŠ¹ã‹ã¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå­˜åœ¨ã™ã‚‹å ´åˆ
                if self._differential_scan_enabled and self._issue_cache is not None:
                    # 1. ä¿®æ­£å‰å¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¯”è¼ƒã—ã¦å¤‰æ›´ç¯„å›²ã‚’æ¤œå‡º
                    modified_ranges = self._detect_modified_ranges_from_files(all_files, fixed_files)
                    
                    if modified_ranges:
                        logger.debug(f"[Differential Scan] Detected {len(modified_ranges)} modified range(s)")
                        
                        # 2. å¤‰æ›´ç¯„å›²ã®ã¿ã‚’å†æ¤œå‡º
                        new_issues = self._detect_issues_for_ranges(modified_ranges, fixed_files, goal)
                        
                        # 3. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨ãƒãƒ¼ã‚¸
                        merged_issues, updated_cache = self._merge_issues(
                            self._issue_cache,
                            new_issues,
                            modified_ranges,
                            fixed_files
                        )
                        
                        post_fix_count = len(merged_issues)
                        
                        # 4. åˆ¤å®š
                        if post_fix_count > current_issue_count + REGRESSION_TOLERANCE:
                            logger.debug(f"[Regression Check] âŒ Fix worsened issues: {current_issue_count} -> {post_fix_count} (+{post_fix_count - current_issue_count})")
                            logger.debug(f"[Comprehensive Review] âš  Fix rejected due to regression (exceeded tolerance of {REGRESSION_TOLERANCE})")
                            apply_fix = False
                            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯å¤‰æ›´ã—ãªã„ï¼ˆãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                        elif post_fix_count > current_issue_count:
                            logger.debug(f"[Regression Check] âš  Slight regression: {current_issue_count} -> {post_fix_count} (+{post_fix_count - current_issue_count}), within tolerance")
                            # è»½å¾®ãªæ‚ªåŒ– â†’ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–°ã—ã¦é©ç”¨
                            self._issue_cache = updated_cache
                        else:
                            improvement = current_issue_count - post_fix_count
                            if improvement > 0:
                                logger.debug(f"[Regression Check] âœ… Improved: {current_issue_count} -> {post_fix_count} (-{improvement})")
                            else:
                                logger.debug(f"[Regression Check] âœ… No change: {current_issue_count} -> {post_fix_count}")
                            # æ”¹å–„ã¾ãŸã¯åŒç­‰ â†’ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–°
                            self._issue_cache = updated_cache
                    else:
                        # å¤‰æ›´ç¯„å›²ãŒæ¤œå‡ºã•ã‚Œãªã‹ã£ãŸå ´åˆã€ä¿®æ­£ã‚’é©ç”¨
                        logger.debug(f"[Differential Scan] No changes detected, applying fix")
                else:
                    # å·®åˆ†ã‚¹ã‚­ãƒ£ãƒ³ç„¡åŠ¹ã¾ãŸã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã— â†’ å¾“æ¥æ–¹å¼ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                    logger.debug(f"[Regression Check] Using full scan (cache not available)")
                    post_fix_result = self._detect_comprehensive_issues(fixed_files, goal)
                    
                    if post_fix_result.get('error', False):
                        logger.debug(f"[Regression Check] Detection failed, applying fix anyway")
                    else:
                        post_fix_count = len(post_fix_result.get('issues', []))
                        
                        if post_fix_count > current_issue_count + REGRESSION_TOLERANCE:
                            logger.debug(f"[Regression Check] âŒ Fix worsened issues: {current_issue_count} -> {post_fix_count} (+{post_fix_count - current_issue_count})")
                            logger.debug(f"[Comprehensive Review] âš  Fix rejected due to regression (exceeded tolerance of {REGRESSION_TOLERANCE})")
                            apply_fix = False
                        elif post_fix_count > current_issue_count:
                            logger.debug(f"[Regression Check] âš  Slight regression: {current_issue_count} -> {post_fix_count} (+{post_fix_count - current_issue_count}), within tolerance")
                        else:
                            improvement = current_issue_count - post_fix_count
                            if improvement > 0:
                                logger.debug(f"[Regression Check] âœ… Improved: {current_issue_count} -> {post_fix_count} (-{improvement})")
                            else:
                                logger.debug(f"[Regression Check] âœ… No change: {current_issue_count} -> {post_fix_count}")
                
                if apply_fix:
                    all_files = fixed_files
                    logger.debug(f"[Comprehensive Review] Applied fixes (attempt {attempt + 1})")
                    logger.debug(f"    {Icon.SUCCESS.value} Issues fixed (attempt {attempt + 1}/{MAX_REVIEW_ATTEMPTS})")
                else:
                    # æ‚ªåŒ–ã«ã‚ˆã‚Šãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ â†’ ãƒ«ãƒ¼ãƒ—çµ‚äº†
                    logger.debug(f"[Comprehensive Review] Fix rolled back due to regression (attempt {attempt + 1})")
                    logger.debug(f"    {Icon.WARNING.value} Fix rejected - would worsen issues")
                    break
            else:
                logger.debug(f"[Comprehensive Review] Fix failed (attempt {attempt + 1})")
                logger.debug(f"    {Icon.WARNING.value} Could not fix all issues (attempt {attempt + 1}/{MAX_REVIEW_ATTEMPTS})")
                break
        
        # â­ Zen HUD: æœ€çµ‚Review issueæ•°ã‚’è¨˜éŒ²
        # å·®åˆ†ã‚¹ã‚­ãƒ£ãƒ³å¯¾å¿œ: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨
        # ã“ã‚Œã«ã‚ˆã‚Šã€ŒãŠåŒ–ã‘Issueã€ã‚’é˜²æ­¢ã—ã€ä¸€è²«ã—ãŸIssueæ•°ã‚’å ±å‘Š
        if self._differential_scan_enabled and self._issue_cache is not None:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰Issueæ•°ã‚’è¨ˆç®—
            final_issue_count = sum(len(issues) for issues in self._issue_cache.values())
            self._zen_summary["review"]["final"] = final_issue_count
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰issuesãƒªã‚¹ãƒˆã‚’æ§‹ç¯‰ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«å˜ä½ãƒšãƒŠãƒ«ãƒ†ã‚£ç”¨ï¼‰
            cached_issues = []
            for file_path, issues in self._issue_cache.items():
                for issue in issues:
                    cached_issues.append({
                        'file': issue.file,
                        'line': issue.line,
                        'type': issue.issue_type,
                        'description': issue.description,
                        'fix': issue.fix
                    })
            self._zen_summary["review"]["issues"] = cached_issues
            self._zen_summary["review"]["fixed"] = (
                self._zen_summary["review"]["initial"] > 0 and 
                final_issue_count < self._zen_summary["review"]["initial"]
            )
            logger.debug(f"[Differential Scan] Final issue count from cache: {final_issue_count}")
        else:
            # å¾“æ¥æ–¹å¼ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—ã®å ´åˆã¯å…¨æ¤œå‡º
            final_result = self._detect_comprehensive_issues(all_files, goal)
            if final_result.get('error', False):
                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ä¸æ˜ã¨ã—ã¦è¨˜éŒ²
                self._zen_summary["review"]["final"] = -1
                self._zen_summary["review"]["issues"] = []  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºãƒªã‚¹ãƒˆ
                self._zen_summary["review"]["fixed"] = False
                logger.debug(f"[Comprehensive Review] âš  Final detection failed, marking as unknown")
            else:
                final_issues = final_result.get('issues', [])
                self._zen_summary["review"]["final"] = len(final_issues)
                self._zen_summary["review"]["issues"] = final_issues  # ãƒ•ã‚¡ã‚¤ãƒ«å˜ä½ã®issueæƒ…å ±ã‚’ä¿å­˜
                self._zen_summary["review"]["fixed"] = (
                    self._zen_summary["review"]["initial"] > 0 and 
                    len(final_issues) < self._zen_summary["review"]["initial"]
                )
        
        return all_files
    
    # ============================================================
    # ğŸ†• æ”¹ä¿®: æ¤œå‡º2ãƒ¬ã‚¤ãƒ¤åŒ–
    # ============================================================
    def _detect_blocking_issues(
        self,
        all_files: Dict[str, str],
        goal: str
    ) -> Dict[str, Any]:
        """
        Layer 1: Blocking issues only (runtime errors, data corruption)
        
        æ¤œå‡ºå¯¾è±¡ï¼ˆ10é …ç›®ï¼‰:
        9. Missing await on async calls
        10. Missing db.session.commit()
        11. Boundary conditions not handled
        13. Missing input validation
        14. References to non-existent config
        17. Library-specific API misuse
        18. Circular imports
        19. Security issues
        20. Resource leaks
        21. Type/return value mismatch
        """
        if not self.llm_manager:
            logger.debug("[Blocking Detection] LLM manager not available")
            return {'issues': [], 'error': False}
        
        files_content = ""
        for filepath, content in all_files.items():
            files_content += f"<file path='{filepath}'>\n{content}\n</file>\n\n"
        
        prompt = f"""You are a code quality reviewer. Analyze for BLOCKING issues only.
Focus ONLY on issues that cause runtime errors, data corruption, or security vulnerabilities.

**Original Goal:**
{goal}

**Project Files:**
{files_content}

**Check for these 10 BLOCKING issue types ONLY:**

9. Missing await on async calls, unhandled Promises
   â†’ Causes: Runtime errors, race conditions
   
10. Missing db.session.commit() after database operations
    â†’ Causes: Data not persisted, silent failures
    
11. Boundary conditions not handled (empty array, null, 0, undefined)
    â†’ Causes: TypeError, crashes
    
13. Missing input validation (empty submission causes errors)
    â†’ Causes: Runtime errors, data corruption
    
14. References to non-existent config/environment variables
    â†’ Causes: KeyError, AttributeError
    
17. Library-specific API misuse (Marshmallow: required=True with load_default, etc.)
    â†’ Causes: Runtime errors, unexpected behavior
    
18. Circular imports (ImportError: cannot import name)
    â†’ Causes: Import failures
    
19. Security issues (SQL injection, XSS, plaintext passwords, hardcoded secrets)
    â†’ Causes: Security vulnerabilities, data breaches
    
20. Resource leaks (open() without with, DB connection not closed)
    â†’ Causes: Resource exhaustion
    
21. Type/return value mismatch (declared -> User but returns None)
    â†’ Causes: TypeError at runtime

**DO NOT report:**
- TODO/FIXME comments (not blocking)
- Dead code or unused variables (not blocking)
- Debug code remaining (not blocking)
- Style issues (not blocking)
- UI/UX issues (not blocking unless crash)

**Output Format:**
Return a JSON array. Each issue must have:
- "file": filepath
- "line": line number (or null)
- "type": issue type number (9, 10, 11, 13, 14, 17, 18, 19, 20, or 21 ONLY)
- "description": specific problem
- "fix": suggested fix

If no BLOCKING issues found: return []

Return ONLY the JSON array, no other text."""

        try:
            logger.debug("[Blocking Detection] Scanning for runtime/security issues...")
            
            response = self.llm_manager.generate_response(
                prompt=prompt,
                system_prompt="You are a code quality expert. Detect ONLY blocking issues (runtime errors, security). Return valid JSON array.",
                max_tokens=6000
            )
            
            response_text = response.content if hasattr(response, 'content') else str(response)
            
            # JSONæŠ½å‡º
            json_match = re.search(r'```(?:json)?\s*(\[.*?\])\s*```', response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                json_match = re.search(r'(\[.*\])', response_text, re.DOTALL)
                if json_match:
                    json_str = json_match.group(1)
                else:
                    logger.debug("[Blocking Detection] No JSON found in response")
                    return {'issues': [], 'error': False}
            
            issues = json.loads(json_str)
            
            # Layer 1ã®ã‚¿ã‚¤ãƒ—ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆå¿µã®ãŸã‚ï¼‰
            blocking_types = [9, 10, 11, 13, 14, 17, 18, 19, 20, 21]
            blocking_issues = [
                i for i in issues 
                if i.get('type') in blocking_types
            ]
            
            logger.debug(f"[Blocking Detection] Found {len(blocking_issues)} blocking issue(s)")
            return {'issues': blocking_issues, 'error': False}
            
        except json.JSONDecodeError as e:
            logger.debug(f"[Blocking Detection] JSON parse error: {e}")
            return {'issues': [], 'error': True, 'error_message': f'JSON parse error: {e}'}
        except Exception as e:
            logger.debug(f"[Blocking Detection] Error: {e}")
            return {'issues': [], 'error': True, 'error_message': str(e)}

    def _detect_comprehensive_issues(
        self,
        all_files: Dict[str, str],
        goal: str
    ) -> Dict[str, Any]:
        """
        25é …ç›®ã®å•é¡Œæ¤œå‡º
        
        æ¤œå‡ºé …ç›®:
        1. TODO/FIXME/æœªå®Ÿè£…ã®ã¾ã¾æ®‹ã£ã¦ã„ã‚‹æ©Ÿèƒ½
        2. ã‚¨ãƒ©ãƒ¼å‡¦ç†ãŒä¸å®Œå…¨ï¼ˆcatchãŒç©ºã€console.logã®ã¿ç­‰ï¼‰
        3. å‘¼ã³å‡ºã—å…ƒãŒãªã„é–¢æ•°ï¼ˆãƒ‡ãƒƒãƒ‰ã‚³ãƒ¼ãƒ‰ï¼‰
        4. å®£è¨€ã•ã‚Œã¦ã„ã‚‹ãŒä½¿ã‚ã‚Œã¦ã„ãªã„å¤‰æ•°
        5. UIã§æ“ä½œã§ãã‚‹ãŒå®Ÿéš›ã«ã¯å‹•ä½œã—ãªã„æ©Ÿèƒ½
        6. çŠ¶æ…‹ã®ãƒªã‚»ãƒƒãƒˆ/åˆæœŸåŒ–æ¼ã‚Œï¼ˆçµ‚äº†å¾Œã«åˆæœŸçŠ¶æ…‹ã«æˆ»ã‚‰ãªã„ç­‰ï¼‰
        7. ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜â†’èª­è¾¼ã®ä¸æ•´åˆï¼ˆã‚»ãƒ¼ãƒ–ã§ãã‚‹ãŒãƒ­ãƒ¼ãƒ‰ã§å¾©å…ƒã•ã‚Œãªã„ç­‰ï¼‰
        8. ç”»é¢é·ç§»ã®è¡Œãæ­¢ã¾ã‚Šï¼ˆæˆ»ã‚‹æ‰‹æ®µãŒãªã„ã€æ¬¡ã«é€²ã‚ãªã„ï¼‰
        9. éåŒæœŸå‡¦ç†ã®awaitæ¼ã‚Œã€Promiseæœªå‡¦ç†
        10. DBæ“ä½œå¾Œã®commitæ¼ã‚Œï¼ˆSQLAlchemy: db.session.commit()ï¼‰
        11. ç©ºé…åˆ—/null/0/undefinedã®å¢ƒç•Œæ¡ä»¶ã§è½ã¡ã‚‹å‡¦ç†
        12. å‡¦ç†ãƒ•ãƒ­ãƒ¼ã®æœªå®Œçµï¼ˆé–‹å§‹ã¯ã‚ã‚‹ãŒçµ‚äº†å‡¦ç†ãŒãªã„ï¼‰
        13. å¿…é ˆå…¥åŠ›ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ¼ã‚Œï¼ˆç©ºé€ä¿¡ã§ã‚¨ãƒ©ãƒ¼ï¼‰
        14. å­˜åœ¨ã—ãªã„è¨­å®š/ç’°å¢ƒå¤‰æ•°ã®å‚ç…§
        15. APIå‘¼ã³å‡ºã—çµæœã®æœªãƒã‚§ãƒƒã‚¯ï¼ˆresponse.okç¢ºèªãªã—ç­‰ï¼‰
        16. ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ã®äºŒé‡ç™»éŒ²
        17. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªå›ºæœ‰ã®APIèª¤ç”¨ï¼ˆMarshmallow, SQLAlchemy, Flaskç­‰ï¼‰
        18. å¾ªç’°ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆImportError: cannot import nameï¼‰
        19. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å•é¡Œï¼ˆSQLi, XSS, å¹³æ–‡ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰, ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ç§˜å¯†éµï¼‰
        20. ãƒªã‚½ãƒ¼ã‚¹ãƒªãƒ¼ã‚¯ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«/DBæ¥ç¶š/ã‚½ã‚±ãƒƒãƒˆã®æœªã‚¯ãƒ­ãƒ¼ã‚ºï¼‰
        21. å‹/æˆ»ã‚Šå€¤ã®ä¸ä¸€è‡´ï¼ˆ-> User ã§ return Noneç­‰ï¼‰
        22. OSä¾å­˜/ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ãƒ‘ã‚¹ï¼ˆC:\\, /home/user/, çµ¶å¯¾ãƒ‘ã‚¹ï¼‰
        23. ãƒ‡ãƒãƒƒã‚°ã‚³ãƒ¼ãƒ‰æ®‹å­˜ï¼ˆprint, console.log, debugger, breakpointï¼‰
        24. å¤–éƒ¨ä¾å­˜ã®æœªå‡¦ç†ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæœªè¨­å®š, ãƒªãƒˆãƒ©ã‚¤ãªã—, ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãªã—ï¼‰
        
        Args:
            all_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ä¸€å¼
            goal: å…ƒã®ã‚´ãƒ¼ãƒ«
        
        Returns:
            Dict with keys:
                - 'issues': List of detected issues
                - 'error': bool indicating if detection failed
                - 'error_message': str (only present when error=True)
        """
        if not self.llm_manager:
            logger.debug("[Comprehensive Review] LLM manager not available")
            return {'issues': [], 'error': False}
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’æ•´å½¢
        files_content = ""
        for filepath, content in all_files.items():
            files_content += f"<file path='{filepath}'>\n{content}\n</file>\n\n"
        
        prompt = f"""You are a code quality reviewer. Analyze the following project for issues that would prevent it from working correctly.

**Original Goal:**
{goal}

**Project Files:**
{files_content}

**Check for these 25 issue types:**
1. TODO/FIXME/unimplemented features remaining
2. Incomplete error handling (empty catch, console.log only)
3. Dead code (functions never called)
4. Unused variables
5. UI elements that don't actually work (buttons that do nothing meaningful)
6. State reset/initialization missing (e.g., game over doesn't reset to initial state)
7. Save/Load inconsistency (data saved but not properly restored on load)
8. Dead-end navigation (no way to go back or proceed)
9. Missing await on async calls, unhandled Promises
10. Missing db.session.commit() after database operations
11. Boundary conditions not handled (empty array, null, 0, undefined)
12. Incomplete process flow (start exists but no proper end/cleanup)
13. Missing input validation (empty submission causes errors)
14. References to non-existent config/environment variables
15. API response not checked (no response.ok verification)
16. Event handler double registration
17. Library-specific API misuse (e.g., Marshmallow: required=True with load_default, SQLAlchemy: invalid cascade, Flask: context issues)
18. Circular imports (ImportError: cannot import name from partially initialized module)
19. Security issues (SQL injection with f-strings, XSS with innerHTML/dangerouslySetInnerHTML, plaintext passwords, hardcoded secrets like SECRET_KEY="literal")
20. Resource leaks (open() without with statement, DB connection/cursor not closed, socket not closed)
21. Type/return value mismatch (function declared -> User but returns None, async function not awaited)
22. OS-dependent/hardcoded paths (C:\\, /home/user/, absolute paths, backslash in path strings)
23. Debug code remaining (print(), console.log(), debugger, breakpoint(), pdb.set_trace())
24. External dependency issues (requests without timeout, no error handling for network calls, no retry/fallback)
25. Stub/placeholder files (files with >80% comments and no real code, files containing "not used", "stub", "placeholder", or referencing other files as the "real" implementation)

**Output Format:**
Return a JSON array of issues found. Each issue should have:
- "file": filepath
- "line": line number (if identifiable, otherwise null)
- "type": issue type number (1-25)
- "description": specific description of the problem
- "fix": suggested fix

If no issues found, return an empty array: []

**Important:**
- Focus on issues that would cause runtime errors or broken functionality
- Ignore minor style issues
- Be specific about what's wrong and how to fix it

**CRITICAL EXCLUSIONS (These are NOT issues - Do NOT report):**

### Game/Interactive Application Patterns (NOT debug code):

**UI Elements (type 23 exclusion):**
- HUD displays (score, lives, stage, combo, level, timer, health bar) are INTENTIONAL game UI
- Game state text (GAME OVER, PAUSED, VICTORY, STAGE CLEAR, WARNING!, READY) are INTENTIONAL
- Score popups, combo indicators, floating damage numbers are INTENTIONAL feedback
- Power-up indicators, status effects, cooldown displays are INTENTIONAL
- DO NOT report these as "debug code remaining"

**Intentional Logging (type 23 exclusion):**
- console.error() â†’ ALWAYS acceptable (error handling)
- console.warn() â†’ ALWAYS acceptable (warnings)
- console.log() inside catch/error blocks â†’ acceptable for diagnostics
- ONLY report console.log if it's clearly temporary: console.log('test'), console.log(123), console.log('debug')

**Game Loop Patterns (NOT issues):**
- requestAnimationFrame() recursive calls â†’ standard game loop, NOT infinite loop
- setInterval/setTimeout for game timing â†’ intentional, NOT resource leak
- Canvas rendering loops â†’ required for games
- Particle system update loops â†’ required for effects

**React/Vue/Angular Patterns (NOT unused code - type 3, 4 exclusion):**
- useState variables only appearing in JSX â†’ they ARE used (rendered)
- useEffect hooks â†’ they ARE used (side effects run)
- useRef values â†’ they ARE used (DOM/value persistence)
- Event handler functions â†’ called by framework, NOT dead code

**Event-Driven Architecture (NOT dead code - type 3 exclusion):**
- Functions only called by event listeners (onClick, onKeyDown) â†’ they ARE called at runtime
- Functions only called by setTimeout/setInterval â†’ they ARE called
- Functions only called by requestAnimationFrame â†’ they ARE called
- Callback functions for promises/async â†’ they ARE called

**Intentional Patterns (NOT issues):**
- Empty catch blocks with explicit comment (// intentionally ignored) â†’ acceptable
- Optional chaining (?.) returning undefined â†’ intentional fallback
- Default parameter values â†’ intentional defaults

### Type 23 (Debug Code) Clarification:

**IS debug code (REPORT):**
- console.log('test'), console.log('here'), console.log('debug')
- console.log(variable) with no context (random debugging)
- debugger; statements
- breakpoint() calls

**IS NOT debug code (DO NOT REPORT):**
- console.log('Game started') â†’ user feedback
- console.error('Failed to load:', error) â†’ error handling
- Any logging inside try/catch blocks
- HUD rendering code that displays game state

Return ONLY the JSON array, no other text."""

        import json
        import re
        
        try:
            logger.debug("[Comprehensive Review] Sending detection request to LLM...")
            logger.debug(f"      Analyzing code for issues...")
            
            response = self.llm_manager.generate_response(
                prompt=prompt,
                system_prompt="You are a code quality expert. Detect issues that prevent code from working correctly. Return ONLY valid JSON.",
                max_tokens=8000
            )
            
            logger.debug("[Comprehensive Review] LLM detection response received")
            
            if hasattr(response, 'content'):
                response_text = response.content
            else:
                response_text = str(response)
            
            # JSONæŠ½å‡º
            # JSONãƒ–ãƒ­ãƒƒã‚¯ã‚’æ¢ã™
            json_match = re.search(r'```(?:json)?\s*(\[.*?\])\s*```', response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # ```ãªã—ã®å ´åˆã€é…åˆ—ã‚’ç›´æ¥æ¢ã™
                json_match = re.search(r'\[.*\]', response_text, re.DOTALL)
                if json_match:
                    json_str = json_match.group(0)
                else:
                    logger.debug("[Comprehensive Review] No JSON array found in response")
                    return {'issues': [], 'error': False}
            
            issues = json.loads(json_str)
            
            if not isinstance(issues, list):
                logger.debug("[Comprehensive Review] Response is not a list")
                return {'issues': [], 'error': False}
            
            logger.debug(f"[Comprehensive Review] Detected {len(issues)} issue(s)")
            return {'issues': issues, 'error': False}
            
        except json.JSONDecodeError as e:
            logger.debug(f"[Comprehensive Review] JSON parse error: {e}")
            return {'issues': [], 'error': True, 'error_message': f'JSON parse error: {e}'}
        except Exception as e:
            logger.debug(f"[Comprehensive Review] Detection error: {e}")
            return {'issues': [], 'error': True, 'error_message': str(e)}
    
    # ============================================================
    # å·®åˆ†ã‚¹ã‚­ãƒ£ãƒ³: Issueã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†
    # ============================================================
    
    def _get_content_hash(self, content: str, line_start: int, line_end: int) -> str:
        """
        æŒ‡å®šè¡Œç¯„å›²ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—
        
        å¤‰æ›´æ¤œçŸ¥ç”¨ã€‚åŒã˜ãƒãƒƒã‚·ãƒ¥ = åŒã˜ã‚³ãƒ¼ãƒ‰ = ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹ã€‚
        
        Args:
            content: ãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã®å†…å®¹
            line_start: é–‹å§‹è¡Œï¼ˆ1-indexedï¼‰
            line_end: çµ‚äº†è¡Œï¼ˆ1-indexedã€inclusiveï¼‰
            
        Returns:
            SHA256ãƒãƒƒã‚·ãƒ¥ã®å…ˆé ­16æ–‡å­—
        """
        import hashlib
        
        lines = content.split('\n')
        # 1-indexed â†’ 0-indexedå¤‰æ›ã€ç¯„å›²å¤–å¯¾ç­–
        start_idx = max(0, line_start - 1)
        end_idx = min(len(lines), line_end)
        
        target_lines = lines[start_idx:end_idx]
        target_content = '\n'.join(target_lines)
        
        return hashlib.sha256(target_content.encode('utf-8')).hexdigest()[:16]
    
    def _cache_issues(
        self,
        issues: List[Dict[str, Any]],
        all_files: Dict[str, str]
    ) -> Dict[str, List[CachedIssue]]:
        """
        Issueæ¤œå‡ºçµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        
        å„Issueã«è©²å½“è¡Œã®content_hashã‚’ä»˜ä¸ã—ã€ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°ã€‚
        
        Args:
            issues: _detect_comprehensive_issuesã®æˆ»ã‚Šå€¤['issues']
            all_files: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹â†’å†…å®¹ã®ãƒãƒƒãƒ—
            
        Returns:
            Dict[str, List[CachedIssue]]: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹â†’CachedIssueãƒªã‚¹ãƒˆ
        """
        cache: Dict[str, List[CachedIssue]] = {}
        
        for issue in issues:
            filepath = issue.get('file', '')
            line = issue.get('line')
            issue_type = issue.get('type', 0)
            description = issue.get('description', '')
            fix = issue.get('fix', '')
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’å–å¾—
            content = all_files.get(filepath, '')
            
            # è¡Œç•ªå·ãŒã‚ã‚‹å ´åˆã€ãã®å‘¨è¾ºï¼ˆÂ±2è¡Œï¼‰ã®ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—
            # è¡Œç•ªå·ãŒãªã„å ´åˆã€ãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã®ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—
            if line is not None and isinstance(line, int) and line > 0:
                line_start = max(1, line - 2)
                line_end = line + 2
                content_hash = self._get_content_hash(content, line_start, line_end)
            else:
                # ãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã®ãƒãƒƒã‚·ãƒ¥
                import hashlib
                content_hash = hashlib.sha256(content.encode('utf-8')).hexdigest()[:16]
            
            # CachedIssueã‚’ä½œæˆ
            cached = CachedIssue(
                file=filepath,
                line=line if isinstance(line, int) else None,
                line_end=None,  # å˜ä¸€è¡Œã®Issueã®å ´åˆ
                issue_type=issue_type if isinstance(issue_type, int) else 0,
                description=description,
                fix=fix,
                content_hash=content_hash
            )
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°
            if filepath not in cache:
                cache[filepath] = []
            cache[filepath].append(cached)
        
        logger.debug(f"[Differential Scan] Cached {len(issues)} issue(s) across {len(cache)} file(s)")
        
        return cache
    
    def _clear_issue_cache(self):
        """
        Issueã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
        
        æ–°ã—ã„ä¿®æ­£ã‚µã‚¤ã‚¯ãƒ«ã®é–‹å§‹æ™‚ã‚„ã€å¤§å¹…ãªå¤‰æ›´ãŒã‚ã£ãŸå ´åˆã«å‘¼ã³å‡ºã™ã€‚
        """
        self._issue_cache = None
        logger.debug("[Differential Scan] Issue cache cleared")
    
    def _is_line_in_modified_range(
        self,
        line: Optional[int],
        modified_ranges: List[ModifiedRange],
        filepath: str,
        margin: int = 3  # ğŸ”§ å°‚é–€å®¶æ¨å¥¨: 5â†’3è¡Œã«ç¸®å°ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡å‘ä¸Šï¼‰
    ) -> bool:
        """
        æŒ‡å®šè¡ŒãŒå¤‰æ›´ç¯„å›²å†…ã‹ã©ã†ã‹ã‚’åˆ¤å®š
        
        Args:
            line: åˆ¤å®šå¯¾è±¡ã®è¡Œç•ªå·ï¼ˆNoneã®å ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“â†’å¸¸ã«Trueï¼‰
            modified_ranges: å¤‰æ›´ç¯„å›²ãƒªã‚¹ãƒˆ
            filepath: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            margin: å‰å¾Œã®ãƒãƒ¼ã‚¸ãƒ³ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ3è¡Œï¼‰
            
        Returns:
            True: å¤‰æ›´ç¯„å›²å†…ï¼ˆå†æ¤œå‡ºãŒå¿…è¦ï¼‰
            False: å¤‰æ›´ç¯„å›²å¤–ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨å¯èƒ½ï¼‰
        """
        # è¡Œç•ªå·ãŒãªã„å ´åˆã€ãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã®Issue â†’ å†æ¤œå‡ºå¿…è¦
        if line is None:
            return True
        
        for mr in modified_ranges:
            if mr.file != filepath:
                continue
            
            # ãƒãƒ¼ã‚¸ãƒ³ã‚’å«ã‚ãŸç¯„å›²åˆ¤å®š
            range_start = max(1, mr.start_line - margin)
            range_end = mr.end_line + margin
            
            if range_start <= line <= range_end:
                return True
        
        return False
    
    def _extract_modified_ranges(
        self,
        diff_content: str,
        filepath: str
    ) -> List[ModifiedRange]:
        """
        unified diffã‹ã‚‰å¤‰æ›´ç¯„å›²ã‚’æŠ½å‡º
        
        DESIGN CHANGE v2.1ãŒå‡ºåŠ›ã™ã‚‹unified diffå½¢å¼ã‚’è§£æã—ã€
        å¤‰æ›´ã•ã‚ŒãŸè¡Œç¯„å›²ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™ã€‚
        
        Args:
            diff_content: unified diffå½¢å¼ã®å·®åˆ†
            filepath: å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            
        Returns:
            List[ModifiedRange]: å¤‰æ›´ç¯„å›²ã®ãƒªã‚¹ãƒˆ
            
        Example:
            å…¥åŠ›:
            @@ -20,6 +20,8 @@
             def main():
            +    if not config.API_KEY:
            +        raise ValueError("API_KEY required")
                 app.run()
            
            å‡ºåŠ›:
            [ModifiedRange(file="main.py", start_line=20, end_line=27, diff_content="...")]
        """
        import re
        
        modified_ranges: List[ModifiedRange] = []
        
        # ãƒãƒ³ã‚¯ãƒ˜ãƒƒãƒ€ãƒ¼ã®ãƒ‘ã‚¿ãƒ¼ãƒ³: @@ -old_start,old_count +new_start,new_count @@
        hunk_pattern = r'@@ -(\d+)(?:,(\d+))? \+(\d+)(?:,(\d+))? @@'
        
        diff_lines = diff_content.split('\n')
        i = 0
        
        while i < len(diff_lines):
            line = diff_lines[i]
            
            # ãƒãƒ³ã‚¯ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ¤œå‡º
            hunk_match = re.match(hunk_pattern, line)
            if hunk_match:
                # æ–°ãƒ•ã‚¡ã‚¤ãƒ«å´ã®é–‹å§‹è¡Œã¨è¡Œæ•°ã‚’å–å¾—
                new_start = int(hunk_match.group(3))
                new_count = int(hunk_match.group(4)) if hunk_match.group(4) else 1
                
                # ãƒãƒ³ã‚¯å†…å®¹ã‚’åé›†
                hunk_content_lines = [line]  # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚‚å«ã‚ã‚‹
                i += 1
                
                # è¿½åŠ è¡Œæ•°ã¨å‰Šé™¤è¡Œæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                added_lines = 0
                deleted_lines = 0
                
                while i < len(diff_lines) and not diff_lines[i].startswith('@@'):
                    current_line = diff_lines[i]
                    hunk_content_lines.append(current_line)
                    
                    if current_line.startswith('+') and not current_line.startswith('+++'):
                        added_lines += 1
                    elif current_line.startswith('-') and not current_line.startswith('---'):
                        deleted_lines += 1
                    
                    i += 1
                
                # å¤‰æ›´ç¯„å›²ã‚’è¨ˆç®—ï¼ˆæ–°ãƒ•ã‚¡ã‚¤ãƒ«åŸºæº–ï¼‰
                # è¿½åŠ è¡ŒãŒã‚ã‚‹å ´åˆã€ãã®ç¯„å›²ã‚’å«ã‚ã‚‹
                actual_end = new_start + new_count - 1 + added_lines
                
                modified_range = ModifiedRange(
                    file=filepath,
                    start_line=new_start,
                    end_line=actual_end,
                    diff_content='\n'.join(hunk_content_lines)
                )
                modified_ranges.append(modified_range)
                
                logger.debug(f"[Differential Scan] Extracted range: {filepath} L{new_start}-L{actual_end}")
            else:
                i += 1
        
        return modified_ranges
    
    def _detect_issues_for_ranges(
        self,
        modified_ranges: List[ModifiedRange],
        all_files: Dict[str, str],
        goal: str,
        margin: int = 3  # ğŸ”§ Â±3è¡Œã§çµ±ä¸€
    ) -> List[Dict[str, Any]]:
        """
        å¤‰æ›´ç¯„å›²ã®ã¿ã‚’LLMã§æ¤œå‡º
        
        å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†æ¤œå‡ºã™ã‚‹ã®ã§ã¯ãªãã€å¤‰æ›´ã•ã‚ŒãŸç¯„å›²ï¼ˆÂ±marginè¡Œï¼‰
        ã®ã¿ã‚’å¯¾è±¡ã«Issueæ¤œå‡ºã‚’è¡Œã†ã€‚ãƒˆãƒ¼ã‚¯ãƒ³æ¶ˆè²»ã‚’å‰Šæ¸›ã—ã€
        ã€ŒãŠåŒ–ã‘Issueã€ã‚’é˜²ãã€‚
        
        Args:
            modified_ranges: å¤‰æ›´ç¯„å›²ãƒªã‚¹ãƒˆ
            all_files: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹â†’å†…å®¹ã®ãƒãƒƒãƒ—
            goal: å…ƒã®ã‚´ãƒ¼ãƒ«
            margin: å‰å¾Œã®ãƒãƒ¼ã‚¸ãƒ³ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ3è¡Œï¼‰
            
        Returns:
            æ¤œå‡ºã•ã‚ŒãŸIssueã®ãƒªã‚¹ãƒˆ
        """
        import json
        import re
        
        if not modified_ranges:
            logger.debug("[Differential Scan] No modified ranges to scan")
            return []
        
        if not self.llm_manager:
            logger.debug("[Differential Scan] LLM manager not available")
            return []
        
        # å¤‰æ›´ç¯„å›²ã”ã¨ã«ã‚³ãƒ¼ãƒ‰ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’åé›†
        snippets_content = ""
        for mr in modified_ranges:
            content = all_files.get(mr.file, '')
            if not content:
                continue
            
            lines = content.split('\n')
            
            # ãƒãƒ¼ã‚¸ãƒ³ã‚’å«ã‚ãŸç¯„å›²ã‚’æŠ½å‡º
            start_idx = max(0, mr.start_line - 1 - margin)
            end_idx = min(len(lines), mr.end_line + margin)
            
            snippet_lines = lines[start_idx:end_idx]
            snippet = '\n'.join(f"{start_idx + i + 1}: {line}" for i, line in enumerate(snippet_lines))
            
            snippets_content += f"""
<modified_section file="{mr.file}" lines="{mr.start_line}-{mr.end_line}">
{snippet}
</modified_section>

"""
        
        prompt = f"""You are a code quality reviewer. Analyze ONLY the modified sections below for issues.

**IMPORTANT**: You are reviewing ONLY the changed code sections, not the entire project.
Focus on issues that could have been introduced or affected by these changes.

**Original Goal:**
{goal}

**Modified Sections:**
{snippets_content}

**Check for these issue types in the modified sections:**
1. TODO/FIXME/unimplemented features
2. Incomplete error handling
3. Unused variables (in the modified section)
4. Missing await on async calls
5. Boundary conditions not handled
6. Missing input validation
7. API response not checked
8. Type/return value mismatch
9. Security issues (SQL injection, XSS, hardcoded secrets)
10. Resource leaks

**Output Format:**
Return a JSON array of issues found. Each issue should have:
- "file": filepath
- "line": line number (must be within the modified range)
- "type": issue type number
- "description": specific description
- "fix": suggested fix

If no issues found in the modified sections, return an empty array: []

Return ONLY the JSON array, no other text."""

        try:
            logger.debug(f"[Differential Scan] Scanning {len(modified_ranges)} modified range(s)...")
            
            response = self.llm_manager.generate_response(
                prompt=prompt,
                system_prompt="You are a code quality expert. Detect issues ONLY in the modified sections. Return ONLY valid JSON.",
                max_tokens=4000  # ç¯„å›²é™å®šãªã®ã§ãƒˆãƒ¼ã‚¯ãƒ³å‰Šæ¸›
            )
            
            if hasattr(response, 'content'):
                response_text = response.content
            else:
                response_text = str(response)
            
            # JSONæŠ½å‡º
            json_match = re.search(r'```(?:json)?\s*(\[.*?\])\s*```', response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                json_match = re.search(r'\[.*\]', response_text, re.DOTALL)
                if json_match:
                    json_str = json_match.group(0)
                else:
                    logger.debug("[Differential Scan] No JSON array found in response")
                    return []
            
            issues = json.loads(json_str)
            
            if not isinstance(issues, list):
                logger.debug("[Differential Scan] Response is not a list")
                return []
            
            # å¤‰æ›´ç¯„å›²å¤–ã®Issueã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆLLMãŒç¯„å›²å¤–ã‚’å ±å‘Šã—ãŸå ´åˆï¼‰
            filtered_issues = []
            for issue in issues:
                issue_file = issue.get('file', '')
                issue_line = issue.get('line')
                
                # è¡Œç•ªå·ãŒã‚ã‚‹å ´åˆã€å¤‰æ›´ç¯„å›²å†…ã‹ãƒã‚§ãƒƒã‚¯
                if issue_line is not None:
                    in_range = False
                    for mr in modified_ranges:
                        if mr.file == issue_file:
                            if mr.start_line - margin <= issue_line <= mr.end_line + margin:
                                in_range = True
                                break
                    
                    if not in_range:
                        logger.debug(f"[Differential Scan] Filtered out-of-range issue: {issue_file}:{issue_line}")
                        continue
                
                filtered_issues.append(issue)
            
            logger.debug(f"[Differential Scan] Detected {len(filtered_issues)} issue(s) in modified ranges")
            return filtered_issues
            
        except json.JSONDecodeError as e:
            logger.debug(f"[Differential Scan] JSON parse error: {e}")
            return []
        except Exception as e:
            logger.debug(f"[Differential Scan] Detection error: {e}")
            return []
    
    def _merge_issues(
        self,
        cache: Dict[str, List[CachedIssue]],
        new_issues: List[Dict[str, Any]],
        modified_ranges: List[ModifiedRange],
        all_files: Dict[str, str],
        margin: int = 3  # ğŸ”§ Â±3è¡Œã§çµ±ä¸€
    ) -> Tuple[List[Dict[str, Any]], Dict[str, List[CachedIssue]]]:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨æ–°è¦æ¤œå‡ºçµæœã‚’ãƒãƒ¼ã‚¸
        
        ãƒ«ãƒ¼ãƒ«:
        1. å¤‰æ›´ç¯„å›²å¤–ã®Issue â†’ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ï¼ˆcontent_hashç…§åˆï¼‰
        2. å¤‰æ›´ç¯„å›²å†…ã®Issue â†’ æ–°è¦æ¤œå‡ºçµæœã‚’ä½¿ç”¨
        3. é‡è¤‡æ’é™¤ï¼ˆåŒä¸€è¡Œã®åŒä¸€Issueç¨®åˆ¥ï¼‰
        
        Args:
            cache: æ—¢å­˜ã®Issueã‚­ãƒ£ãƒƒã‚·ãƒ¥
            new_issues: _detect_issues_for_ranges()ã®çµæœ
            modified_ranges: å¤‰æ›´ç¯„å›²ãƒªã‚¹ãƒˆ
            all_files: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹â†’å†…å®¹ã®ãƒãƒƒãƒ—
            margin: ãƒãƒ¼ã‚¸ãƒ³ï¼ˆ_is_line_in_modified_rangeã§ä½¿ç”¨ï¼‰â˜…3è¡Œã«çµ±ä¸€
            
        Returns:
            Tuple[List[Dict], Dict[str, List[CachedIssue]]]:
                - ãƒãƒ¼ã‚¸å¾Œã®Issueãƒªã‚¹ãƒˆ
                - æ›´æ–°å¾Œã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        """
        merged_issues: List[Dict[str, Any]] = []
        updated_cache: Dict[str, List[CachedIssue]] = {}
        
        # å¤‰æ›´ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚»ãƒƒãƒˆ
        modified_files = set(mr.file for mr in modified_ranges)
        
        # 1. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰Issueã‚’å¾©å…ƒï¼ˆå¤‰æ›´ç¯„å›²å¤–ã®ã¿ï¼‰
        for filepath, cached_issues in cache.items():
            updated_cache[filepath] = []
            
            for cached in cached_issues:
                # ã“ã®IssueãŒå¤‰æ›´ç¯„å›²å†…ã‹ãƒã‚§ãƒƒã‚¯
                in_modified_range = self._is_line_in_modified_range(
                    cached.line,
                    modified_ranges,
                    filepath,
                    margin
                )
                
                if in_modified_range:
                    # å¤‰æ›´ç¯„å›²å†… â†’ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰é™¤å¤–ï¼ˆæ–°è¦æ¤œå‡ºçµæœã‚’ä½¿ã†ï¼‰
                    logger.debug(f"[Differential Scan] Invalidated cached issue: {filepath}:{cached.line}")
                    continue
                
                # å¤‰æ›´ç¯„å›²å¤– â†’ content_hashã‚’ç…§åˆ
                content = all_files.get(filepath, '')
                if cached.line is not None:
                    current_hash = self._get_content_hash(
                        content,
                        max(1, cached.line - 2),
                        cached.line + 2
                    )
                else:
                    import hashlib
                    current_hash = hashlib.sha256(content.encode('utf-8')).hexdigest()[:16]
                
                if current_hash == cached.content_hash:
                    # ãƒãƒƒã‚·ãƒ¥ä¸€è‡´ â†’ ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹ã€Issueã‚’å¾©å…ƒ
                    merged_issues.append({
                        'file': cached.file,
                        'line': cached.line,
                        'type': cached.issue_type,
                        'description': cached.description,
                        'fix': cached.fix
                    })
                    updated_cache[filepath].append(cached)
                else:
                    # ãƒãƒƒã‚·ãƒ¥ä¸ä¸€è‡´ â†’ ã‚³ãƒ¼ãƒ‰ãŒå¤‰æ›´ã•ã‚ŒãŸã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹
                    logger.debug(f"[Differential Scan] Hash mismatch, invalidated: {filepath}:{cached.line}")
        
        # 2. æ–°è¦æ¤œå‡ºçµæœã‚’è¿½åŠ 
        for issue in new_issues:
            filepath = issue.get('file', '')
            line = issue.get('line')
            issue_type = issue.get('type', 0)
            description = issue.get('description', '')
            fix = issue.get('fix', '')
            
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆåŒä¸€ãƒ•ã‚¡ã‚¤ãƒ«ãƒ»åŒä¸€è¡Œãƒ»åŒä¸€ç¨®åˆ¥ï¼‰
            is_duplicate = False
            for existing in merged_issues:
                if (existing.get('file') == filepath and
                    existing.get('line') == line and
                    existing.get('type') == issue_type):
                    is_duplicate = True
                    break
            
            if is_duplicate:
                logger.debug(f"[Differential Scan] Skipped duplicate issue: {filepath}:{line}")
                continue
            
            # Issueã‚’è¿½åŠ 
            merged_issues.append(issue)
            
            # æ–°è¦Issueã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«è¿½åŠ 
            content = all_files.get(filepath, '')
            if line is not None and isinstance(line, int) and line > 0:
                content_hash = self._get_content_hash(content, max(1, line - 2), line + 2)
            else:
                import hashlib
                content_hash = hashlib.sha256(content.encode('utf-8')).hexdigest()[:16]
            
            cached_issue = CachedIssue(
                file=filepath,
                line=line if isinstance(line, int) else None,
                line_end=None,
                issue_type=issue_type if isinstance(issue_type, int) else 0,
                description=description,
                fix=fix,
                content_hash=content_hash
            )
            
            if filepath not in updated_cache:
                updated_cache[filepath] = []
            updated_cache[filepath].append(cached_issue)
        
        # ç©ºã®ãƒªã‚¹ãƒˆã‚’æŒã¤ã‚­ãƒ¼ã‚’å‰Šé™¤
        updated_cache = {k: v for k, v in updated_cache.items() if v}
        
        logger.debug(f"[Differential Scan] Merged: {len(merged_issues)} total issues "
                    f"(cached: {sum(len(v) for v in cache.values())}, new: {len(new_issues)})")
        
        return merged_issues, updated_cache
    
    def _detect_modified_ranges_from_files(
        self,
        original_files: Dict[str, str],
        modified_files: Dict[str, str]
    ) -> List[ModifiedRange]:
        """
        ä¿®æ­£å‰å¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¯”è¼ƒã—ã¦å¤‰æ›´ç¯„å›²ã‚’æ¤œå‡º
        
        unified diffã‚’ä½¿ã‚ãšã«ã€ä¿®æ­£å‰å¾Œã®ã‚³ãƒ¼ãƒ‰ã‚’ç›´æ¥æ¯”è¼ƒã—ã¦
        å¤‰æ›´ã•ã‚ŒãŸè¡Œç¯„å›²ã‚’ç‰¹å®šã™ã‚‹ã€‚
        
        Args:
            original_files: ä¿®æ­£å‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒãƒ—
            modified_files: ä¿®æ­£å¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒãƒ—
            
        Returns:
            List[ModifiedRange]: æ¤œå‡ºã•ã‚ŒãŸå¤‰æ›´ç¯„å›²
        """
        import difflib
        
        modified_ranges: List[ModifiedRange] = []
        
        for filepath in modified_files:
            original = original_files.get(filepath, '')
            modified = modified_files.get(filepath, '')
            
            if original == modified:
                continue  # å¤‰æ›´ãªã—
            
            original_lines = original.split('\n')
            modified_lines = modified.split('\n')
            
            # difflib.SequenceMatcher ã§å¤‰æ›´ç®‡æ‰€ã‚’æ¤œå‡º
            matcher = difflib.SequenceMatcher(None, original_lines, modified_lines)
            
            for tag, i1, i2, j1, j2 in matcher.get_opcodes():
                if tag == 'equal':
                    continue
                
                # replace, insert, delete ã„ãšã‚Œã‚‚å¤‰æ›´ã¨ã—ã¦æ‰±ã†
                # æ–°ãƒ•ã‚¡ã‚¤ãƒ«å´ã®è¡Œç•ªå·ã‚’ä½¿ç”¨ï¼ˆ1-indexedï¼‰
                start_line = j1 + 1
                end_line = max(j2, j1 + 1)  # ç©ºç¯„å›²ã‚’é˜²ã
                
                # å¤‰æ›´å†…å®¹ã‚’æŠ½å‡º
                if tag == 'replace':
                    diff_content = f"Replaced lines {i1+1}-{i2} with lines {j1+1}-{j2}"
                elif tag == 'insert':
                    diff_content = f"Inserted lines {j1+1}-{j2}"
                else:  # delete
                    diff_content = f"Deleted lines {i1+1}-{i2}"
                    # å‰Šé™¤ã®å ´åˆã€å‰Šé™¤ä½ç½®ã®å‘¨è¾ºã‚’å½±éŸ¿ç¯„å›²ã¨ã™ã‚‹
                    start_line = max(1, j1)
                    end_line = max(1, j1 + 1)
                
                modified_range = ModifiedRange(
                    file=filepath,
                    start_line=start_line,
                    end_line=end_line,
                    diff_content=diff_content
                )
                modified_ranges.append(modified_range)
                
                logger.debug(f"[Differential Scan] Detected change: {filepath} L{start_line}-L{end_line} ({tag})")
        
        # éš£æ¥ã™ã‚‹ç¯„å›²ã‚’ãƒãƒ¼ã‚¸ï¼ˆ10è¡Œä»¥å†…ãªã‚‰çµåˆï¼‰
        merged = self._merge_adjacent_ranges(modified_ranges)
        
        return merged
    
    def _merge_adjacent_ranges(
        self,
        ranges: List[ModifiedRange],
        gap_threshold: int = 10
    ) -> List[ModifiedRange]:
        """
        éš£æ¥ã™ã‚‹å¤‰æ›´ç¯„å›²ã‚’ãƒãƒ¼ã‚¸
        
        Args:
            ranges: å¤‰æ›´ç¯„å›²ãƒªã‚¹ãƒˆ
            gap_threshold: ã“ã®è¡Œæ•°ä»¥ä¸‹ã®ã‚®ãƒ£ãƒƒãƒ—ãªã‚‰çµåˆ
            
        Returns:
            ãƒãƒ¼ã‚¸å¾Œã®å¤‰æ›´ç¯„å›²ãƒªã‚¹ãƒˆ
        """
        if not ranges:
            return []
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°
        by_file: Dict[str, List[ModifiedRange]] = {}
        for r in ranges:
            if r.file not in by_file:
                by_file[r.file] = []
            by_file[r.file].append(r)
        
        merged: List[ModifiedRange] = []
        
        for filepath, file_ranges in by_file.items():
            # é–‹å§‹è¡Œã§ã‚½ãƒ¼ãƒˆ
            sorted_ranges = sorted(file_ranges, key=lambda r: r.start_line)
            
            current = sorted_ranges[0]
            
            for next_range in sorted_ranges[1:]:
                # éš£æ¥ã¾ãŸã¯é‡è¤‡ã—ã¦ã„ã‚‹å ´åˆã¯ãƒãƒ¼ã‚¸
                if next_range.start_line <= current.end_line + gap_threshold:
                    # ç¯„å›²ã‚’æ‹¡å¼µ
                    current = ModifiedRange(
                        file=filepath,
                        start_line=current.start_line,
                        end_line=max(current.end_line, next_range.end_line),
                        diff_content=current.diff_content + "\n" + next_range.diff_content
                    )
                else:
                    merged.append(current)
                    current = next_range
            
            merged.append(current)
        
        return merged
    
    # ============================================================
    # FixAttemptTracker: Track fix attempts per file
    # Max 2 attempts per file, 3rd attempt forces design change
    # ============================================================
    class FixAttemptTracker:
        """Track fix attempts per file to prevent Whack-a-mole problem"""
        
        def __init__(self, max_attempts: int = 2):
            self.attempts_per_file: Dict[str, int] = {}
            self.max_attempts = max_attempts
            self.issue_history: Dict[str, List[str]] = {}
        
        def record_attempt(self, filepath: str, issue_ids: List[str]):
            """Record a fix attempt"""
            self.attempts_per_file[filepath] = self.attempts_per_file.get(filepath, 0) + 1
            if filepath not in self.issue_history:
                self.issue_history[filepath] = []
            self.issue_history[filepath].extend(issue_ids)
        
        def get_attempt_count(self, filepath: str) -> int:
            """Get current fix attempt count"""
            return self.attempts_per_file.get(filepath, 0)
        
        def needs_design_change(self, filepath: str) -> bool:
            """If fixed 2+ times but issues remain, design change is required"""
            return self.get_attempt_count(filepath) >= self.max_attempts
        
        def get_previous_issues(self, filepath: str) -> List[str]:
            """Get previous issue IDs"""
            return self.issue_history.get(filepath, [])
    
    # ============================================
    # ğŸ†• ä¾å­˜é–¢ä¿‚åˆ†æï¼ˆCross-File Dependency Analysisï¼‰
    # ç ”ç©¶æ ¹æ‹ : AlphaCodium (arXiv:2401.08500), Aider Benchmarks
    # ç›®çš„: ãƒ¢ã‚°ãƒ©å©ãç¾è±¡ã‚’è»½æ¸›ã™ã‚‹ãŸã‚ã€ãƒ•ã‚¡ã‚¤ãƒ«ä¿®æ­£æ™‚ã«
    #       ä»–ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã®ä¾å­˜é–¢ä¿‚æƒ…å ±ã‚’LLMã«æä¾›ã™ã‚‹
    # ============================================
    
    def _extract_function_snippet(self, content: str, func_name: str) -> str:
        """
        é–¢æ•°ã®æœ€åˆã®æ•°è¡Œã‚’æŠ½å‡ºï¼ˆã‚·ã‚°ãƒãƒãƒ£+æ¦‚è¦ï¼‰
        
        Args:
            content: ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹
            func_name: é–¢æ•°å
        
        Returns:
            é–¢æ•°ã®ã‚¹ãƒ‹ãƒšãƒƒãƒˆï¼ˆæœ€å¤§200æ–‡å­—ï¼‰
        """
        import re
        
        if not content or not func_name:
            return ""
        
        # JavaScript: function funcName(...) { ... }
        js_pattern = rf"((?:async\s+)?function\s+{re.escape(func_name)}\s*\([^)]*\)\s*\{{)"
        match = re.search(js_pattern, content)
        if match:
            start = match.start()
            snippet = content[start:start + 200]
            last_newline = snippet.rfind('\n')
            if last_newline > 50:
                snippet = snippet[:last_newline]
            return snippet.strip()
        
        # JavaScript: const funcName = (...) => { ... }
        js_arrow_pattern = rf"((?:const|let|var)\s+{re.escape(func_name)}\s*=\s*(?:async\s*)?\([^)]*\)\s*=>)"
        match = re.search(js_arrow_pattern, content)
        if match:
            start = match.start()
            snippet = content[start:start + 200]
            last_newline = snippet.rfind('\n')
            if last_newline > 50:
                snippet = snippet[:last_newline]
            return snippet.strip()
        
        # JavaScript: ES6 class method - methodName(...) {
        js_class_pattern = rf"((?:async\s+)?{re.escape(func_name)}\s*\([^)]*\)\s*\{{)"
        match = re.search(js_class_pattern, content)
        if match:
            start = match.start()
            snippet = content[start:start + 200]
            last_newline = snippet.rfind('\n')
            if last_newline > 50:
                snippet = snippet[:last_newline]
            return snippet.strip()
        
        # Python: def func_name(...):
        py_pattern = rf"((?:async\s+)?def\s+{re.escape(func_name)}\s*\([^)]*\):)"
        match = re.search(py_pattern, content)
        if match:
            start = match.start()
            snippet = content[start:start + 200]
            last_newline = snippet.rfind('\n')
            if last_newline > 50:
                snippet = snippet[:last_newline]
            return snippet.strip()
        
        return ""
    
    def _analyze_file_dependencies(
        self,
        all_files: Dict[str, str]
    ) -> Dict[str, Dict[str, Any]]:
        """
        å…¨ãƒ•ã‚¡ã‚¤ãƒ«é–“ã®ä¾å­˜é–¢ä¿‚ã‚’åˆ†æ
        
        ç ”ç©¶æ ¹æ‹ :
        - AlphaCodium: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ4000ãƒˆãƒ¼ã‚¯ãƒ³è¶…ã§ç²¾åº¦ä½ä¸‹
        - Aider: é–¢ä¿‚ãªã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ ã™ã‚‹ã¨LLMãŒæ··ä¹±
        
        ãã®ãŸã‚ã€ãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã§ã¯ãªãã€Œé–¢é€£éƒ¨åˆ†ã®ã¿ã€ã‚’æŠ½å‡ºã™ã‚‹ãŸã‚ã®
        ä¾å­˜é–¢ä¿‚æƒ…å ±ã‚’åé›†ã™ã‚‹ã€‚
        
        Args:
            all_files: å…¨ãƒ•ã‚¡ã‚¤ãƒ« {filepath: content}
        
        Returns:
            Dict[filepath, DependencyInfo]: å„ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¾å­˜é–¢ä¿‚æƒ…å ±
            DependencyInfo = {
                'shared_dom': Dict[str, List[str]],       # DOM ID â†’ ä»–ã®ä½¿ç”¨ãƒ•ã‚¡ã‚¤ãƒ«
                'duplicate_functions': Dict[str, List[str]], # é–¢æ•°å â†’ ä»–ã®å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«
                'related_files': List[str]                # é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ
            }
        """
        import re
        
        # ========================================
        # Step 1: å„ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰DOMå‚ç…§ã‚’æŠ½å‡º
        # ========================================
        dom_refs_by_file: Dict[str, Set[str]] = {}
        
        dom_patterns = [
            r"getElementById\s*\(\s*['\"]([^'\"]+)['\"]\s*\)",
            r"getElementById\s*\(\s*`([^`$]+)`\s*\)",  # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒªãƒ†ãƒ©ãƒ«ï¼ˆå¤‰æ•°ãªã—ï¼‰
            r"querySelector\s*\(\s*['\"]#([^'\"]+)['\"]\s*\)",
        ]
        
        for filepath, content in all_files.items():
            if not any(filepath.endswith(ext) for ext in ['.js', '.html', '.htm', '.jsx', '.tsx']):
                continue
            
            refs = set()
            for pattern in dom_patterns:
                refs.update(re.findall(pattern, content))
            
            if refs:
                dom_refs_by_file[filepath] = refs
        
        # ========================================
        # Step 2: å„ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰é–¢æ•°å®šç¾©ã‚’æŠ½å‡º
        # ========================================
        func_defs_by_file: Dict[str, Set[str]] = {}
        
        js_func_patterns = [
            r"(?:async\s+)?function\s+(\w+)\s*\(",
            r"(?:const|let|var)\s+(\w+)\s*=\s*(?:async\s*)?\([^)]*\)\s*=>",
            r"(?:const|let|var)\s+(\w+)\s*=\s*(?:async\s+)?function",
            # ğŸ†• ES6ã‚¯ãƒ©ã‚¹ãƒ¡ã‚½ãƒƒãƒ‰: methodName(...) { ã¾ãŸã¯ async methodName(...) {
            r"^\s+(?:async\s+)?(\w+)\s*\([^)]*\)\s*\{",
        ]
        
        py_func_patterns = [
            r"(?:async\s+)?def\s+(\w+)\s*\(",
        ]
        
        # é™¤å¤–ã™ã‚‹ä¸€èˆ¬çš„ãªåå‰ï¼ˆèª¤æ¤œå‡ºé˜²æ­¢ï¼‰- æ‹¡å¼µç‰ˆ
        common_names = {
            # ğŸ”´ JavaScript/Pythonäºˆç´„èªï¼ˆforãƒ«ãƒ¼ãƒ—ç­‰ã®èª¤æ¤œå‡ºé˜²æ­¢ï¼‰
            'for', 'if', 'else', 'while', 'switch', 'case', 'default',
            'try', 'catch', 'finally', 'throw', 'with', 'do',
            'return', 'break', 'continue', 'new', 'this', 'super',
            'class', 'extends', 'import', 'export', 'from', 'as',
            'async', 'await', 'yield', 'typeof', 'instanceof', 'in',
            'let', 'const', 'var', 'function',
            # ğŸ”´ Pythonç‰¹æ®Šãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆ__init__ç­‰ã®èª¤æ¤œå‡ºé˜²æ­¢ï¼‰
            '__init__', '__new__', '__del__', '__repr__', '__str__',
            '__call__', '__len__', '__iter__', '__next__', '__getitem__',
            '__setitem__', '__delitem__', '__contains__', '__enter__', '__exit__',
            # ä¸€èˆ¬çš„ãªãƒ¡ã‚½ãƒƒãƒ‰å
            'init', 'initialize', 'setup', 'main', 'run', 'start', 'stop',
            'get', 'set', 'update', 'render', 'load', 'save',
            'constructor', 'toString', 'valueOf',
            # ğŸ”´ ä¸€èˆ¬çš„ãªã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—/ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ï¼ˆèª¤æ¤œå‡ºé˜²æ­¢ï¼‰
            'setupEventListeners', 'addEventListener', 'removeEventListener',
            'health_check', 'healthCheck', 'ping', 'status',
            # ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©
            'handleClick', 'handleChange', 'handleSubmit', 'handleError',
            'onClick', 'onChange', 'onSubmit', 'onLoad', 'onError',
            # Reactãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«
            'componentDidMount', 'componentWillUnmount', 'componentDidUpdate',
            'useEffect', 'useState', 'useCallback', 'useMemo',
            # ä¸€èˆ¬çš„ãªæ“ä½œ
            'fetch', 'parse', 'format', 'validate', 'process',
            'create', 'delete', 'remove', 'add', 'find', 'filter',
            # çŠ¶æ…‹ç®¡ç†
            'show', 'hide', 'toggle', 'enable', 'disable',
            'open', 'close', 'reset', 'clear', 'refresh',
            # æ¥ç¶šãƒ»ãƒã‚¤ãƒ³ãƒ‰
            'connect', 'disconnect', 'bind', 'unbind', 'attach', 'detach',
            # ãã®ä»–
            'destroy', 'dispose', 'cleanup', 'teardown',
            # çŸ­ã™ãã‚‹åå‰ï¼ˆ2æ–‡å­—ä»¥ä¸‹ã¯é™¤å¤–ï¼‰
        }
        
        for filepath, content in all_files.items():
            is_python = filepath.endswith('.py')
            is_js = any(filepath.endswith(ext) for ext in ['.js', '.jsx', '.ts', '.tsx'])
            
            if not (is_python or is_js):
                continue
            
            patterns = py_func_patterns if is_python else js_func_patterns
            defs = set()
            
            for pattern in patterns:
                matches = re.findall(pattern, content, re.MULTILINE)
                # ä¸€èˆ¬çš„ãªåå‰ã¨çŸ­ã™ãã‚‹åå‰ã‚’é™¤å¤–
                defs.update(m for m in matches if m not in common_names and len(m) > 2)
            
            if defs:
                func_defs_by_file[filepath] = defs
        
        # ========================================
        # Step 3: å…±æœ‰DOMè¦ç´ ã‚’ç‰¹å®š
        # ========================================
        dom_to_files: Dict[str, List[str]] = {}
        
        for filepath, refs in dom_refs_by_file.items():
            for dom_id in refs:
                if dom_id not in dom_to_files:
                    dom_to_files[dom_id] = []
                dom_to_files[dom_id].append(filepath)
        
        # è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã§å‚ç…§ã•ã‚Œã‚‹ã‚‚ã®ã®ã¿
        shared_dom = {dom_id: files for dom_id, files in dom_to_files.items() if len(files) > 1}
        
        # ========================================
        # Step 4: é‡è¤‡é–¢æ•°ã‚’ç‰¹å®š
        # ========================================
        func_to_files: Dict[str, List[str]] = {}
        
        for filepath, defs in func_defs_by_file.items():
            for func_name in defs:
                if func_name not in func_to_files:
                    func_to_files[func_name] = []
                func_to_files[func_name].append(filepath)
        
        # è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã§å®šç¾©ã•ã‚Œã‚‹ã‚‚ã®ã®ã¿
        duplicate_funcs = {func: files for func, files in func_to_files.items() if len(files) > 1}
        
        # ========================================
        # Step 5: å„ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ä¾å­˜é–¢ä¿‚æƒ…å ±ã‚’æ§‹ç¯‰
        # ========================================
        dependency_info: Dict[str, Dict[str, Any]] = {}
        
        for filepath in all_files.keys():
            # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒé–¢ä¿‚ã™ã‚‹å…±æœ‰DOM
            file_shared_dom = {}
            for dom_id, files in shared_dom.items():
                if filepath in files:
                    other_files = [f for f in files if f != filepath]
                    if other_files:
                        file_shared_dom[dom_id] = other_files
            
            # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒé–¢ä¿‚ã™ã‚‹é‡è¤‡é–¢æ•°
            file_duplicate_funcs = {}
            for func_name, files in duplicate_funcs.items():
                if filepath in files:
                    other_files = [f for f in files if f != filepath]
                    if other_files:
                        file_duplicate_funcs[func_name] = other_files
            
            # é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆï¼ˆé‡è¤‡æ’é™¤ï¼‰
            related_files = set()
            for files in file_shared_dom.values():
                related_files.update(files)
            for files in file_duplicate_funcs.values():
                related_files.update(files)
            
            dependency_info[filepath] = {
                'shared_dom': file_shared_dom,
                'duplicate_functions': file_duplicate_funcs,
                'related_files': sorted(related_files)
            }
        
        # ãƒ­ã‚°å‡ºåŠ›
        dep_count = sum(1 for info in dependency_info.values() 
                        if info['shared_dom'] or info['duplicate_functions'])
        logger.debug(f"[Dependency Analysis] Analyzed {len(all_files)} files, {dep_count} have cross-file dependencies")
        
        if shared_dom:
            logger.debug(f"[Dependency Analysis] Shared DOM: {list(shared_dom.keys())[:5]}")
        if duplicate_funcs:
            logger.debug(f"[Dependency Analysis] Duplicate funcs: {list(duplicate_funcs.keys())[:5]}")
        
        return dependency_info
    
    def _extract_dependency_context(
        self,
        filepath: str,
        all_files: Dict[str, str],
        dependency_info: Dict[str, Any]
    ) -> str:
        """
        æŒ‡å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿®æ­£ã«å¿…è¦ãªä¾å­˜é–¢ä¿‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ
        
        ç ”ç©¶æ ¹æ‹ :
        - AlphaCodium: 4000ãƒˆãƒ¼ã‚¯ãƒ³ä»¥å†…ã«æŠ‘ãˆã‚‹
        - ãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã§ã¯ãªãé–¢é€£éƒ¨åˆ†ã®ã¿ã‚’æä¾›
        
        Args:
            filepath: å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            all_files: å…¨ãƒ•ã‚¡ã‚¤ãƒ«
            dependency_info: ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¾å­˜é–¢ä¿‚æƒ…å ±
        
        Returns:
            ä¾å­˜é–¢ä¿‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—åˆ—ï¼ˆæœ€å¤§2000æ–‡å­— â‰ˆ 500ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰
        """
        shared_dom = dependency_info.get('shared_dom', {})
        duplicate_funcs = dependency_info.get('duplicate_functions', {})
        
        # ä¾å­˜é–¢ä¿‚ãŒãªã‘ã‚Œã°ç©ºæ–‡å­—åˆ—
        if not shared_dom and not duplicate_funcs:
            return ""
        
        lines = []
        lines.append("**ã€Cross-File Dependencies - IMPORTANTã€‘**")
        lines.append("This file has dependencies with other files. Consider these when fixing:")
        lines.append("")
        
        # ========================================
        # 1. å…±æœ‰DOMè¦ç´ 
        # ========================================
        if shared_dom:
            lines.append("**1. Shared DOM Elements:**")
            for dom_id, other_files in list(shared_dom.items())[:5]:  # æœ€å¤§5å€‹
                files_str = ', '.join(other_files[:3])  # æœ€å¤§3ãƒ•ã‚¡ã‚¤ãƒ«
                if len(other_files) > 3:
                    files_str += f" (+{len(other_files) - 3} more)"
                lines.append(f"   - `#{dom_id}`: also used in {files_str}")
            # ğŸ†• Phase A - P8: è¡Œå‹•æŒ‡ç¤ºã®å¼·åŒ–
            lines.append("   â†’ **COORDINATION REQUIRED**: Ensure consistent ID names.")
            lines.append("   â†’ Do NOT rename these IDs - it will break other files.")
            lines.append("   â†’ If modifying DOM structure, ensure changes are compatible with all using files.")
            lines.append("")
        
        # ========================================
        # 2. é‡è¤‡é–¢æ•°ï¼ˆæœ€é‡è¦ï¼‰
        # ========================================
        if duplicate_funcs:
            lines.append("**2. âš ï¸ DUPLICATE FUNCTIONS (MUST RESOLVE):**")
            
            for func_name, other_files in list(duplicate_funcs.items())[:3]:  # æœ€å¤§3é–¢æ•°
                lines.append(f"   - `{func_name}()` is also defined in:")
                
                for other_file in other_files[:2]:  # å„é–¢æ•°ã«ã¤ãæœ€å¤§2ãƒ•ã‚¡ã‚¤ãƒ«
                    # é–¢æ•°ã®ã‚·ã‚°ãƒãƒãƒ£ã¨æœ€åˆã®æ•°è¡Œã‚’æŠ½å‡º
                    snippet = self._extract_function_snippet(all_files.get(other_file, ''), func_name)
                    if snippet:
                        lines.append(f"     {other_file}:")
                        lines.append(f"     ```")
                        for snippet_line in snippet.split('\n')[:4]:  # æœ€å¤§4è¡Œ
                            lines.append(f"     {snippet_line}")
                        lines.append(f"     ```")
                    else:
                        lines.append(f"     {other_file}")
            
            lines.append("")
            lines.append("   â†’ **ACTION REQUIRED**: Keep function in ONE file only.")
            lines.append("   â†’ If this file should own it, implement fully here.")
            lines.append("   â†’ If another file owns it, REMOVE the duplicate from this file.")
            lines.append("")
        
        # ========================================
        # 3. æ•´åˆæ€§ã®æ³¨æ„äº‹é …
        # ========================================
        lines.append("**3. Consistency Rules:**")
        lines.append("   - If you rename a function/variable, note that other files may need updates")
        lines.append("   - Maintain consistent API patterns across related files")
        
        result = '\n'.join(lines)
        
        # æ–‡å­—æ•°åˆ¶é™ï¼ˆç´„500ãƒˆãƒ¼ã‚¯ãƒ³ = ç´„2000æ–‡å­—ï¼‰
        MAX_CONTEXT_CHARS = 2000
        if len(result) > MAX_CONTEXT_CHARS:
            result = result[:MAX_CONTEXT_CHARS - 50] + "\n... (truncated for token limit)"
            logger.debug(f"[Dependency Context] Truncated context for {filepath}")
        
        return result
    
    # ============================================
    # ğŸ†• Phase A - P4: é–¢æ•°åæŠ½å‡ºãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
    # ç›®çš„: ä¿®æ­£å‰å¾Œã®é–¢æ•°å‰Šé™¤ã‚’æ¤œå‡ºã™ã‚‹ãŸã‚
    # ============================================
    
    def _extract_function_names(self, filepath: str, content: str) -> Set[str]:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰é–¢æ•°åã‚’æŠ½å‡ºï¼ˆP4: é–¢æ•°å‰Šé™¤æ¤œè¨¼ç”¨ï¼‰
        
        Args:
            filepath: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆæ‹¡å¼µå­åˆ¤å®šç”¨ï¼‰
            content: ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹
        
        Returns:
            é–¢æ•°åã®ã‚»ãƒƒãƒˆ
        """
        import re
        
        is_python = filepath.endswith('.py')
        is_js = any(filepath.endswith(ext) for ext in ['.js', '.jsx', '.ts', '.tsx'])
        
        if not (is_python or is_js):
            return set()
        
        # æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ_analyze_file_dependenciesã¨åŒã˜ï¼‰
        js_func_patterns = [
            r"(?:async\s+)?function\s+(\w+)\s*\(",
            r"(?:const|let|var)\s+(\w+)\s*=\s*(?:async\s*)?\([^)]*\)\s*=>",
            r"(?:const|let|var)\s+(\w+)\s*=\s*(?:async\s+)?function",
            r"^\s+(?:async\s+)?(\w+)\s*\([^)]*\)\s*\{",  # ES6ã‚¯ãƒ©ã‚¹ãƒ¡ã‚½ãƒƒãƒ‰
        ]
        
        py_func_patterns = [
            r"(?:async\s+)?def\s+(\w+)\s*\(",
        ]
        
        # é™¤å¤–ãƒªã‚¹ãƒˆï¼ˆäºˆç´„èªãƒ»ä¸€èˆ¬çš„ãªåå‰ï¼‰
        # _analyze_file_dependenciesã¨åŒã˜ãƒªã‚¹ãƒˆã‚’ä½¿ç”¨
        common_names = {
            # JavaScript/Pythonäºˆç´„èª
            'for', 'if', 'else', 'while', 'switch', 'case', 'default',
            'try', 'catch', 'finally', 'throw', 'with', 'do',
            'return', 'break', 'continue', 'new', 'this', 'super',
            'class', 'extends', 'import', 'export', 'from', 'as',
            'async', 'await', 'yield', 'typeof', 'instanceof', 'in',
            'let', 'const', 'var', 'function',
            # Pythonç‰¹æ®Šãƒ¡ã‚½ãƒƒãƒ‰
            '__init__', '__new__', '__del__', '__repr__', '__str__',
            '__call__', '__len__', '__iter__', '__next__', '__getitem__',
            '__setitem__', '__delitem__', '__contains__', '__enter__', '__exit__',
            # ä¸€èˆ¬çš„ãªãƒ¡ã‚½ãƒƒãƒ‰å
            'init', 'initialize', 'setup', 'main', 'run', 'start', 'stop',
            'get', 'set', 'update', 'render', 'load', 'save',
            'constructor', 'toString', 'valueOf',
            'setupEventListeners', 'addEventListener', 'removeEventListener',
            'health_check', 'healthCheck', 'ping', 'status',
            'handleClick', 'handleChange', 'handleSubmit', 'handleError',
            'onClick', 'onChange', 'onSubmit', 'onLoad', 'onError',
            'componentDidMount', 'componentWillUnmount', 'componentDidUpdate',
            'useEffect', 'useState', 'useCallback', 'useMemo',
            'fetch', 'parse', 'format', 'validate', 'process',
            'create', 'delete', 'remove', 'add', 'find', 'filter',
            'show', 'hide', 'toggle', 'enable', 'disable',
            'open', 'close', 'reset', 'clear', 'refresh',
            'connect', 'disconnect', 'bind', 'unbind', 'attach', 'detach',
            'destroy', 'dispose', 'cleanup', 'teardown',
        }
        
        patterns = py_func_patterns if is_python else js_func_patterns
        functions = set()
        
        for pattern in patterns:
            matches = re.findall(pattern, content, re.MULTILINE)
            functions.update(m for m in matches if m not in common_names and len(m) > 2)
        
        return functions
    
    # ============================================
    # ğŸ†• Phase A - P4/P5: ä¿®æ­£å“è³ªæ¤œè¨¼
    # ç›®çš„: è†¨å¼µãƒ»é–¢æ•°å‰Šé™¤ã‚’æ¤œå‡ºã—ã¦ä½å“è³ªãªä¿®æ­£ã‚’å´ä¸‹
    # ç ”ç©¶æ ¹æ‹ : LLM-Meta-SR (arXiv:2505.18602), 
    #           Code Inefficiencies Taxonomy (arXiv:2503.06327)
    # ============================================
    
    def _validate_fix_quality(
        self,
        filepath: str,
        original_content: str,
        fixed_content: str
    ) -> Tuple[bool, str]:
        """
        ä¿®æ­£å¾Œã®ã‚³ãƒ¼ãƒ‰å“è³ªã‚’æ¤œè¨¼ï¼ˆPhase A: P4é–¢æ•°å‰Šé™¤æ¤œè¨¼ + P5è†¨å¼µæ¤œçŸ¥ï¼‰
        
        æ¤œè¨¼é …ç›®:
        1. éåº¦ãªç¸®å°ï¼ˆ50%ä»¥ä¸‹ï¼‰ã®æ¤œå‡º [æ—¢å­˜]
        2. éåº¦ãªè†¨å¼µï¼ˆ150%ä»¥ä¸Šï¼‰ã®æ¤œå‡º [P5]
        3. é–¢æ•°å‰Šé™¤ã®æ¤œå‡º [P4]
        
        Args:
            filepath: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            original_content: ä¿®æ­£å‰ã®ã‚³ãƒ¼ãƒ‰
            fixed_content: ä¿®æ­£å¾Œã®ã‚³ãƒ¼ãƒ‰
        
        Returns:
            Tuple[bool, str]: (æ¤œè¨¼åˆæ ¼, ä¸åˆæ ¼ç†ç”±)
        """
        original_lines = len(original_content.strip().split('\n'))
        fixed_lines = len(fixed_content.strip().split('\n'))
        
        # ç©ºãƒ•ã‚¡ã‚¤ãƒ«ã¯æ¤œè¨¼ã‚¹ã‚­ãƒƒãƒ—
        if original_lines == 0:
            return True, ""
        
        # ========================================
        # æ¤œè¨¼1: éåº¦ãªç¸®å°ï¼ˆæ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
        # ========================================
        MIN_RATIO = 0.5  # 50%ä»¥ä¸‹ã«ç¸®å°ã—ãŸã‚‰å´ä¸‹
        if fixed_lines < original_lines * MIN_RATIO:
            reason = f"Code shrunk too much: {original_lines} -> {fixed_lines} lines ({fixed_lines/original_lines:.1%})"
            logger.debug(f"[Fix Quality] {filepath}: {reason}")
            return False, reason
        
        # ========================================
        # æ¤œè¨¼2: éåº¦ãªè†¨å¼µ [P5]
        # test8ãƒ‡ãƒ¼ã‚¿: 158â†’742è¡Œ(4.7å€)ãŒå•é¡Œã ã£ãŸ
        # é–¾å€¤1.5å€ã¯ååˆ†ã«ç·©ã„
        # ========================================
        MAX_RATIO = 1.5  # 150%ä»¥ä¸Šã«è†¨å¼µã—ãŸã‚‰å´ä¸‹
        MIN_LINES_FOR_BLOAT_CHECK = 50  # 50è¡Œä»¥ä¸Šã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ãƒã‚§ãƒƒã‚¯
        
        if original_lines >= MIN_LINES_FOR_BLOAT_CHECK:
            if fixed_lines > original_lines * MAX_RATIO:
                reason = f"Code bloat detected: {original_lines} -> {fixed_lines} lines ({fixed_lines/original_lines:.1%})"
                logger.debug(f"[Fix Quality] {filepath}: {reason}")
                return False, reason
        
        # ========================================
        # æ¤œè¨¼3: é–¢æ•°å‰Šé™¤ [P4æ”¹ - ç´”æ¸›åˆ¤å®šæ–¹å¼]
        # ç´”æ¸›ï¼ˆå‰Šé™¤æ•° > è¿½åŠ æ•°ï¼‰ã¯å´ä¸‹
        # åŒæ•°ã®å‰Šé™¤ãƒ»è¿½åŠ ã¯ãƒªãƒãƒ¼ãƒ ã¨ã—ã¦è­¦å‘Šã®ã¿
        # test22ãƒã‚°: search_trackså‰Šé™¤ã§èµ·å‹•ä¸å¯ â†’ ç´”æ¸›åˆ¤å®šã§é˜²æ­¢
        # ========================================
        original_funcs = self._extract_function_names(filepath, original_content)
        fixed_funcs = self._extract_function_names(filepath, fixed_content)
        
        deleted_funcs = original_funcs - fixed_funcs
        added_funcs = fixed_funcs - original_funcs
        
        if deleted_funcs:
            net_deletion = len(deleted_funcs) - len(added_funcs)
            if net_deletion > 0:
                # é–¢æ•°ã®ç´”æ¸› â†’ å´ä¸‹
                func_list = ', '.join(sorted(deleted_funcs))
                reason = f"Net function loss ({net_deletion}): {func_list}"
                logger.debug(f"[Fix Quality] {filepath}: {reason}")
                return False, reason
            else:
                # å‰Šé™¤ã‚ã‚‹ãŒè¿½åŠ ã§ç›¸æ®º â†’ ãƒªãƒãƒ¼ãƒ ã®å¯èƒ½æ€§ã€è­¦å‘Šã®ã¿
                # å°‚é–€å®¶æ¨å¥¨: Destructive Swapå¯¾ç­–ã¨ã—ã¦è©³ç´°ãƒ­ã‚°ã‚’å‡ºåŠ›
                logger.warning(f"âš ï¸ [Fix Quality] Structural Change Detected in {filepath}")
                logger.warning(f"  - DELETED: {deleted_funcs}")
                logger.warning(f"  - ADDED  : {added_funcs}")
        
        # å…¨æ¤œè¨¼åˆæ ¼
        return True, ""
    
    def _fix_comprehensive_issues(
        self,
        all_files: Dict[str, str],
        issues: List[Dict[str, Any]],
        goal: str,
        attempt_tracker: Optional['SemiAutoEngine.FixAttemptTracker'] = None,
        rejected_files: Optional[Set[str]] = None,
        round_number: int = 0  # ğŸ†• æ¡ˆ3: ãƒ©ã‚¦ãƒ³ãƒ‰ç•ªå·ï¼ˆ0=ãƒ©ã‚¦ãƒ³ãƒ‰1, 1=ãƒ©ã‚¦ãƒ³ãƒ‰2, ...ï¼‰
    ) -> Optional[Dict[str, str]]:
        """
        Fix detected issues (per-file processing + attempt tracking)
        
        Improvement: Instead of sending all issues to LLM at once, split by file
        and fix individually to improve LLM accuracy.
        
        After fixing, performs runtime import validation.
        If import fails, reverts the fix for the failing file.
        
        Args:
            all_files: Generated files
            issues: List of detected issues
            goal: Original goal
            attempt_tracker: Fix attempt tracker (optional)
            rejected_files: Set of files rejected by P4/P5 (D1: skip these files)
            round_number: Current round number (0-indexed) for progressive expansion
        
        Returns:
            Fixed files (None on failure)
        """
        if not issues or not self.llm_manager:
            return None
        
        # ========================================
        # Step 1: Group issues by file
        # ========================================
        issues_by_file: Dict[str, List[Dict[str, Any]]] = {}
        for issue in issues:
            filepath = issue.get('file')
            if filepath and filepath in all_files:
                if filepath not in issues_by_file:
                    issues_by_file[filepath] = []
                issues_by_file[filepath].append(issue)
        
        if not issues_by_file:
            logger.debug("[Comprehensive Review] No files to fix")
            return None
        
        # ğŸ†• æ¡ˆ3: ãƒ©ã‚¦ãƒ³ãƒ‰é€²è¡Œæ‹¡å¤§ï¼ˆProgressive Expansionï¼‰
        # æ ¹æ‹ : è«–æ–‡çŸ¥è¦‹ã€Œ2-3ãƒ©ã‚¦ãƒ³ãƒ‰ã§åŠ¹æœé ­æ‰“ã¡ã€
        #       â†’ ãƒ©ã‚¦ãƒ³ãƒ‰1ã§é«˜issueå„ªå…ˆã€ãƒ©ã‚¦ãƒ³ãƒ‰2ä»¥é™ã§å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚«ãƒãƒ¼
        # ãƒ©ã‚¦ãƒ³ãƒ‰1: 5ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆæœ€ã‚‚å•é¡Œã®å¤šã„ãƒ•ã‚¡ã‚¤ãƒ«ã«é›†ä¸­ï¼‰
        # ãƒ©ã‚¦ãƒ³ãƒ‰2: 15ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä¸­é–“ãƒ¬ãƒ™ãƒ«ã®ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼‰
        # ãƒ©ã‚¦ãƒ³ãƒ‰3+: 30ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå…¨ãƒ•ã‚¡ã‚¤ãƒ«å¯¾è±¡ï¼‰
        if round_number == 0:
            MAX_FIX_FILES_LIMIT = 5
        elif round_number == 1:
            MAX_FIX_FILES_LIMIT = 15
        else:
            MAX_FIX_FILES_LIMIT = 30  # å®‰å…¨å¼ï¼ˆå¤§è¦æ¨¡ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå¯¾å¿œï¼‰
        
        original_file_count = len(issues_by_file)
        if original_file_count > MAX_FIX_FILES_LIMIT:
            # Issueæ•°ãŒå¤šã„é †ã«ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½Nä»¶ã®ã¿
            sorted_files = sorted(issues_by_file.items(), key=lambda x: len(x[1]), reverse=True)
            issues_by_file = dict(sorted_files[:MAX_FIX_FILES_LIMIT])
            skipped_count = original_file_count - MAX_FIX_FILES_LIMIT
            logger.debug(f"[Comprehensive Review] Round {round_number + 1}: Limiting to top {MAX_FIX_FILES_LIMIT} files (skipped {skipped_count} files)")
        
        logger.debug(f"[Comprehensive Review] Fixing {len(issues_by_file)} file(s) individually...")
        
        # Create tracker if not provided (backward compatibility)
        if attempt_tracker is None:
            attempt_tracker = self.FixAttemptTracker(max_attempts=2)
        
        # ========================================
        # ğŸ†• Step 1.5: ä¾å­˜é–¢ä¿‚åˆ†æ
        # ç ”ç©¶æ ¹æ‹ : AlphaCodium, Aider Benchmarks
        # ========================================
        dependency_info = self._analyze_file_dependencies(all_files)
        
        # ========================================
        # Step 2: Fix each file individually (strategy changes based on attempt count)
        # ========================================
        updated_files = all_files.copy()
        actually_fixed_files = set()  # ğŸ†• å®Ÿéš›ã«ä¿®æ­£ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½è·¡
        total_fixed = 0
        
        for filepath, file_issues in issues_by_file.items():
            # ğŸ†• D1: P4/P5ã§å´ä¸‹ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—
            if rejected_files and filepath in rejected_files:
                logger.debug(f"[Comprehensive Review] {filepath}: Skipping (previously rejected by P4/P5)")
                continue
            
            # Check attempt count
            attempt_count = attempt_tracker.get_attempt_count(filepath)
            
            # ğŸ†• ä¾å­˜é–¢ä¿‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ
            dep_context = self._extract_dependency_context(
                filepath, all_files, dependency_info.get(filepath, {})
            )
            if dep_context:
                logger.debug(f"[Comprehensive Review] {filepath}: Adding dependency context ({len(dep_context)} chars)")
            
            # ============================================================
            # ğŸ†• æ”¹ä¿®: Reviewer/Implementerãƒ­ãƒ¼ãƒ«åˆ†é›¢
            # ============================================================
            use_design_change = False
            review_reason = ""
            
            if attempt_tracker.needs_design_change(filepath):
                # 3rd+ attempt: Force design change (æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ç¶­æŒ)
                use_design_change = True
                review_reason = "3rd+ attempt"
                logger.debug(f"[Comprehensive Review] {filepath}: DESIGN CHANGE required (attempt {attempt_count + 1})")
            else:
                # ğŸ†• 1-2å›ç›®ã§ã‚‚ReviewerãŒåˆ¤æ–­
                review_decision, review_reason = self._review_before_fix(
                    filepath=filepath,
                    content=all_files[filepath],
                    issues=file_issues,
                    all_files=all_files,
                    goal=goal
                )
                use_design_change = (review_decision == "DESIGN_CHANGE")
                
                if use_design_change:
                    logger.debug(f"[Comprehensive Review] {filepath}: DESIGN CHANGE (Reviewer: {review_reason})")
                else:
                    logger.debug(f"[Comprehensive Review] {filepath}: MINIMAL CHANGE approved (Reviewer: {review_reason})")
            
            # å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚ºï¼ˆImplementerãƒ­ãƒ¼ãƒ«ï¼‰
            if use_design_change:
                fixed_content = self._fix_with_design_change(
                    filepath=filepath,
                    content=all_files[filepath],
                    issues=file_issues,
                    goal=goal,
                    previous_issues=attempt_tracker.get_previous_issues(filepath),
                    dependency_context=dep_context
                )
                
                # ============================================================
                # DESIGN CHANGE v2.1: Abortæˆ»ã‚Šå€¤ã®å‡¦ç†
                # ============================================================
                if fixed_content and isinstance(fixed_content, str):
                    if fixed_content.startswith("HARD_ABORT:") or fixed_content.startswith("ESCALATE:"):
                        abort_reason = fixed_content.split(":", 1)[1] if ":" in fixed_content else "Unknown"
                        logger.debug(f"[DESIGN CHANGE v2.1] {filepath}: {fixed_content.split(':')[0]} - {abort_reason}")
                        # HARD_ABORT/ESCALATEã®å ´åˆã¯ä¿®æ­£ã‚’ã‚¹ã‚­ãƒƒãƒ—
                        if rejected_files is not None:
                            rejected_files.add(filepath)
                        continue
                    elif fixed_content.startswith("SOFT_ABORT:"):
                        abort_reason = fixed_content.split(":", 1)[1] if ":" in fixed_content else "Unknown"
                        logger.debug(f"[DESIGN CHANGE v2.1] {filepath}: SOFT_ABORT - {abort_reason}")
                        
                        # ============================================================
                        # ğŸ†• v2.1: SOFT_ABORTå¾Œã®æ®‹å­˜å•é¡Œã‚’å†è©•ä¾¡
                        # æ®‹ã£ãŸå•é¡ŒãŒMINIMAL_CHANGEã§å¯¾å¿œå¯èƒ½ãªã‚‰é™æ ¼ã—ã¦å†è©¦è¡Œ
                        # ============================================================
                        if file_issues:
                            re_review_decision, re_review_reason = self._review_before_fix(
                                filepath=filepath,
                                content=all_files[filepath],  # å…ƒã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
                                issues=file_issues,
                                all_files=all_files,
                                goal=goal
                            )
                            
                            if re_review_decision == "MINIMAL_CHANGE":
                                logger.debug(f"[DESIGN CHANGE v2.1] {filepath}: Demoting to MINIMAL_CHANGE after SOFT_ABORT - {re_review_reason}")
                                fixed_content = self._fix_with_invariants(
                                    filepath=filepath,
                                    content=all_files[filepath],
                                    issues=file_issues,
                                    goal=goal,
                                    dependency_context=dep_context
                                )
                                # MINIMAL_CHANGEã®çµæœã‚’æ¤œè¨¼
                                if fixed_content and isinstance(fixed_content, str) and fixed_content.startswith("ESCALATE:"):
                                    # ESCALATEã—ãŸå ´åˆã€æ—¢ã«DESIGN CHANGEã§å¤±æ•—ã—ã¦ã„ã‚‹ã®ã§ã‚¹ã‚­ãƒƒãƒ—
                                    logger.debug(f"[DESIGN CHANGE v2.1] {filepath}: MINIMAL_CHANGE escalated, but DESIGN_CHANGE already failed, skipping")
                                    continue
                                elif not fixed_content:
                                    logger.debug(f"[DESIGN CHANGE v2.1] {filepath}: MINIMAL_CHANGE returned no content, skipping")
                                    continue
                                # æœ‰åŠ¹ãªä¿®æ­£ãŒã‚ã‚Œã°å¾Œç¶šã®å‡¦ç†ã¸æµã™ï¼ˆcontinueã—ãªã„ï¼‰
                            else:
                                logger.debug(f"[DESIGN CHANGE v2.1] {filepath}: Re-review still requires DESIGN_CHANGE ({re_review_reason}), skipping")
                                continue
                        else:
                            continue
            else:
                fixed_content = self._fix_with_invariants(
                    filepath=filepath,
                    content=all_files[filepath],
                    issues=file_issues,
                    goal=goal,
                    dependency_context=dep_context
                )
                
                # ğŸ†• ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡º: MINIMAL CHANGEãŒè¨¼æ˜ã§ããªã‹ã£ãŸå ´åˆ
                if fixed_content and isinstance(fixed_content, str) and fixed_content.startswith("ESCALATE:"):
                    escalate_reason = fixed_content[9:]  # "ESCALATE:"ã‚’é™¤å»
                    logger.debug(f"[Comprehensive Review] {filepath}: Proof failed, escalating to DESIGN CHANGE - {escalate_reason}")
                    
                    # DESIGN CHANGEã§å†è©¦è¡Œ
                    fixed_content = self._fix_with_design_change(
                        filepath=filepath,
                        content=all_files[filepath],
                        issues=file_issues,
                        goal=goal,
                        previous_issues=attempt_tracker.get_previous_issues(filepath),
                        dependency_context=dep_context
                    )
                    
                    # DESIGN CHANGE v2.1: å†è©¦è¡Œå¾Œã®Abortå‡¦ç†
                    if fixed_content and isinstance(fixed_content, str):
                        if fixed_content.startswith("HARD_ABORT:") or fixed_content.startswith("ESCALATE:") or fixed_content.startswith("SOFT_ABORT:"):
                            abort_type = fixed_content.split(":")[0]
                            abort_reason = fixed_content.split(":", 1)[1] if ":" in fixed_content else "Unknown"
                            logger.debug(f"[DESIGN CHANGE v2.1] {filepath}: {abort_type} after escalation - {abort_reason}")
                            if rejected_files is not None:
                                rejected_files.add(filepath)
                            continue
            
            # Abortç³»ã®æˆ»ã‚Šå€¤ã‚’é™¤å¤–ã—ã¦ã‹ã‚‰å‡¦ç†
            is_abort_response = (
                fixed_content and 
                isinstance(fixed_content, str) and 
                (fixed_content.startswith("ESCALATE:") or 
                 fixed_content.startswith("HARD_ABORT:") or 
                 fixed_content.startswith("SOFT_ABORT:"))
            )
            
            if fixed_content and not is_abort_response:
                # ğŸ†• Diff Sanitizer: Issueç¯„å›²å¤–ã®å¤‰æ›´ã‚’å‰Šé™¤ï¼ˆtest19è¿½åŠ ï¼‰
                # `_fix_with_invariants`ã‚„`_fix_with_design_change`ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡ºåŠ›ã«ã‚‚é©ç”¨
                original_content = all_files[filepath]
                sanitized_content, removed_count = self._sanitize_diff(
                    original_content, fixed_content, file_issues, filepath
                )
                
                # ã‚µãƒ‹ã‚¿ã‚¤ã‚ºå¾Œã®æ§‹æ–‡ãƒã‚§ãƒƒã‚¯
                is_syntax_valid, syntax_error = self._validate_python_syntax(sanitized_content, filepath)
                if not is_syntax_valid:
                    logger.debug(f"[Diff Sanitizer] Post-sanitize syntax error in {filepath}: {syntax_error}")
                    # ã‚µãƒ‹ã‚¿ã‚¤ã‚ºã§å£Šã‚ŒãŸå ´åˆã€ã‚µãƒ‹ã‚¿ã‚¤ã‚ºå‰ã®ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨
                    sanitized_content = fixed_content
                    logger.debug(f"[Diff Sanitizer] Falling back to unsanitized code for {filepath}")
                
                # ã‚µãƒ‹ã‚¿ã‚¤ã‚ºå¾Œã®ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨
                fixed_content = sanitized_content
                
                # ğŸ†• Phase A: ä¿®æ­£å“è³ªã®æ¤œè¨¼ï¼ˆP4: é–¢æ•°å‰Šé™¤ã€P5: è†¨å¼µæ¤œçŸ¥ï¼‰
                is_valid, reject_reason = self._validate_fix_quality(
                    filepath, all_files[filepath], fixed_content
                )
                
                if is_valid:
                    # Record issue IDs only when fix is actually applied
                    issue_ids = [f"{i.get('type', '?')}:{i.get('description', '')[:30]}" for i in file_issues]
                    attempt_tracker.record_attempt(filepath, issue_ids)
                    
                    updated_files[filepath] = fixed_content
                    actually_fixed_files.add(filepath)
                    total_fixed += 1
                    logger.debug(f"[Comprehensive Review] âœ… Fixed {filepath}")
                else:
                    # ğŸ†• å“è³ªæ¤œè¨¼ã«å¤±æ•—ã—ãŸå ´åˆã¯å´ä¸‹
                    logger.debug(f"[Comprehensive Review] âš  Rejected fix for {filepath}: {reject_reason}")
                    # ğŸ†• D1: å´ä¸‹ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¨˜éŒ²ï¼ˆä»¥é™ã®ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã‚¹ã‚­ãƒƒãƒ—ï¼‰
                    if rejected_files is not None:
                        rejected_files.add(filepath)
            else:
                logger.debug(f"[Comprehensive Review] âš  Could not fix {filepath}")
        
        if total_fixed > 0:
            logger.debug(f"[Comprehensive Review] Successfully fixed {total_fixed}/{len(issues_by_file)} file(s)")
            
            # ğŸ†• å®Ÿè¡Œæ™‚æ¤œè¨¼: ä¿®æ­£å¾Œã®ã‚³ãƒ¼ãƒ‰ãŒã‚¤ãƒ³ãƒãƒ¼ãƒˆå¯èƒ½ã‹ç¢ºèª
            import_success, import_error, failing_file = self._validate_runtime_import(updated_files)
            
            if import_success:
                # æˆåŠŸã¾ãŸã¯EXTERNAL_PACKAGEï¼ˆå¤–éƒ¨ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ä¸è¶³ã¯ã‚³ãƒ¼ãƒ‰ã®ãƒã‚°ã§ã¯ãªã„ï¼‰
                if import_error and import_error.startswith("EXTERNAL_PACKAGE:"):
                    logger.debug(f"[Comprehensive Review] Runtime import OK (external package required)")
                else:
                    logger.debug("[Comprehensive Review] âœ… Runtime import validation passed")
            else:
                # å®Ÿè¡Œæ™‚ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ
                logger.debug(f"[Comprehensive Review] âš  Runtime import failed after fix: {import_error}")
                
                if failing_file:
                    logger.debug(f"[Comprehensive Review] âš  Failing file: {failing_file}")
                    
                    # ä¿®æ­£ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã®ã¿revert
                    if failing_file in actually_fixed_files:
                        logger.debug(f"[Comprehensive Review] Reverting fix for {failing_file}")
                        updated_files[failing_file] = all_files[failing_file]  # å…ƒã«æˆ»ã™
                        actually_fixed_files.discard(failing_file)
                        total_fixed -= 1
                        
                        if total_fixed <= 0:
                            logger.debug("[Comprehensive Review] All fixes reverted due to runtime errors")
                            return None
                else:
                    # failing_fileãŒç‰¹å®šã§ããªã„å ´åˆï¼ˆã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼ç­‰ï¼‰
                    logger.debug("[Comprehensive Review] âš  Could not identify failing file, continuing without revert")
            
            return updated_files
        else:
            logger.debug("[Comprehensive Review] No files were successfully fixed")
            return None
    
    def _extract_target_line_ranges(
        self,
        issues: List[Dict[str, Any]],
        margin: int = None  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯DIFF_SANITIZER_MARGINã‚’ä½¿ç”¨
    ) -> str:
        """
        Issueæƒ…å ±ã‹ã‚‰å¯¾è±¡è¡Œç¯„å›²ã‚’æŠ½å‡ºï¼ˆSCOPE ENFORCEMENTç”¨ï¼‰
        
        Args:
            issues: Issueæƒ…å ±ã®ãƒªã‚¹ãƒˆ
            margin: è¨±å¯ã™ã‚‹ãƒãƒ¼ã‚¸ãƒ³ï¼ˆå‰å¾ŒNè¡Œï¼‰
            
        Returns:
            "L10-L16, L25-L31" å½¢å¼ã®æ–‡å­—åˆ—
        """
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’å®šæ•°ã‹ã‚‰å–å¾—
        if margin is None:
            margin = DIFF_SANITIZER_MARGIN
            
        ranges = []
        for issue in issues:
            line = issue.get('line')
            if line:
                start = max(1, line - margin)
                end = line + margin
                ranges.append((start, end))
        
        if not ranges:
            return "entire file (no specific line numbers)"
        
        # é‡è¤‡ãƒ»éš£æ¥ç¯„å›²ã‚’ãƒãƒ¼ã‚¸
        ranges.sort(key=lambda x: x[0])
        merged = []
        for start, end in ranges:
            if merged and start <= merged[-1][1] + 1:
                # éš£æ¥ã¾ãŸã¯é‡è¤‡ â†’ ãƒãƒ¼ã‚¸
                merged[-1] = (merged[-1][0], max(merged[-1][1], end))
            else:
                merged.append((start, end))
        
        return ", ".join(f"L{s}-L{e}" for s, e in merged)
    
    def _sanitize_diff(
        self,
        original_content: str,
        fixed_content: str,
        issues: List[Dict[str, Any]],
        filepath: str,
        margin: int = None  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯DIFF_SANITIZER_MARGINã‚’ä½¿ç”¨
    ) -> Tuple[str, int]:
        """
        ğŸ†• Diff Sanitizer: Issueå¤–ã®å¤‰æ›´ã‚’æ©Ÿæ¢°çš„ã«å‰Šé™¤
        
        LLMãŒå‹æ‰‹ã«è¡Œã£ãŸã€Œãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã€ã‚„ã€ŒãŠç¯€ä»‹ä¿®æ­£ã€ã‚’
        æ©Ÿæ¢°çš„ã«å‰Šãè½ã¨ã—ã€Issueç¯„å›²å†…ã®å¤‰æ›´ã®ã¿ã‚’é©ç”¨ã™ã‚‹ã€‚
        
        Args:
            original_content: ä¿®æ­£å‰ã®ã‚³ãƒ¼ãƒ‰
            fixed_content: LLMãŒå‡ºåŠ›ã—ãŸä¿®æ­£å¾Œã®ã‚³ãƒ¼ãƒ‰
            issues: Issueæƒ…å ±ï¼ˆè¨±å¯ã™ã‚‹è¡Œç¯„å›²ã®è¨ˆç®—ã«ä½¿ç”¨ï¼‰
            filepath: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆãƒ­ã‚°ç”¨ï¼‰
            margin: è¨±å¯ã™ã‚‹ãƒãƒ¼ã‚¸ãƒ³ï¼ˆå‰å¾ŒNè¡Œï¼‰
            
        Returns:
            Tuple[str, int]: (ã‚µãƒ‹ã‚¿ã‚¤ã‚ºå¾Œã®ã‚³ãƒ¼ãƒ‰, å‰Šé™¤ã•ã‚ŒãŸå¤‰æ›´æ•°)
        """
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’å®šæ•°ã‹ã‚‰å–å¾—
        if margin is None:
            margin = DIFF_SANITIZER_MARGIN
            
        try:
            import difflib
            
            original_lines = original_content.splitlines(keepends=True)
            fixed_lines = fixed_content.splitlines(keepends=True)
            
            # æœ€å¾Œã®è¡Œã«æ”¹è¡ŒãŒãªã„å ´åˆã®å¯¾å¿œ
            if original_lines and not original_lines[-1].endswith('\n'):
                original_lines[-1] += '\n'
            if fixed_lines and not fixed_lines[-1].endswith('\n'):
                fixed_lines[-1] += '\n'
            
            # è¨±å¯ã•ã‚ŒãŸè¡Œç•ªå·ã®ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ï¼ˆ1-indexedï¼‰
            allowed_lines = set()
            for issue in issues:
                line = issue.get('line')
                if line:
                    for l in range(max(1, line - margin), line + margin + 1):
                        allowed_lines.add(l)
            
            # è¡Œç•ªå·ã®æŒ‡å®šãŒãªã„å ´åˆã€å…¨è¡Œã‚’è¨±å¯
            if not allowed_lines:
                return fixed_content, 0
            
            # è¡Œã”ã¨ã«å·®åˆ†ã‚’æ¯”è¼ƒ
            result_lines = []
            removed_count = 0
            matcher = difflib.SequenceMatcher(None, original_lines, fixed_lines)
            
            for tag, i1, i2, j1, j2 in matcher.get_opcodes():
                if tag == 'equal':
                    # å¤‰æ›´ãªã— â†’ ãã®ã¾ã¾è¿½åŠ 
                    result_lines.extend(original_lines[i1:i2])
                elif tag == 'replace':
                    # ç½®æ›: å½±éŸ¿ã‚’å—ã‘ã‚‹å…ƒã®è¡ŒãŒè¨±å¯ç¯„å›²å†…ã‹ãƒã‚§ãƒƒã‚¯
                    affected_lines = set(range(i1 + 1, i2 + 1))  # 1-indexed
                    if affected_lines & allowed_lines:
                        # è¨±å¯ç¯„å›²å†… â†’ ä¿®æ­£ã‚’é©ç”¨
                        result_lines.extend(fixed_lines[j1:j2])
                    else:
                        # è¨±å¯ç¯„å›²å¤– â†’ å…ƒã«æˆ»ã™
                        result_lines.extend(original_lines[i1:i2])
                        removed_count += (i2 - i1)
                elif tag == 'delete':
                    # å‰Šé™¤: å‰Šé™¤ã•ã‚Œã‚‹è¡ŒãŒè¨±å¯ç¯„å›²å†…ã‹ãƒã‚§ãƒƒã‚¯
                    affected_lines = set(range(i1 + 1, i2 + 1))  # 1-indexed
                    if affected_lines & allowed_lines:
                        # è¨±å¯ç¯„å›²å†… â†’ å‰Šé™¤ã‚’é©ç”¨ï¼ˆä½•ã‚‚è¿½åŠ ã—ãªã„ï¼‰
                        pass
                    else:
                        # è¨±å¯ç¯„å›²å¤– â†’ å…ƒã«æˆ»ã™
                        result_lines.extend(original_lines[i1:i2])
                        removed_count += (i2 - i1)
                elif tag == 'insert':
                    # æŒ¿å…¥: æŒ¿å…¥ä½ç½®ï¼ˆå…ƒãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼‰ãŒè¨±å¯ç¯„å›²å†…ã‹ãƒã‚§ãƒƒã‚¯
                    # å°‚é–€å®¶æ¨å¥¨: ã€ŒæŒ¿å…¥ã•ã‚ŒãŸä¸­èº«ã€ã§ã¯ãªãã€ŒæŒ¿å…¥ã•ã‚ŒãŸå ´æ‰€ã€ã‚’è©•ä¾¡
                    insert_position = i1 + 1  # 1-indexed (æŒ¿å…¥ã¯ i1 ã®å¾Œã«è¡Œã‚ã‚Œã‚‹)
                    # éš£æ¥ã™ã‚‹è¡ŒãŒè¨±å¯ç¯„å›²å†…ã§ã‚ã‚Œã°ã€æŒ¿å…¥ã‚’è¨±å¯
                    adjacent_allowed = (insert_position in allowed_lines or 
                                       (insert_position - 1) in allowed_lines or
                                       (insert_position + 1) in allowed_lines)
                    if adjacent_allowed:
                        # è¨±å¯ç¯„å›²å†… â†’ æŒ¿å…¥ã‚’é©ç”¨
                        result_lines.extend(fixed_lines[j1:j2])
                    else:
                        # è¨±å¯ç¯„å›²å¤– â†’ æŒ¿å…¥ã‚’ç„¡è¦–
                        removed_count += (j2 - j1)
            
            sanitized = ''.join(result_lines)
            
            # ãƒ­ã‚°å‡ºåŠ›
            if removed_count > 0:
                logger.debug(f"[Diff Sanitizer] {filepath}: Removed {removed_count} unauthorized change(s) outside target ranges")
            else:
                logger.debug(f"[Diff Sanitizer] {filepath}: All changes within allowed scope")
            
            return sanitized, removed_count
            
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ã‚µãƒ‹ã‚¿ã‚¤ã‚ºã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€å…ƒã®fixed_contentã‚’è¿”ã™
            logger.debug(f"[Diff Sanitizer] Error processing {filepath}: {e} - Skipping sanitization")
            return fixed_content, 0
    
    def _validate_python_syntax(
        self,
        content: str,
        filepath: str
    ) -> Tuple[bool, str]:
        """
        ğŸ†• Pythonã‚³ãƒ¼ãƒ‰ã®æ§‹æ–‡ã‚’æ¤œè¨¼
        
        Args:
            content: æ¤œè¨¼ã™ã‚‹ã‚³ãƒ¼ãƒ‰
            filepath: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            
        Returns:
            Tuple[bool, str]: (æœ‰åŠ¹ã‹ã©ã†ã‹, ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸)
        """
        if not filepath.endswith('.py'):
            return True, ""
        
        try:
            import ast
            ast.parse(content)
            return True, ""
        except SyntaxError as e:
            error_msg = f"Line {e.lineno}: {e.msg}"
            logger.debug(f"[Syntax Check] âŒ {filepath}: {error_msg}")
            return False, error_msg
        except Exception as e:
            # ast.parse()ã®äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ï¼ˆä¾‹: contentãŒNoneç­‰ï¼‰
            logger.debug(f"[Syntax Check] âš  Unexpected error for {filepath}: {e}")
            return True, ""  # æ¤œè¨¼ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ç¶šè¡Œ
    
    def _fix_single_file_issues(
        self,
        filepath: str,
        content: str,
        issues: List[Dict[str, Any]],
        goal: str
    ) -> Optional[str]:
        """
        å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®å•é¡Œã‚’ä¿®æ­£
        
        Args:
            filepath: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            content: ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹
            issues: ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®å•é¡Œãƒªã‚¹ãƒˆ
            goal: å…ƒã®ã‚´ãƒ¼ãƒ«
        
        Returns:
            ä¿®æ­£å¾Œã®ã‚³ãƒ¼ãƒ‰ï¼ˆå¤±æ•—æ™‚ã¯Noneï¼‰
        """
        if not issues or not self.llm_manager:
            return None
        
        # å•é¡Œæƒ…å ±ã‚’æ•´å½¢
        issues_text = ""
        for i, issue in enumerate(issues, 1):
            issues_text += f"{i}. [{issue.get('type', '?')}]"
            if issue.get('line'):
                issues_text += f" Line {issue['line']}"
            issues_text += f"\n   Problem: {issue.get('description', 'No description')}\n"
            issues_text += f"   Fix: {issue.get('fix', 'No suggestion')}\n\n"
        
        # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯åˆ¶ç´„ã‚’æ§‹ç¯‰
        framework_constraint = ""
        if getattr(self, '_no_framework_mode', False):
            framework_constraint = """
âš ï¸ CRITICAL FRAMEWORK RESTRICTION:
- DO NOT use React, Vue, Angular, or any framework
- DO NOT generate JSX syntax
- YOU MUST generate REAL, FUNCTIONAL vanilla JavaScript code
- The fixed code must work standalone without any framework
"""
        
        # ã‚´ãƒ¼ãƒ«ã®åˆ‡ã‚Šè©°ã‚ï¼ˆ4000æ–‡å­—ã¾ã§è¨±å®¹ï¼‰
        goal_truncated = goal[:4000] + "..." if len(goal) > 4000 else goal
        
        # ğŸ†• SCOPE ENFORCEMENT: å¯¾è±¡è¡Œç¯„å›²ã‚’æŠ½å‡º
        target_line_ranges = self._extract_target_line_ranges(issues)
        
        # ğŸ†• SCOPE ENFORCEMENT ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆæœ€ä¸Šæ®µã«é…ç½®ï¼‰
        scope_enforcement = f"""
### âš ï¸ SCOPE ENFORCEMENT (HIGHEST PRIORITY) âš ï¸

Your edit scope is STRICTLY LIMITED to the following line ranges: {target_line_ranges}

**ABSOLUTE RULES - VIOLATION = IMMEDIATE REJECTION:**
1. You are NOT an optimizer. You are a surgery robot.
2. You may ONLY modify lines within Â±5 lines of each issue location.
3. Every line outside the scope MUST remain byte-identical to the original.
4. If you change a single character outside the scope, the ENTIRE operation FAILS.
5. DO NOT refactor, optimize, or "improve" any code outside the issue locations.
6. DO NOT rename variables, reformat code, or reorganize imports outside the scope.
7. DO NOT change indentation unless necessary for the fix.
8. DO NOT add or remove comments unrelated to the issues.

"""
        
        prompt = f"""Fix the following issues in this single file.
{scope_enforcement}
{framework_constraint}
**File:** {filepath}

**Original Goal (for context):**
{goal_truncated}

**Issues to Fix ({len(issues)} issue(s)):**
{issues_text}

**Current Code:**
```
{content}
```

**Instructions:**
1. Fix ALL listed issues completely
2. For TODO/unimplemented features: Implement the actual functionality
3. For error handling: Add proper try-catch with user feedback
4. For state management: Ensure proper initialization and reset
5. For save/load: Ensure data is properly serialized and restored
6. For navigation: Add back buttons or proper flow
7. For library API misuse: Fix according to library documentation (e.g., Marshmallow: remove required=True when using load_default)
8. For circular imports: Use lazy imports, move imports inside functions, or restructure modules
9. For security issues: Use parameterized queries, escape user input, hash passwords, use environment variables for secrets
10. For resource leaks: Use context managers (with statement), ensure close() is called in finally blocks
11. For type mismatches: Add Optional[] to return type or ensure consistent return values
12. For hardcoded paths: Use os.path.join(), pathlib, or environment variables
13. For debug code: Remove all print/console.log/debugger statements (except intentional logging)
14. For external dependencies: Add timeout parameter, wrap in try-catch, add retry logic or fallback
15. Do NOT add new TODO comments
16. Do NOT remove existing functionality
17. Maintain the existing code style

**Output Format:**
Return the complete fixed code wrapped in:
<file path='{filepath}'>
complete fixed code here
</file>"""

        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã«å¿œã˜ã¦max_tokensã‚’èª¿æ•´
            content_lines = len(content.split('\n'))
            max_tokens = 8000 if content_lines < 300 else 12000 if content_lines < 500 else 16000
            
            logger.debug(f"[Comprehensive Review] Sending fix request for {filepath} ({len(issues)} issues, {content_lines} lines, max_tokens={max_tokens})...")
            
            response = self.llm_manager.generate_response(
                prompt=prompt,
                system_prompt="You are a code quality expert. Fix the issues in this single file. Return only the complete fixed code.",
                max_tokens=max_tokens
            )
            
            logger.debug(f"[Comprehensive Review] LLM response received for {filepath}")
            
            if hasattr(response, 'content'):
                response_text = response.content
            else:
                response_text = str(response)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«æŠ½å‡º
            fixed_files = self._parse_llm_response(response_text)
            
            # ä¿®æ­£ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—
            fixed_code = None
            if fixed_files:
                logger.debug(f"[Comprehensive Review] Extracted {len(fixed_files)} file(s) from LLM response")
                
                # å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’æ¢ã™
                for fpath, code in fixed_files.items():
                    # ãƒ‘ã‚¹ã®æ­£è¦åŒ–ï¼ˆãƒãƒƒã‚¯ã‚¹ãƒ©ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰
                    normalized_fpath = fpath.replace('\\', '/')
                    normalized_filepath = filepath.replace('\\', '/')
                    if normalized_fpath == normalized_filepath or fpath == filepath:
                        logger.debug(f"[Comprehensive Review] Found exact match for {filepath}")
                        fixed_code = code
                        break
                
                # ãƒ•ã‚¡ã‚¤ãƒ«åã ã‘ã§ä¸€è‡´ã‚’è©¦ã¿ã‚‹
                if not fixed_code:
                    target_filename = filepath.split('/')[-1].split('\\')[-1]
                    for fpath, code in fixed_files.items():
                        if fpath.endswith(target_filename):
                            logger.debug(f"[Comprehensive Review] Found filename match: {fpath} -> {filepath}")
                            fixed_code = code
                            break
                
                # 1ã¤ã—ã‹ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã‘ã‚Œã°ãã‚Œã‚’ä½¿ã†
                if not fixed_code and len(fixed_files) == 1:
                    logger.debug(f"[Comprehensive Review] Using single file from response for {filepath}")
                    fixed_code = list(fixed_files.values())[0]
                
                if not fixed_code:
                    logger.debug(f"[Comprehensive Review] Could not match response to {filepath}, available: {list(fixed_files.keys())}")
            else:
                logger.debug(f"[Comprehensive Review] No files extracted from LLM response for {filepath}")
            
            # ğŸ†• ä¿®æ­£ã‚³ãƒ¼ãƒ‰ãŒå–å¾—ã§ããŸå ´åˆã€ã‚µãƒ‹ã‚¿ã‚¤ã‚ºã¨æ§‹æ–‡ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œ
            if fixed_code:
                # Step 1: æ§‹æ–‡ãƒã‚§ãƒƒã‚¯ï¼ˆPythonãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ï¼‰
                is_valid, error_msg = self._validate_python_syntax(fixed_code, filepath)
                if not is_valid:
                    logger.debug(f"[Syntax Check] âŒ {filepath}: {error_msg} - Rejecting fix")
                    return None
                
                # Step 2: Diff Sanitizerï¼ˆIssueç¯„å›²å¤–ã®å¤‰æ›´ã‚’å‰Šé™¤ï¼‰
                sanitized_code, removed_count = self._sanitize_diff(
                    content, fixed_code, issues, filepath
                )
                
                # Step 3: ã‚µãƒ‹ã‚¿ã‚¤ã‚ºå¾Œã®æ§‹æ–‡ãƒã‚§ãƒƒã‚¯
                is_valid, error_msg = self._validate_python_syntax(sanitized_code, filepath)
                if not is_valid:
                    logger.debug(f"[Syntax Check] âŒ Post-sanitize syntax error in {filepath}: {error_msg}")
                    # ã‚µãƒ‹ã‚¿ã‚¤ã‚ºã§å£Šã‚ŒãŸå ´åˆã€ã‚µãƒ‹ã‚¿ã‚¤ã‚ºå‰ã®ã‚³ãƒ¼ãƒ‰ã‚’è¿”ã™
                    logger.debug(f"[Diff Sanitizer] Falling back to unsanitized code for {filepath}")
                    return fixed_code
                
                return sanitized_code
            
            return None
            
        except Exception as e:
            logger.debug(f"[Comprehensive Review] Fix error for {filepath}: {e}")
            return None

    # ============================================================
    # ğŸ†• æ”¹ä¿®: Reviewer/Implementerãƒ­ãƒ¼ãƒ«åˆ†é›¢
    # ============================================================
    def _review_before_fix(
        self,
        filepath: str,
        content: str,
        issues: List[Dict[str, Any]],
        all_files: Dict[str, str],
        goal: str
    ) -> Tuple[str, str]:
        """
        ä¿®æ­£ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®ãƒ¬ãƒ“ãƒ¥ãƒ¼åˆ¤æ–­ï¼ˆReviewerãƒ­ãƒ¼ãƒ«ï¼‰
        
        LLMã«ã€Œã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã‹ã›ãšã€åˆ¤æ–­ã®ã¿ã•ã›ã‚‹ã€ã“ã¨ã§ã€
        MINIMAL CHANGE vs DESIGN CHANGE ã®é©åˆ‡ãªé¸æŠã‚’è¡Œã†ã€‚
        
        æ ¹æ‹ : LLMã¯ã€Œåˆ¤æ–­ã€ã‚ˆã‚Šã€Œæ­£å½“åŒ–ã€ãŒå¾—æ„
              â†’ åˆ¤æ–­ã•ã›ã‚‹ã®ã§ã¯ãªãã€è¨¼æ˜ã‚’è¦æ±‚ã™ã‚‹
        
        Args:
            filepath: å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            content: ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹
            issues: ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®Issueãƒªã‚¹ãƒˆ
            all_files: å…¨ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä¾å­˜é–¢ä¿‚ç¢ºèªç”¨ï¼‰
            goal: å…ƒã®ã‚´ãƒ¼ãƒ«
        
        Returns:
            Tuple[str, str]: (decision, reason)
                - decision: "MINIMAL_CHANGE" or "DESIGN_CHANGE"
                - reason: åˆ¤æ–­ç†ç”±
        """
        if not issues or not self.llm_manager:
            return ("MINIMAL_CHANGE", "No issues or LLM unavailable")
        
        # Issueæƒ…å ±ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        issues_text = ""
        for i, issue in enumerate(issues, 1):
            issues_text += f"{i}. [{issue.get('type', '?')}] {issue.get('description', 'No description')[:100]}\n"
        
        # ä¾å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ—æŒ™
        dependent_files = []
        filepath_basename = filepath.split('/')[-1].split('\\')[-1]
        for other_path, other_content in all_files.items():
            if other_path != filepath:
                # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹ã‹ç¢ºèª
                if filepath_basename.replace('.py', '') in other_content:
                    dependent_files.append(other_path)
        
        dependent_files_text = ", ".join(dependent_files[:10]) if dependent_files else "None detected"
        content_lines = len(content.split('\n'))
        
        prompt = f"""You are a CODE REVIEWER, not an implementer.
Your job is to decide whether MINIMAL CHANGE is appropriate for these issues.
DO NOT write any code in this response. Only provide analysis and decision.

**File under review:** {filepath}
**File size:** {content_lines} lines

**Issues to evaluate:**
{issues_text}

**Files that may depend on this file:**
{dependent_files_text}

**Your task:**
To approve MINIMAL CHANGE, you must PROVE all of the following:

1. LOCALITY PROOF: Can each issue be fixed by modifying â‰¤5 lines?
   - Count the minimum lines that must change for each issue
   - If total > 5 lines: FAIL

2. ISOLATION PROOF: Will changing these lines affect other files?
   - Check if any dependent files use the functions/variables being changed
   - If cross-file impact exists: FAIL

3. PATTERN PROOF: Is this a one-off bug or a systemic pattern?
   - Could the same bug exist in similar code elsewhere?
   - If systemic pattern: FAIL

**Output format (EXACTLY this structure):**
<review>
<locality_proof>
[PASS or FAIL] - [specific line count and reasoning]
</locality_proof>

<isolation_proof>
[PASS or FAIL] - [list impacted files or "none"]
</isolation_proof>

<pattern_proof>
[PASS or FAIL] - [one-off or systemic, with reasoning]
</pattern_proof>

<decision>[MINIMAL_CHANGE or DESIGN_CHANGE]</decision>
<reason>[One sentence explaining the decision]</reason>
</review>"""
        
        try:
            logger.debug(f"[Review] Evaluating fix approach for {filepath}...")
            
            response = self.llm_manager.generate_response(
                prompt=prompt,
                system_prompt="You are a senior code reviewer. Evaluate whether minimal changes are safe. DO NOT write any code.",
                max_tokens=2000  # ãƒ¬ãƒ“ãƒ¥ãƒ¼ãªã®ã§å°‘ãªã‚ã§OK
            )
            
            response_text = response.content if hasattr(response, 'content') else str(response)
            
            # åˆ¤æ–­çµæœã‚’ãƒ‘ãƒ¼ã‚¹
            decision_match = re.search(r'<decision>\s*(MINIMAL_CHANGE|DESIGN_CHANGE)\s*</decision>', response_text, re.IGNORECASE)
            reason_match = re.search(r'<reason>([^<]+)</reason>', response_text)
            
            if decision_match:
                decision = decision_match.group(1).upper()
                reason = reason_match.group(1).strip() if reason_match else "No reason provided"
                
                # FAILåˆ¤å®šã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆå®‰å…¨è£…ç½®ï¼‰
                fail_count = response_text.upper().count('FAIL')
                if fail_count > 0 and decision == "MINIMAL_CHANGE":
                    # FAILãŒã‚ã‚‹ã®ã«MINIMAL_CHANGEã‚’é¸ã‚“ã å ´åˆã¯çŸ¯æ­£
                    logger.debug(f"[Review] Correcting decision: {fail_count} FAILs found but MINIMAL_CHANGE selected")
                    decision = "DESIGN_CHANGE"
                    reason = f"Corrected: {fail_count} proof(s) failed"
                
                logger.debug(f"[Review] {filepath}: {decision} - {reason}")
                return (decision, reason)
            else:
                # ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§MINIMAL_CHANGEï¼ˆä¿å®ˆçš„ï¼‰
                logger.debug(f"[Review] {filepath}: Could not parse decision, defaulting to MINIMAL_CHANGE")
                return ("MINIMAL_CHANGE", "Parse failed, using default")
                
        except Exception as e:
            logger.debug(f"[Review] Error reviewing {filepath}: {e}")
            return ("MINIMAL_CHANGE", f"Review error: {e}")

    def _fix_with_invariants(
        self,
        filepath: str,
        content: str,
        issues: List[Dict[str, Any]],
        goal: str,
        dependency_context: str = ""  # ğŸ†• ä¾å­˜é–¢ä¿‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
    ) -> Optional[str]:
        """
        Minimal-change fix (1st-2nd attempt) - v2: æœ€å°å¤‰æ›´åŸå‰‡
        
        æ ¹æ‹ : å­¦è¡“è«–æ–‡ã®çŸ¥è¦‹
          - LLM4CVE (2025): "make the minimal amount of changes required"
          - Less is More (AAAI2025): "generate patches with minimal modifications"
        
        Args:
            filepath: File path
            content: File content
            issues: List of issues for this file
            goal: Original goal
            dependency_context: Cross-file dependency information (optional)
        
        Returns:
            Fixed code (None on failure)
        """
        if not issues or not self.llm_manager:
            return None
        
        # Format issue information
        issues_text = ""
        for i, issue in enumerate(issues, 1):
            issues_text += f"{i}. [{issue.get('type', '?')}]"
            if issue.get('line'):
                issues_text += f" Line {issue['line']}"
            issues_text += f"\n   Problem: {issue.get('description', 'No description')}\n"
            issues_text += f"   Why bad: {issue.get('fix', 'Unknown')}\n\n"
        
        # Framework constraint
        framework_constraint = ""
        if getattr(self, '_no_framework_mode', False):
            framework_constraint = """
âš ï¸ FRAMEWORK RESTRICTION: Do NOT use React, Vue, Angular. Use vanilla JavaScript only.
"""
        
        # Truncate goal
        goal_truncated = goal[:3000] + "..." if len(goal) > 3000 else goal
        
        # ğŸ†• ä¾å­˜é–¢ä¿‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³
        dependency_section = ""
        if dependency_context:
            dependency_section = f"""
{dependency_context}

"""
        
        # ğŸ†• C1: ã‚³ãƒ¼ãƒ‰ã‚µã‚¤ã‚ºè¨ˆç®—ï¼ˆè†¨å¼µæŠ‘åˆ¶æŒ‡ç¤ºç”¨ï¼‰
        content_lines = len(content.split('\n'))
        max_allowed_lines = int(content_lines * 1.3)  # 130%ã¾ã§è¨±å®¹
        
        # ğŸ†• test19: SCOPE ENFORCEMENTï¼ˆå¯¾è±¡è¡Œç¯„å›²ã‚’æŠ½å‡ºï¼‰
        target_line_ranges = self._extract_target_line_ranges(issues)
        
        scope_enforcement = f"""
### âš ï¸ SCOPE ENFORCEMENT (HIGHEST PRIORITY) âš ï¸
Your edit scope is STRICTLY LIMITED to: {target_line_ranges}

**ABSOLUTE RULES - VIOLATION = IMMEDIATE REJECTION:**
1. You are NOT an optimizer. You are a surgery robot.
2. You may ONLY modify lines within Â±5 lines of each issue location.
3. Every line outside the scope MUST remain byte-identical to the original.
4. If you change a single character outside the scope, the ENTIRE operation FAILS.
5. DO NOT refactor, optimize, or "improve" any code outside the issue locations.
6. DO NOT rename variables, reformat code, or reorganize imports outside the scope.
7. DO NOT change indentation unless necessary for the fix.
8. DO NOT add or remove comments unrelated to the issues.

"""
        
        # ğŸ†• v2: Minimal Change Fix ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        # æ ¹æ‹ : å­¦è¡“è«–æ–‡ã®çŸ¥è¦‹
        #   - LLM4CVE (2025): "make the minimal amount of changes required"
        #   - Less is More (AAAI2025): "generate patches with minimal modifications"
        #   - CigaR (2024): "maximize trade-off between conciseness and information"
        prompt = f"""**MINIMAL CHANGE FIX** (æœ€å°å¤‰æ›´ä¿®æ­£)
{scope_enforcement}
{dependency_section}
**CORE PRINCIPLE:** Make the SMALLEST change that fixes the issue.
Smaller changes = fewer side effects = no whack-a-mole.
{framework_constraint}

**PROOF REQUIREMENT (MANDATORY):**
Before writing ANY code, you must PROVE all of the following.
If you CANNOT prove ANY of these, you MUST output <escalate> and STOP.

1. LOCALITY PROOF: This bug can be fixed by modifying â‰¤5 lines in this file only
   â†’ List the exact line numbers that need to change
   â†’ If more than 5 lines needed: FAIL â†’ escalate

2. ISOLATION PROOF: No other file imports, calls, or depends on the lines being changed
   â†’ Check imports/exports affected by your change
   â†’ If cross-file impact exists: FAIL â†’ escalate

3. NON-RECURRENCE PROOF: This exact bug pattern does not exist elsewhere in the codebase
   â†’ Is this a one-off mistake or a systemic pattern?
   â†’ If systemic pattern: FAIL â†’ escalate

**CRITICAL:** If ANY proof fails, output ONLY this and STOP:
<escalate reason="[WHICH proof failed: LOCALITY/ISOLATION/NON-RECURRENCE] - [specific reason]">
DESIGN CHANGE required
</escalate>

DO NOT write any code if escalation is needed.

**FIX RULES (only if all proofs PASS):**
1. Change ONLY the problematic lines - do NOT touch unrelated code
2. Do NOT refactor or reorganize code structure
3. Do NOT add new functions unless absolutely necessary
4. Do NOT change function signatures (breaks callers)
5. Do NOT change import/export names (breaks dependents)
6. Keep the fix as SMALL as possible

**SIZE CONSTRAINT (CRITICAL):**
- Current file: {content_lines} lines
- Maximum allowed: {max_allowed_lines} lines (130% of original)
- FEWER lines = BETTER (if functionality preserved)

**Issues to fix:**
{issues_text}

**Goal context:** {goal_truncated}

**File:** {filepath}
```
{content}
```

**Output Format:**

Option A - If escalation needed (any proof failed):
<escalate reason="[proof that failed] - [specific reason]">
DESIGN CHANGE required
</escalate>

Option B - If all proofs pass:
<proof>
LOCALITY: PASS - Lines [X, Y, Z] will be modified (â‰¤5 lines)
ISOLATION: PASS - No other files depend on these lines
NON-RECURRENCE: PASS - This is a one-off issue, not a pattern
</proof>

<file path='{filepath}'>
COMPLETE fixed code here (entire file content)
</file>"""

        try:
            max_tokens = 8000 if content_lines < 300 else 12000 if content_lines < 500 else 16000
            
            logger.debug(f"[Comprehensive Review] Sending minimal-change fix for {filepath} ({len(issues)} issues, {content_lines} lines)")
            
            response = self.llm_manager.generate_response(
                prompt=prompt,
                system_prompt="You are a code repair specialist. Make the SMALLEST change that fixes the issue. Return COMPLETE file contents.",
                max_tokens=max_tokens
            )
            
            response_text = response.content if hasattr(response, 'content') else str(response)
            
            # ğŸ†• æ”¹ä¿®: ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡º
            escalate_match = re.search(r'<escalate[^>]*reason="([^"]*)"[^>]*>', response_text, re.IGNORECASE)
            if not escalate_match:
                # åˆ¥å½¢å¼ã®ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚‚æ¤œå‡º
                escalate_match = re.search(r'<escalate[^>]*>.*?reason[:\s]*([^<\n]+)', response_text, re.IGNORECASE | re.DOTALL)
            
            if escalate_match:
                escalate_reason = escalate_match.group(1).strip()
                logger.debug(f"[Comprehensive Review] {filepath}: ESCALATED by proof failure - {escalate_reason}")
                # "ESCALATE"æ–‡å­—åˆ—ã‚’è¿”ã™ã“ã¨ã§ã€å‘¼ã³å‡ºã—å…ƒã§DESIGN CHANGEã¸ç§»è¡Œ
                return "ESCALATE:" + escalate_reason
            
            # é€šå¸¸ã®ãƒ•ã‚¡ã‚¤ãƒ«æŠ½å‡ºå‡¦ç†
            fixed_files = self._parse_llm_response(response_text)
            
            if fixed_files:
                logger.debug(f"[Comprehensive Review] Extracted {len(fixed_files)} file(s) from minimal-change fix")
                # Path normalization and matching
                normalized_filepath = filepath.replace('\\', '/')
                for fpath, code in fixed_files.items():
                    normalized_fpath = fpath.replace('\\', '/')
                    if normalized_fpath == normalized_filepath or fpath == filepath:
                        return code
                    if fpath.endswith(filepath.split('/')[-1].split('\\')[-1]):
                        return code
                if len(fixed_files) == 1:
                    return list(fixed_files.values())[0]
            
            return None
            
        except Exception as e:
            logger.debug(f"[Comprehensive Review] Minimal-change fix error for {filepath}: {e}")
            return None

    # ============================================================
    # DESIGN CHANGE åˆ¶å¾¡ä»•æ§˜ v2.0
    # ============================================================
    # ã‚³ãƒ³ã‚»ãƒ—ãƒˆ: ã€Œç¦æ­¢ã‚ˆã‚ŠæŒ‡ç¤ºã€ã€Œå…·ä½“çš„ã«1ã¤ã ã‘ã€ã€Œå·®åˆ†å‡ºåŠ›ã€
    _design_change_hard_abort_counts: Dict[str, int] = {}
    
    def _fix_with_design_change(
        self,
        filepath: str,
        content: str,
        issues: List[Dict[str, Any]],
        goal: str,
        previous_issues: List[str],
        dependency_context: str = ""
    ) -> Optional[str]:
        """
        DESIGN CHANGE with Diff-based Minimal Fix (v2.0)
        
        v2.0å¤‰æ›´ç‚¹:
        - ç¦æ­¢ãƒªã‚¹ãƒˆã‚’å‰Šé™¤ã€ã€Œã‚„ã‚‹ã“ã¨ã€ã‚’å…·ä½“çš„ã«æŒ‡ç¤º
        - å®Œå…¨ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›â†’å·®åˆ†å‡ºåŠ›ã«å¤‰æ›´
        - WRONG/RIGHTã®å…·ä½“ä¾‹ã‚’æç¤º
        - æˆåŠŸã®å®šç¾©ã‚’ã€Œæœ€å°é™ã®å¤‰æ›´ã€ã¨æ˜ç¢ºåŒ–
        - ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å‰Šæ¸›ï¼ˆ12000â†’4000ï¼‰
        
        Args:
            filepath: File path
            content: File content
            issues: List of issues for this file
            goal: Original goal
            previous_issues: History of previous issues (not used in v2.0)
            dependency_context: Cross-file dependency information
        
        Returns:
            Fixed code, "SOFT_ABORT:reason", or None
        """
        if not issues or not self.llm_manager:
            return None
        
        # Format issue information with numbers
        issues_text = ""
        for i, issue in enumerate(issues, 1):
            issues_text += f"{i}. [{issue.get('type', '?')}] {issue.get('description', 'No description')}\n"
        
        # Framework constraint
        framework_constraint = ""
        if getattr(self, '_no_framework_mode', False):
            framework_constraint = "\nâš ï¸ FRAMEWORK RESTRICTION: Do NOT use React, Vue, Angular. Use vanilla JavaScript only.\n"
        
        # Dependency section
        dependency_section = f"\n{dependency_context}\n" if dependency_context else ""
        
        # Code metrics (for display only)
        content_lines = len(content.split('\n'))
        
        # ğŸ†• test19: SCOPE ENFORCEMENTï¼ˆå¯¾è±¡è¡Œç¯„å›²ã‚’æŠ½å‡ºï¼‰
        target_line_ranges = self._extract_target_line_ranges(issues)
        
        scope_enforcement = f"""
### âš ï¸ SCOPE ENFORCEMENT (HIGHEST PRIORITY) âš ï¸
Your edit scope is STRICTLY LIMITED to: {target_line_ranges}

**ABSOLUTE RULES - VIOLATION = IMMEDIATE REJECTION:**
1. You are NOT an optimizer. You are a surgery robot.
2. You may ONLY modify lines within Â±5 lines of each issue location.
3. Every line outside the scope MUST remain byte-identical to the original.
4. If you change a single character outside the scope, the ENTIRE operation FAILS.
5. DO NOT refactor, optimize, or "improve" any code outside the issue locations.
6. DO NOT rename variables, reformat code, or reorganize imports outside the scope.
7. DO NOT change indentation unless necessary for the fix.
8. DO NOT add or remove comments unrelated to the issues.
"""
        
        # ============================================================
        # DESIGN CHANGE ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ v2.0
        # ã‚³ãƒ³ã‚»ãƒ—ãƒˆ: ã€Œç¦æ­¢ã‚ˆã‚ŠæŒ‡ç¤ºã€ã€Œå…·ä½“çš„ã«1ã¤ã ã‘ã€ã€Œå·®åˆ†å‡ºåŠ›ã€
        # ============================================================
        prompt = f"""
{scope_enforcement}
{framework_constraint}
{dependency_section}
===== YOUR TASK =====
Fix the issues below. Output ONLY a unified diff.

===== ISSUES TO FIX =====
{issues_text}

===== SUCCESS DEFINITION =====
Your fix is SUCCESSFUL if:
âœ“ Each issue is fixed with the MINIMUM possible change
âœ“ You output ONLY a diff (not the full file)
âœ“ Your diff is 30 lines or less

Your fix is a FAILURE if:
âœ— You changed anything not required by the issues
âœ— You output more than the diff
âœ— Your diff exceeds 30 lines

A SMALL fix is BETTER than a COMPREHENSIVE fix.

===== WRONG vs RIGHT EXAMPLES =====

Example Issue: "playTrack() does not validate track.audio_url"

âŒ WRONG (too much):
```diff
@@ -45,8 +45,15 @@
 playTrack(track) {{
+    // Validate track data before playing
+    if (!track || typeof track !== 'object') {{
+        console.error('Invalid track object:', track);
+        return false;
+    }}
+    if (!track.audio_url || typeof track.audio_url !== 'string') {{
+        console.warn('Missing or invalid audio URL');
+        return false;
+    }}
     this.audio.src = track.audio_url;
```

âœ… RIGHT (minimal):
```diff
@@ -45,6 +45,7 @@
 playTrack(track) {{
+    if (!track?.audio_url) return;
     this.audio.src = track.audio_url;
```

The RIGHT example adds 1 line. The WRONG example adds 9 lines.
We want RIGHT.

===== OUTPUT FORMAT =====
<diff>
```diff
[Your unified diff here - ONLY the changed parts]
[Maximum 30 lines]
```
</diff>

If fix requires more than 30 diff lines, output instead:
<abort>
SCOPE_EXCEEDED: Fix requires [N] lines, limit is 30.
Issues that CAN be fixed within limit: [list or "none"]
</abort>

===== FILE CONTENT =====
File: {filepath} ({content_lines} lines)
```
{content}
```"""

        try:
            # v2.1: å·®åˆ†å‡ºåŠ›ãªã®ã§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å‰Šæ¸›
            max_tokens = 4000
            
            logger.debug(f"[DESIGN CHANGE v2.1] Sending request for {filepath}")
            
            response = self.llm_manager.generate_response(
                prompt=prompt,
                system_prompt="You are a precise code editor. Output ONLY unified diff format. Minimal changes only.",
                max_tokens=max_tokens
            )
            
            response_text = response.content if hasattr(response, 'content') else str(response)
            
            # ============================================================
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‘ãƒ¼ã‚¹ï¼ˆv2.1ä»•æ§˜ - å·®åˆ†å½¢å¼ï¼‰
            # ============================================================
            
            # 1. Abortæ¤œå‡ºï¼ˆSCOPE_EXCEEDEDï¼‰
            if '<abort>' in response_text.lower() or 'SCOPE_EXCEEDED' in response_text:
                abort_reason_match = re.search(r'SCOPE_EXCEEDED[:\s]*(.+?)(?:\n|$)', response_text)
                abort_reason = abort_reason_match.group(1).strip() if abort_reason_match else "Scope exceeded"
                logger.debug(f"[DESIGN CHANGE v2.1] {filepath}: ABORT - {abort_reason}")
                return f"SOFT_ABORT:{abort_reason}"
            
            # 2. å·®åˆ†æŠ½å‡º
            diff_match = re.search(r'<diff>\s*```diff\s*(.*?)\s*```\s*</diff>', response_text, re.DOTALL)
            if not diff_match:
                # <diff>ã‚¿ã‚°ãªã—ã§ã‚‚```diff```ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ¢ã™
                diff_match = re.search(r'```diff\s*(.*?)\s*```', response_text, re.DOTALL)
            
            if diff_match:
                diff_content = diff_match.group(1).strip()
                diff_lines = diff_content.split('\n')
                
                # å·®åˆ†è¡Œæ•°ãƒã‚§ãƒƒã‚¯ï¼ˆ30è¡ŒæŒ‡ç¤ºã€50è¡Œå—ã‘å…¥ã‚Œ - ãƒãƒƒãƒ•ã‚¡ä»˜ãåˆ¶é™ v2.1ï¼‰
                if len(diff_lines) > 50:
                    logger.debug(f"[DESIGN CHANGE v2.1] {filepath}: Diff too large ({len(diff_lines)} lines, limit=50)")
                    return f"SOFT_ABORT:Diff exceeded limit ({len(diff_lines)} lines, limit=50)"
                
                # å·®åˆ†ã‚’å…ƒã®ã‚³ãƒ¼ãƒ‰ã«é©ç”¨ï¼ˆSICæ¤œè¨¼ä»˜ãï¼‰
                try:
                    fixed_content = self._apply_unified_diff(content, diff_content)
                    if fixed_content and fixed_content != content:
                        # ============================================================
                        # SIC: Structural Integrity Checkï¼ˆtest19ãƒã‚°å¯¾ç­–ï¼‰
                        # ============================================================
                        sic_ok, sic_error = self._structural_integrity_check(content, fixed_content, filepath)
                        
                        if not sic_ok:
                            logger.debug(f"[SIC] {filepath}: REJECTED - {sic_error}")
                            
                            # ============================================================
                            # LLMã¸ã®è‡ªå‹•ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å†è©¦è¡Œï¼ˆv2.0: 3è€…æ¯”è¼ƒæ–¹å¼ï¼‰
                            # ============================================================
                            retry_content = self._retry_with_sic_feedback(
                                filepath=filepath,
                                original_code=content,      # DNA: ä¿®æ­£å‰ã®æ­£ã—ã„ã‚³ãƒ¼ãƒ‰
                                failed_code=fixed_content,  # FAILURE: SICã§å¼¾ã‹ã‚ŒãŸã‚³ãƒ¼ãƒ‰
                                issues=issues,
                                sic_error=sic_error,
                                goal=goal
                            )
                            if retry_content:
                                # å†è©¦è¡Œçµæœã‚‚SICã§æ¤œè¨¼
                                retry_sic_ok, retry_sic_error = self._structural_integrity_check(content, retry_content, filepath)
                                if retry_sic_ok:
                                    logger.debug(f"[SIC Retry] {filepath}: Retry succeeded - SIC passed")
                                    return retry_content
                                else:
                                    logger.debug(f"[SIC Retry] {filepath}: Retry also failed - {retry_sic_error}")
                            
                            logger.debug(f"[SIC] {filepath}: Rolling back diff, using fallback")
                            # diffã‚’å´ä¸‹ã—ã¦ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¸
                        else:
                            logger.debug(f"[DESIGN CHANGE v2.1] {filepath}: Applied diff ({len(diff_lines)} lines) - SIC passed")
                            return fixed_content
                    else:
                        logger.debug(f"[DESIGN CHANGE v2.1] {filepath}: Diff application failed or no change")
                except Exception as e:
                    logger.debug(f"[DESIGN CHANGE v2.1] {filepath}: Diff parse error: {e}")
            
            # 3. ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®å®Œå…¨ã‚³ãƒ¼ãƒ‰æŠ½å‡º
            fixed_files = self._parse_llm_response(response_text)
            
            if fixed_files:
                logger.debug(f"[DESIGN CHANGE v2.1] {filepath}: Fallback - extracted {len(fixed_files)} file(s)")
                normalized_filepath = filepath.replace('\\', '/')
                for fpath, code in fixed_files.items():
                    normalized_fpath = fpath.replace('\\', '/')
                    if normalized_fpath == normalized_filepath or fpath == filepath:
                        return code
                    if fpath.endswith(filepath.split('/')[-1].split('\\')[-1]):
                        return code
                if len(fixed_files) == 1:
                    return list(fixed_files.values())[0]
            
            logger.debug(f"[DESIGN CHANGE v2.1] {filepath}: No valid output extracted")
            return None
            
        except Exception as e:
            logger.debug(f"[DESIGN CHANGE v2.1] Error for {filepath}: {e}")
            return None
    
    def _apply_unified_diff(self, original: str, diff: str) -> Optional[str]:
        """
        Unified diffå½¢å¼ã®å·®åˆ†ã‚’å…ƒã®ã‚³ãƒ¼ãƒ‰ã«é©ç”¨ã™ã‚‹
        
        Args:
            original: å…ƒã®ã‚³ãƒ¼ãƒ‰
            diff: unified diffå½¢å¼ã®å·®åˆ†
            
        Returns:
            é©ç”¨å¾Œã®ã‚³ãƒ¼ãƒ‰ã€ã¾ãŸã¯å¤±æ•—æ™‚None
        """
        try:
            original_lines = original.split('\n')
            result_lines = original_lines.copy()
            
            # å·®åˆ†ã®ãƒãƒ³ã‚¯ã‚’è§£æ
            hunk_pattern = r'@@ -(\d+)(?:,(\d+))? \+(\d+)(?:,(\d+))? @@'
            
            diff_lines = diff.split('\n')
            
            # ã™ã¹ã¦ã®ãƒãƒ³ã‚¯ã‚’å…ˆã«è§£æ
            hunks = []
            i = 0
            while i < len(diff_lines):
                line = diff_lines[i]
                hunk_match = re.match(hunk_pattern, line)
                if hunk_match:
                    old_start = int(hunk_match.group(1))  # 1-indexed
                    old_count = int(hunk_match.group(2)) if hunk_match.group(2) else 1
                    new_start = int(hunk_match.group(3))  # 1-indexed
                    new_count = int(hunk_match.group(4)) if hunk_match.group(4) else 1
                    
                    i += 1
                    hunk_lines = []
                    
                    # ãƒãƒ³ã‚¯ã®å†…å®¹ã‚’åé›†
                    while i < len(diff_lines) and not diff_lines[i].startswith('@@'):
                        hunk_line = diff_lines[i]
                        # diffã®è¡Œã¯ ' ', '-', '+' ã§å§‹ã¾ã‚‹ï¼ˆã¾ãŸã¯ç©ºè¡Œï¼‰
                        if hunk_line.startswith((' ', '-', '+')) or hunk_line == '':
                            hunk_lines.append(hunk_line)
                        i += 1
                    
                    hunks.append({
                        'old_start': old_start,
                        'old_count': old_count,
                        'new_start': new_start,
                        'new_count': new_count,
                        'lines': hunk_lines
                    })
                else:
                    i += 1
            
            if not hunks:
                logger.debug("[Diff Apply] No hunks found in diff")
                return None
            
            # ãƒãƒ³ã‚¯ã‚’é€†é †ã«é©ç”¨ï¼ˆå¾Œã‚ã‹ã‚‰é©ç”¨ã™ã‚‹ã“ã¨ã§offsetè¨ˆç®—ãŒä¸è¦ï¼‰
            hunks.sort(key=lambda h: h['old_start'], reverse=True)
            
            for hunk in hunks:
                old_start = hunk['old_start'] - 1  # 0-indexed
                hunk_lines = hunk['lines']
                
                # ã“ã®ãƒãƒ³ã‚¯ã§å‰Šé™¤ã•ã‚Œã‚‹è¡Œã¨è¿½åŠ ã•ã‚Œã‚‹è¡Œã‚’åˆ†é›¢
                lines_to_delete = 0
                new_lines = []
                
                for hunk_line in hunk_lines:
                    if hunk_line.startswith('-'):
                        lines_to_delete += 1
                    elif hunk_line.startswith('+'):
                        new_lines.append(hunk_line[1:])  # '+' ã‚’é™¤å»
                    elif hunk_line.startswith(' '):
                        new_lines.append(hunk_line[1:])  # ' ' ã‚’é™¤å»
                    elif hunk_line == '':
                        # ç©ºã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¡Œï¼ˆå…ƒã®ã‚³ãƒ¼ãƒ‰ã®ç©ºè¡Œï¼‰
                        new_lines.append('')
                
                # å¤ã„è¡Œã‚’å‰Šé™¤ã—ã¦æ–°ã—ã„è¡Œã‚’æŒ¿å…¥
                # old_start ã‹ã‚‰ old_count è¡Œã‚’å‰Šé™¤
                old_count = hunk['old_count']
                
                # ç¯„å›²ãƒã‚§ãƒƒã‚¯
                if old_start < 0 or old_start > len(result_lines):
                    logger.debug(f"[Diff Apply] Invalid hunk start: {old_start + 1} (file has {len(result_lines)} lines)")
                    return None
                
                # å‰Šé™¤ã¨æŒ¿å…¥ã‚’ä¸€åº¦ã«è¡Œã†
                end_index = min(old_start + old_count, len(result_lines))
                result_lines[old_start:end_index] = new_lines
            
            result = '\n'.join(result_lines)
            
            # å¤‰æ›´ãŒã‚ã£ãŸã‹ç¢ºèª
            if result == original:
                logger.debug("[Diff Apply] No changes detected after applying diff")
                return None
            
            return result
            
        except Exception as e:
            logger.debug(f"[Diff Apply] Error applying diff: {e}")
            return None

    # ============================================================
    # Structural Integrity Check (SIC) - test19ãƒã‚°å¯¾ç­–
    # ============================================================
    
    def _get_structural_snapshot(self, code: str, filepath: str) -> Optional[Dict[str, Any]]:
        """
        ã‚³ãƒ¼ãƒ‰ã®æ§‹é€ ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’å–å¾—
        
        Args:
            code: ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰
            filepath: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆ.pyã®ã¿å¯¾è±¡ï¼‰
            
        Returns:
            æ§‹é€ æƒ…å ±ã®è¾æ›¸ ã¾ãŸã¯ None
        """
        if not filepath.endswith('.py'):
            return None
        
        try:
            tree = ast.parse(code)
        except SyntaxError as e:
            logger.debug(f"[SIC] Syntax error in {filepath}: {e}")
            return None
        
        class_count = 0
        function_count = 0
        class_names = []
        function_names = []
        top_level_names = []
        top_level_typed = set()  # å‹ãƒ’ãƒ³ãƒˆä»˜ãã®ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«å¤‰æ•°
        class_attributes = {}
        typed_attributes = {}  # ã‚¯ãƒ©ã‚¹å -> å‹ãƒ’ãƒ³ãƒˆä»˜ãå±æ€§ã®set
        
        for node in tree.body:
            if isinstance(node, ast.ClassDef):
                class_count += 1
                class_names.append(node.name)
                
                # ã‚¯ãƒ©ã‚¹å†…ã®å±æ€§ã‚’åé›†
                attrs = []
                typed_attrs = set()
                for item in node.body:
                    if isinstance(item, ast.AnnAssign) and isinstance(item.target, ast.Name):
                        attrs.append(item.target.id)
                        typed_attrs.add(item.target.id)  # å‹ãƒ’ãƒ³ãƒˆä»˜ã
                    elif isinstance(item, ast.Assign):
                        for target in item.targets:
                            if isinstance(target, ast.Name):
                                attrs.append(target.id)
                                # Assignã¯å‹ãƒ’ãƒ³ãƒˆãªã—
                class_attributes[node.name] = attrs
                typed_attributes[node.name] = typed_attrs
                
            elif isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                function_count += 1
                function_names.append(node.name)
                
            elif isinstance(node, ast.AnnAssign) and isinstance(node.target, ast.Name):
                top_level_names.append(node.target.id)
                top_level_typed.add(node.target.id)  # å‹ãƒ’ãƒ³ãƒˆä»˜ã
                
            elif isinstance(node, ast.Assign):
                for target in node.targets:
                    if isinstance(target, ast.Name):
                        top_level_names.append(target.id)
                        # Assignã¯å‹ãƒ’ãƒ³ãƒˆãªã—
        
        return {
            'class_count': class_count,
            'function_count': function_count,
            'class_names': class_names,
            'function_names': function_names,
            'top_level_names': top_level_names,
            'top_level_typed': top_level_typed,
            'class_attributes': class_attributes,
            'typed_attributes': typed_attributes
        }

    def _structural_integrity_check(
        self,
        original: str,
        modified: str,
        filepath: str
    ) -> Tuple[bool, Optional[str]]:
        """
        Structural Integrity Check (SIC)
        
        diffé©ç”¨å¾Œã®ã‚³ãƒ¼ãƒ‰ãŒæ§‹é€ çš„ã«å¥å…¨ã‹ãƒã‚§ãƒƒã‚¯
        
        Args:
            original: å…ƒã®ã‚³ãƒ¼ãƒ‰
            modified: ä¿®æ­£å¾Œã®ã‚³ãƒ¼ãƒ‰
            filepath: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            
        Returns:
            (æˆåŠŸ, ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸)
        """
        # HTML/JSãƒ•ã‚¡ã‚¤ãƒ«ç”¨ã®ãƒã‚§ãƒƒã‚¯
        if filepath.endswith(('.html', '.htm', '.js', '.jsx', '.ts', '.tsx')):
            return self._structural_integrity_check_html_js(original, modified, filepath)
        
        if not filepath.endswith('.py'):
            return (True, None)
        
        before = self._get_structural_snapshot(original, filepath)
        after = self._get_structural_snapshot(modified, filepath)
        
        if before is None:
            logger.debug(f"[SIC] {filepath}: Original code unparseable, skipping")
            return (True, None)
        
        if after is None:
            return (False, "Syntax error after diff application")
        
        # Check 1: Symbol Count
        if after['class_count'] < before['class_count']:
            return (False, f"Class count decreased: {before['class_count']} -> {after['class_count']}")
        
        if after['function_count'] < before['function_count'] - 1:
            return (False, f"Function count significantly decreased: {before['function_count']} -> {after['function_count']}")
        
        # Check 2: Top-level Pollution
        for class_name, attrs in before['class_attributes'].items():
            for attr in attrs:
                if attr in after['top_level_names'] and attr not in before['top_level_names']:
                    return (False, f"Attribute '{attr}' leaked from class '{class_name}' to top-level")
        
        # Check 3: Attribute Drift
        CRITICAL_ATTRS = ['DATABASE_URL', 'CORS_ORIGINS', 'SECRET_KEY', 'API_KEY', 'STATIC_DIR']
        
        for class_name in before['class_names']:
            if class_name in after['class_attributes']:
                before_attrs = set(before['class_attributes'].get(class_name, []))
                after_attrs = set(after['class_attributes'].get(class_name, []))
                
                missing_critical = before_attrs - after_attrs
                for attr in missing_critical:
                    if attr in CRITICAL_ATTRS:
                        return (False, f"Critical attribute '{attr}' missing from class '{class_name}'")
        
        # Check 4: Class Name Preservation
        for class_name in before['class_names']:
            if class_name not in after['class_names']:
                return (False, f"Class '{class_name}' disappeared after diff")
        
        # Check 5: Type Annotation Driftï¼ˆå‹ãƒ’ãƒ³ãƒˆã®æ¶ˆå¤±æ¤œçŸ¥ï¼‰
        # AnnAssignï¼ˆå‹ä»˜ãï¼‰ãŒAssignï¼ˆå‹ãªã—ï¼‰ã«å¤‰ã‚ã£ã¦ã„ãªã„ã‹
        for class_name in before['class_names']:
            # ã‚¯ãƒ©ã‚¹ãŒæ¶ˆãˆãŸå ´åˆã¯Check 4ã§æ¤œçŸ¥ã•ã‚Œã‚‹ã®ã§ã‚¹ã‚­ãƒƒãƒ—
            if class_name not in after['class_names']:
                continue
            
            before_typed = before.get('typed_attributes', {}).get(class_name, set())
            after_typed = after.get('typed_attributes', {}).get(class_name, set())
            
            # å…ƒã€…å‹ãƒ’ãƒ³ãƒˆä»˜ãã ã£ãŸå±æ€§ãŒå‹ãªã—ã«ãªã£ã¦ã„ãªã„ã‹
            lost_typing = before_typed - after_typed
            # ãŸã ã—ã€å±æ€§è‡ªä½“ãŒæ¶ˆãˆãŸå ´åˆã¯Check 3ã§æ¤œçŸ¥æ¸ˆã¿ãªã®ã§é™¤å¤–
            after_attrs = set(after.get('class_attributes', {}).get(class_name, []))
            lost_typing_still_exists = lost_typing & after_attrs
            
            if lost_typing_still_exists:
                return (False, f"Type annotations lost in class '{class_name}': {lost_typing_still_exists}")
        
        logger.debug(f"[SIC] {filepath}: All checks passed")
        return (True, None)
    
    def _structural_integrity_check_html_js(
        self,
        original: str,
        modified: str,
        filepath: str
    ) -> Tuple[bool, Optional[str]]:
        """
        HTML/JavaScriptç”¨ã®Structural Integrity Check
        
        ãƒã‚§ãƒƒã‚¯é …ç›®:
        1. æ‹¬å¼§ã®ãƒãƒ©ãƒ³ã‚¹ï¼ˆ{}, (), []ï¼‰
        2. HTMLã‚¿ã‚°ã®ãƒãƒ©ãƒ³ã‚¹ï¼ˆ<script>, </script>ãªã©ï¼‰
        3. class/functionå®šç¾©ã®æ•°ãŒæ¸›ã£ã¦ã„ãªã„ã‹
        4. ç•°å¸¸ãªæ§‹æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
        
        Args:
            original: å…ƒã®ã‚³ãƒ¼ãƒ‰
            modified: ä¿®æ­£å¾Œã®ã‚³ãƒ¼ãƒ‰
            filepath: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            
        Returns:
            (æˆåŠŸ, ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸)
        """
        # Check 1: æ‹¬å¼§ã®ãƒãƒ©ãƒ³ã‚¹
        def count_brackets(code: str) -> Dict[str, int]:
            # æ–‡å­—åˆ—ãƒªãƒ†ãƒ©ãƒ«å†…ã®æ‹¬å¼§ã‚’é™¤å¤–ã™ã‚‹ãŸã‚ã®ç°¡æ˜“å‡¦ç†
            # å®Œå…¨ã§ã¯ãªã„ãŒã€å¤§ããªæ§‹é€ ç ´å£Šã¯æ¤œå‡ºã§ãã‚‹
            counts = {'{': 0, '}': 0, '(': 0, ')': 0, '[': 0, ']': 0}
            in_string = False
            string_char = None
            prev_char = ''
            
            for char in code:
                if not in_string:
                    if char in ('"', "'", '`'):
                        in_string = True
                        string_char = char
                    elif char in counts:
                        counts[char] += 1
                else:
                    if char == string_char and prev_char != '\\':
                        in_string = False
                        string_char = None
                prev_char = char
            
            return counts
        
        orig_brackets = count_brackets(original)
        mod_brackets = count_brackets(modified)
        
        # æ‹¬å¼§ã®ãƒãƒ©ãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯
        orig_balance = {
            'braces': orig_brackets['{'] - orig_brackets['}'],
            'parens': orig_brackets['('] - orig_brackets[')'],
            'squares': orig_brackets['['] - orig_brackets[']']
        }
        mod_balance = {
            'braces': mod_brackets['{'] - mod_brackets['}'],
            'parens': mod_brackets['('] - mod_brackets[')'],
            'squares': mod_brackets['['] - mod_brackets[']']
        }
        
        # ä¿®æ­£å¾Œã«ãƒãƒ©ãƒ³ã‚¹ãŒå´©ã‚ŒãŸå ´åˆï¼ˆå…ƒãŒæ­£å¸¸ã ã£ãŸå ´åˆï¼‰
        if mod_balance['braces'] != 0 and orig_balance['braces'] == 0:
            return (False, f"Brace balance broken: {'{' * abs(mod_balance['braces'])} {'missing }' if mod_balance['braces'] > 0 else 'extra }'}")
        if mod_balance['parens'] != 0 and orig_balance['parens'] == 0:
            return (False, f"Parenthesis balance broken: {'(' * abs(mod_balance['parens'])} {'missing )' if mod_balance['parens'] > 0 else 'extra )'}")
        if mod_balance['squares'] != 0 and orig_balance['squares'] == 0:
            return (False, f"Bracket balance broken: {'[' * abs(mod_balance['squares'])} {'missing ]' if mod_balance['squares'] > 0 else 'extra ]'}")
        
        # ğŸ†• è¿½åŠ : ä¿®æ­£å¾Œã«ãƒãƒ©ãƒ³ã‚¹ãŒæ‚ªåŒ–ã—ãŸå ´åˆã‚‚æ¤œå‡º
        if abs(mod_balance['braces']) > abs(orig_balance['braces']):
            return (False, f"Brace balance worsened: {orig_balance['braces']} -> {mod_balance['braces']}")
        if abs(mod_balance['parens']) > abs(orig_balance['parens']):
            return (False, f"Parenthesis balance worsened: {orig_balance['parens']} -> {mod_balance['parens']}")
        if abs(mod_balance['squares']) > abs(orig_balance['squares']):
            return (False, f"Bracket balance worsened: {orig_balance['squares']} -> {mod_balance['squares']}")
        
        # Check 2: class/functionå®šç¾©ã®æ•°
        def count_definitions(code: str) -> Dict[str, int]:
            # JavaScriptã®classå®šç¾©
            class_count = len(re.findall(r'\bclass\s+\w+', code))
            # functionå®šç¾©ï¼ˆé€šå¸¸ã®functionã¨çŸ¢å°é–¢æ•°ã¯åˆ¥ã€…ã«ã‚«ã‚¦ãƒ³ãƒˆï¼‰
            function_count = len(re.findall(r'\bfunction\s+\w+', code))
            # ãƒ¡ã‚½ãƒƒãƒ‰å®šç¾©ï¼ˆclasså†…ï¼‰
            method_count = len(re.findall(r'^\s+\w+\s*\([^)]*\)\s*\{', code, re.MULTILINE))
            
            return {
                'classes': class_count,
                'functions': function_count,
                'methods': method_count
            }
        
        orig_defs = count_definitions(original)
        mod_defs = count_definitions(modified)
        
        # ã‚¯ãƒ©ã‚¹æ•°ãŒæ¸›å°‘ã—ãŸå ´åˆ
        if mod_defs['classes'] < orig_defs['classes']:
            return (False, f"Class count decreased: {orig_defs['classes']} -> {mod_defs['classes']}")
        
        # é–¢æ•°æ•°ãŒå¤§å¹…ã«æ¸›å°‘ã—ãŸå ´åˆï¼ˆ1ã¤ã¾ã§ã¯è¨±å®¹ï¼‰
        if mod_defs['functions'] < orig_defs['functions'] - 1:
            return (False, f"Function count significantly decreased: {orig_defs['functions']} -> {mod_defs['functions']}")
        
        # Check 3: HTMLã‚¿ã‚°ã®ãƒãƒ©ãƒ³ã‚¹ï¼ˆHTMLãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ï¼‰
        if filepath.endswith(('.html', '.htm')):
            important_tags = ['script', 'style', 'head', 'body', 'html']
            for tag in important_tags:
                orig_open = len(re.findall(rf'<{tag}[\s>]', original, re.IGNORECASE))
                orig_close = len(re.findall(rf'</{tag}>', original, re.IGNORECASE))
                mod_open = len(re.findall(rf'<{tag}[\s>]', modified, re.IGNORECASE))
                mod_close = len(re.findall(rf'</{tag}>', modified, re.IGNORECASE))
                
                # å…ƒã€…ãƒãƒ©ãƒ³ã‚¹ãŒå–ã‚Œã¦ã„ãŸã®ã«å´©ã‚ŒãŸå ´åˆ
                if orig_open == orig_close and mod_open != mod_close:
                    return (False, f"HTML tag <{tag}> balance broken: {mod_open} opens, {mod_close} closes")
        
        # Check 4: ç•°å¸¸ãªæ§‹æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
        # ãƒ¡ã‚½ãƒƒãƒ‰å®šç¾©ã®é€”ä¸­ã«åˆ¥ã®ã‚³ãƒ¼ãƒ‰ãŒæ··å…¥ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
        # ä¾‹: update() { ... const x = 1; draw() { ...
        anomaly_pattern = r'\b(update|render|draw|init|constructor)\s*\([^)]*\)\s*\{[^}]*\b(const|let|var)\s+\w+\s*=\s*[^;]+;\s*\n\s*(update|render|draw|init|constructor|class)\s*[\s(]'
        if re.search(anomaly_pattern, modified):
            return (False, "Code structure anomaly detected: method definition appears to be corrupted")
        
        logger.debug(f"[SIC-HTML/JS] {filepath}: All checks passed")
        return (True, None)

    def _retry_with_sic_feedback(
        self,
        filepath: str,
        original_code: str,
        failed_code: str,
        issues: List[Dict[str, Any]],
        sic_error: str,
        goal: str
    ) -> Optional[str]:
        """
        SIC Rejectæ™‚ã«LLMã«ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã—ã¦å†è©¦è¡Œï¼ˆv2.0: 3è€…æ¯”è¼ƒæ–¹å¼ï¼‰
        
        æ”¹å–„ç‚¹:
        1. 3è€…æ¯”è¼ƒ: Originalï¼ˆDNAï¼‰ + Failedï¼ˆFAILUREï¼‰ + Diagnosis
        2. å¾©å…ƒã¨ä¿®æ­£ã®åˆ†é›¢: ã€Œã¾ãšORIGINALã«æˆ»ã›ã€æŒ‡ç¤º
        3. å…·ä½“çš„è¨ºæ–­: ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—åˆ¥ã®è©³ç´°ä¾‹ç¤º
        
        Args:
            filepath: å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            original_code: ä¿®æ­£å‰ã®æ­£ã—ã„ã‚³ãƒ¼ãƒ‰ï¼ˆDNAï¼‰
            failed_code: SICã§å¼¾ã‹ã‚ŒãŸã‚³ãƒ¼ãƒ‰ï¼ˆFAILUREï¼‰
            issues: ä¿®æ­£å¯¾è±¡ã®Issueãƒªã‚¹ãƒˆ
            sic_error: SICã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            goal: å…ƒã®ã‚´ãƒ¼ãƒ«
            
        Returns:
            ä¿®æ­£å¾Œã®ã‚³ãƒ¼ãƒ‰ã€ã¾ãŸã¯å¤±æ•—æ™‚None
        """
        # llm_managerã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        if not self.llm_manager:
            logger.debug(f"[SIC Retry v2.0] {filepath}: LLM manager not available")
            return None
        
        logger.debug(f"[SIC Retry v2.0] {filepath}: Initiating 3-way comparison recovery")
        
        # Issueæƒ…å ±ã‚’æ•´å½¢
        issues_for_file = [i for i in issues if i.get('file') == filepath]
        issues_text = ""
        for i, issue in enumerate(issues_for_file[:3], 1):
            issues_text += f"{i}. {issue.get('description', 'No description')}\n"
        
        # ã‚¨ãƒ©ãƒ¼ã‚’æ§‹é€ åŒ–
        structured_error = self._parse_sic_error(sic_error)
        
        # è¨ºæ–­ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆ
        diagnosis = self._generate_sic_diagnosis(
            structured_error,
            original_code,
            failed_code
        )
        
        # 3è€…æ¯”è¼ƒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
        prompt = f"""=== STRUCTURAL INTEGRITY CHECK (SIC) RECOVERY REQUEST ===

You attempted to fix a file, but your output BROKE the code's structural integrity.
You must RECOVER the structure while still applying the necessary fix.

---

## [DNA: ORIGINAL]
This is the STRUCTURALLY CORRECT code BEFORE your modification.
Your new output MUST preserve this exact structure (classes, indentation, attribute locations).

```
{original_code}
```

---

## [FAILURE: YOUR PREVIOUS OUTPUT]
This is YOUR broken output that failed SIC validation.
DO NOT use this as a template. It has structural defects.

```
{failed_code}
```

---

## [DIAGNOSIS: SIC ERROR]
Why your output was rejected:

**Error Type:** {diagnosis["error_type"]}
**Details:** {diagnosis["details"]}

{diagnosis["examples"]}

---

## [MISSION: SURGICAL RECOVERY]

### PROCEDURE (Follow in order):

1. **REVERT FIRST**
   - Start with [DNA: ORIGINAL] as your base template
   - This code has the CORRECT structural integrity
   - Copy its exact indentation pattern

2. **SURGICAL RE-FIX**
   - Apply ONLY the fix for: {issues_text if issues_text else 'the previously listed issues'}
   - Do NOT touch any other lines
   - Do NOT reorganize, refactor, or "improve" anything else

3. **INDENTATION LOCK**
   - Class attributes MUST remain inside their class (indented with 4 spaces)
   - Top-level code MUST remain at column 0
   - Match [DNA: ORIGINAL] indentation exactly

4. **SCOPE CHECK (Before submitting)**
   - Verify: Are all class attributes still INSIDE their class?
   - Verify: Did any class variables leak to module level?
   - Verify: Are all original classes/functions still present?

### CRITICAL CONSTRAINTS

âŒ DO NOT: Remove any existing classes or functions
âŒ DO NOT: Move class attributes to module level (outside class)
âŒ DO NOT: Change indentation patterns
âŒ DO NOT: Add "improvements" not related to the original issue
âŒ DO NOT: Fix PEP8/style issues (focus on functionality only)

âœ… DO: Preserve exact structure from [DNA: ORIGINAL]
âœ… DO: Fix only the specific issue mentioned
âœ… DO: Return the complete file content

### OUTPUT FORMAT

You MUST use this exact format (the system parses <file> tags):

<file path="{filepath}">
<your complete corrected code here - no markdown code blocks inside>
</file>
"""

        try:
            response = self.llm_manager.generate_response(
                prompt=prompt,
                system_prompt=(
                    "You are a SURGICAL CODE RECOVERY SPECIALIST. "
                    "Your job is to RESTORE structural integrity while applying minimal fixes. "
                    "NEVER reorganize, refactor, or 'improve' code beyond the specific fix requested. "
                    "Pay EXTREME attention to Python indentation - it determines code structure."
                ),
                max_tokens=8000
            )
            
            response_text = response.content if hasattr(response, 'content') else str(response)
            
            # ã‚³ãƒ¼ãƒ‰æŠ½å‡º
            fixed_files = self._parse_llm_response(response_text)
            
            if fixed_files:
                for fpath, code in fixed_files.items():
                    if fpath == filepath or fpath.endswith(filepath.split('/')[-1]):
                        logger.debug(f"[SIC Retry v2.0] {filepath}: Extracted recovered code ({len(code)} chars)")
                        return code
                if len(fixed_files) == 1:
                    return list(fixed_files.values())[0]
            
            logger.debug(f"[SIC Retry v2.0] {filepath}: No valid code extracted from recovery attempt")
            return None
            
        except Exception as e:
            logger.debug(f"[SIC Retry v2.0] {filepath}: Recovery failed - {e}")
            return None

    def _parse_sic_error(self, sic_error: str) -> SICError:
        """
        SICã‚¨ãƒ©ãƒ¼æ–‡å­—åˆ—ã‚’æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›
        
        Args:
            sic_error: SICã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            
        Returns:
            SICError ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹
        """
        # ã‚¬ãƒ¼ãƒ‰ç¯€: None/ç©ºæ–‡å­—ã®å ´åˆ
        if not sic_error:
            return SICError(
                error_type="UNKNOWN",
                affected_class=None,
                affected_items=[],
                human_readable="Unknown SIC error"
            )
        
        error_type = "UNKNOWN"
        affected_class = None
        affected_items = []
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã§ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
        if "leaked" in sic_error.lower() and "top-level" in sic_error.lower():
            error_type = "TOP_LEVEL_POLLUTION"
            match = re.search(r"Attribute '(\w+)' leaked from class '(\w+)'", sic_error)
            if match:
                affected_items = [match.group(1)]
                affected_class = match.group(2)
                
        elif "count" in sic_error.lower() and "decreased" in sic_error.lower():
            error_type = "SYMBOL_DELETION"
            if "class" in sic_error.lower():
                error_type = "CLASS_DELETION"
            elif "function" in sic_error.lower():
                error_type = "FUNCTION_DELETION"
                
        elif "missing" in sic_error.lower() and "class" in sic_error.lower():
            error_type = "ATTRIBUTE_DRIFT"
            match = re.search(r"attribute '(\w+)' missing from class '(\w+)'", sic_error, re.IGNORECASE)
            if match:
                affected_items = [match.group(1)]
                affected_class = match.group(2)
                
        elif "disappeared" in sic_error.lower():
            error_type = "CLASS_DELETION"
            match = re.search(r"Class '(\w+)' disappeared", sic_error)
            if match:
                affected_class = match.group(1)
                
        elif "type" in sic_error.lower() and ("lost" in sic_error.lower() or "annotation" in sic_error.lower()):
            error_type = "TYPE_ANNOTATION_DRIFT"
            
        elif "syntax" in sic_error.lower():
            error_type = "SYNTAX_ERROR"
        
        return SICError(
            error_type=error_type,
            affected_class=affected_class,
            affected_items=affected_items,
            human_readable=sic_error
        )

    def _generate_sic_diagnosis(
        self,
        error: SICError,
        original_code: str,
        failed_code: str
    ) -> Dict[str, str]:
        """
        SICã‚¨ãƒ©ãƒ¼ã‹ã‚‰å…·ä½“çš„ãªè¨ºæ–­ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆ
        
        Args:
            error: æ§‹é€ åŒ–ã•ã‚ŒãŸSICã‚¨ãƒ©ãƒ¼
            original_code: ä¿®æ­£å‰ã®ã‚³ãƒ¼ãƒ‰
            failed_code: SICã§å¼¾ã‹ã‚ŒãŸã‚³ãƒ¼ãƒ‰
            
        Returns:
            {"error_type": str, "details": str, "examples": str}
        """
        error_type = error.error_type
        details = error.human_readable
        examples = ""
        
        if error_type == "TOP_LEVEL_POLLUTION":
            attr = error.affected_items[0] if error.affected_items else "ATTRIBUTE"
            cls = error.affected_class or "CLASS"
            
            details = (
                f"Attribute '{attr}' was INSIDE class '{cls}' in [DNA: ORIGINAL], "
                f"but YOUR output moved it to module level (top-level). "
                f"This BREAKS the class structure and will cause AttributeError at runtime."
            )
            
            examples = f"""**What went wrong:**

In [DNA: ORIGINAL] (CORRECT):
```
class {cls}:
    {attr} = ...  # â† Correctly INSIDE the class (4-space indent)
```

In [FAILURE: YOUR OUTPUT] (WRONG):
```
{attr} = ...  # â† WRONG: At module level (no indent)
class {cls}:
    ...  # â† The attribute is missing from here!
```

**How to fix:**
Keep ALL class attributes INSIDE their class definition with proper indentation (4 spaces).
"""

        elif error_type == "CLASS_DELETION":
            cls = error.affected_class or "SOME_CLASS"
            
            details = (
                f"Class '{cls}' exists in [DNA: ORIGINAL] but is MISSING from your output. "
                f"You accidentally deleted or renamed it."
            )
            
            examples = f"""**What went wrong:**

[DNA: ORIGINAL] has:
```
class {cls}:
    ...
```

[FAILURE: YOUR OUTPUT] is MISSING this class entirely.

**How to fix:**
Make sure ALL classes from [DNA: ORIGINAL] are present in your output.
"""

        elif error_type == "FUNCTION_DELETION":
            details = (
                f"Functions were deleted from your output. "
                f"The function count significantly decreased compared to [DNA: ORIGINAL]."
            )
            
            examples = """**What went wrong:**

[DNA: ORIGINAL] has several functions defined.
[FAILURE: YOUR OUTPUT] is missing some of these functions.

**How to fix:**
Do NOT remove any existing functions. Preserve all function definitions from [DNA: ORIGINAL].
"""

        elif error_type == "SYMBOL_DELETION":
            details = (
                f"Symbols (classes or functions) were deleted from your output. "
                f"The count significantly decreased compared to [DNA: ORIGINAL]."
            )
            
            examples = """**What went wrong:**

[DNA: ORIGINAL] has a certain number of classes/functions.
[FAILURE: YOUR OUTPUT] has fewer classes/functions.

**How to fix:**
Do NOT remove any existing classes or functions. Preserve all definitions from [DNA: ORIGINAL].
"""

        elif error_type == "ATTRIBUTE_DRIFT":
            attr = error.affected_items[0] if error.affected_items else "ATTRIBUTE"
            cls = error.affected_class or "CLASS"
            
            details = (
                f"Critical attribute '{attr}' exists in class '{cls}' in [DNA: ORIGINAL], "
                f"but is MISSING from your output."
            )
            
            examples = f"""**What went wrong:**

[DNA: ORIGINAL] has:
```
class {cls}:
    {attr} = ...  # â† This attribute exists
```

[FAILURE: YOUR OUTPUT]:
```
class {cls}:
    # {attr} is MISSING!
```

**How to fix:**
Do NOT remove existing attributes. Preserve all attributes from [DNA: ORIGINAL].
"""

        elif error_type == "TYPE_ANNOTATION_DRIFT":
            details = (
                f"Type annotations were lost in your output. "
                f"Variables that had type hints (e.g., `x: str = ...`) were changed to untyped (e.g., `x = ...`)."
            )
            
            examples = """**What went wrong:**

[DNA: ORIGINAL] (CORRECT):
```
DATABASE_URL: str = "sqlite:///..."  # â† Has type hint `: str`
```

[FAILURE: YOUR OUTPUT] (WRONG):
```
DATABASE_URL = "sqlite:///..."  # â† Type hint removed
```

**How to fix:**
Preserve ALL type annotations exactly as they appear in [DNA: ORIGINAL].
"""

        elif error_type == "SYNTAX_ERROR":
            details = (
                f"Your output has a Python syntax error. "
                f"The code cannot be parsed. Check for: missing colons, unmatched brackets, indentation errors."
            )
            
            examples = """**Common syntax errors:**
- Missing `:` after `if`, `for`, `def`, `class`
- Unmatched `(`, `)`, `[`, `]`, `{`, `}`
- Incorrect indentation (mixing tabs and spaces)
- Missing closing quotes in strings

**How to fix:**
Ensure your output is valid Python that can be parsed by `ast.parse()`.
"""

        else:
            details = error.human_readable
            examples = """**How to fix:**
Compare [DNA: ORIGINAL] and [FAILURE: YOUR OUTPUT] carefully.
Identify what structural changes you made, and undo them while keeping only the necessary fix.
"""

        return {
            "error_type": error_type.replace("_", " "),
            "details": details,
            "examples": examples
        }


    def _fix_html_script_reference_for_vanilla_js(
        self,
        all_files: Dict[str, str]
    ) -> Dict[str, str]:
        """
        ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ç¦æ­¢ãƒ¢ãƒ¼ãƒ‰æ™‚ã€index.htmlã®<script>å‚ç…§ã‚’vanilla JSã«ä¿®æ­£
        
        å®‰å…¨ç­–:
        1. _no_framework_mode ãŒ True ã®å ´åˆã®ã¿å®Ÿè¡Œ
        2. script.js ãŒå­˜åœ¨ã—ã€ã‹ã¤ã‚¹ã‚¿ãƒ–ã§ãªã„ã“ã¨ã‚’ç¢ºèª
        3. React/Vue ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.jsx/.tsx/.vueï¼‰ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
        4. ä¿®æ­£å†…å®¹ã‚’ãƒ­ã‚°ã«è¨˜éŒ²
        """
        if not getattr(self, '_no_framework_mode', False):
            return all_files
        
        # script.js ã‚’æ¤œç´¢
        vanilla_js_file = None
        vanilla_js_content = None
        for filename in all_files:
            if filename == 'script.js' or filename.endswith('/script.js'):
                vanilla_js_file = filename
                vanilla_js_content = all_files[filename]
                break
        
        if not vanilla_js_file or not vanilla_js_content:
            logger.debug("[HTML Fix] No script.js found, skipping")
            return all_files
        
        # ã‚¹ã‚¿ãƒ–ãƒã‚§ãƒƒã‚¯: script.js ãŒå®Ÿã‚³ãƒ¼ãƒ‰ã‚’å«ã‚€ã‹ç¢ºèª
        lines = vanilla_js_content.strip().split('\n')
        non_empty_lines = [l for l in lines if l.strip()]
        
        if not non_empty_lines:
            logger.debug("[HTML Fix] script.js is empty, skipping")
            return all_files
        
        comment_patterns = ('//', '#', '*', '/*', '<!--', '"""', "'''")
        comment_lines = [l for l in non_empty_lines 
                        if l.strip().startswith(comment_patterns) or 
                        l.strip() in ('*/', '-->', '"""', "'''")]
        comment_ratio = len(comment_lines) / len(non_empty_lines)
        
        stub_indicators = ['not used', 'stub', 'placeholder', 'this file is', 'does nothing']
        has_stub_indicator = any(ind in vanilla_js_content.lower() for ind in stub_indicators)
        
        if comment_ratio > 0.8 or has_stub_indicator:
            logger.debug(f"[HTML Fix] script.js is stub (comment_ratio={int(comment_ratio*100)}%, stub_indicator={has_stub_indicator}), skipping")
            return all_files
        
        # React/Vueãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèªï¼ˆä¿®æ­£ãŒå¿…è¦ãªçŠ¶æ³ã‹ã©ã†ã‹ï¼‰
        framework_files = [f for f in all_files if f.endswith(('.jsx', '.tsx', '.vue'))]
        if not framework_files:
            logger.debug("[HTML Fix] No framework files found, no fix needed")
            return all_files
        
        # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ã—ã¦ä¿®æ­£
        import re
        modified_files = []
        
        for filename in all_files:
            if not filename.endswith('.html'):
                continue
            
            content = all_files[filename]
            original_content = content
            
            # React/Vueã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆã¸ã®å‚ç…§ãƒ‘ã‚¿ãƒ¼ãƒ³
            patterns_to_replace = [
                # <script src="app.js" type="module">...</script>
                (r'<script\s+src=["\']app\.js["\']\s+type=["\']module["\'][^>]*>\s*</script>', 
                 f'<script src="{vanilla_js_file}"></script>'),
                # <script type="module" src="app.js">...</script>
                (r'<script\s+type=["\']module["\']\s+src=["\']app\.js["\'][^>]*>\s*</script>', 
                 f'<script src="{vanilla_js_file}"></script>'),
                # <script src="./src/App.jsx" ...>...</script>
                (r'<script[^>]*src=["\']\.?/?src/[^"\']+\.jsx["\'][^>]*>\s*</script>', 
                 f'<script src="{vanilla_js_file}"></script>'),
            ]
            
            for pattern, replacement in patterns_to_replace:
                if re.search(pattern, content, re.IGNORECASE):
                    content = re.sub(pattern, replacement, content, flags=re.IGNORECASE)
            
            if content != original_content:
                all_files[filename] = content
                modified_files.append(filename)
                logger.debug(f"[HTML Fix] Modified {filename}: script reference -> {vanilla_js_file}")
        
        if modified_files:
            logger.debug(f"[HTML Fix] Fixed {len(modified_files)} HTML file(s): {modified_files}")
        
        return all_files

    def retry_full_validation(
        self,
        generated_code: Dict[str, str],
        goal: str,
        prev_lint_result: Optional[Dict[str, Any]] = None,
        attempt: int = 1,
        max_attempts: int = 3
    ) -> Tuple[Dict[str, str], Optional[Dict[str, Any]], Dict[str, float]]:
        """
        å…¨ãƒã‚§ãƒƒã‚¯å†å®Ÿè¡Œï¼ˆ[t] Try againç”¨ï¼‰
        
        Gãƒã‚§ãƒƒã‚¯(G-1ã€œG-31) â†’ Import Test â†’ Lint â†’ Quality â†’ ç·åˆãƒ¬ãƒ“ãƒ¥ãƒ¼
        ã®å…¨å·¥ç¨‹ã‚’å†å®Ÿè¡Œã™ã‚‹ã€‚StepHUDã§Code Generationã‚¹ã‚¿ã‚¤ãƒ«ã®é€²æ—è¡¨ç¤ºã€‚
        
        Args:
            generated_code: ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰
            goal: å…ƒã®ã‚´ãƒ¼ãƒ«
            prev_lint_result: å‰å›ã®Lintçµæœï¼ˆæ”¹å–„æ¤œå‡ºç”¨ãƒ»ã‚¨ãƒ©ãƒ¼æ™‚ã®è¿”å´ç”¨ï¼‰
            attempt: ç¾åœ¨ã®è©¦è¡Œå›æ•°ï¼ˆStepHUDã‚¿ã‚¤ãƒˆãƒ«ç”¨ï¼‰
            max_attempts: æœ€å¤§è©¦è¡Œå›æ•°ï¼ˆStepHUDã‚¿ã‚¤ãƒˆãƒ«ç”¨ï¼‰
        
        Returns:
            Tuple[updated_code, lint_result, quality_scores]
        """
        try:
            from rich.console import Console
            console = Console()
        except ImportError:
            console = err_console
        
        try:
            from cognix import hud_components as hud_comp
        except ImportError:
            class HudCompFallback:
                @staticmethod
                def lint_summary_line(ok, warn, fail):
                    from rich.text import Text
                    return Text.from_ansi(f"â“˜ Lint Summary   OK {ok} / WARN {warn} / FAIL {fail}")
            hud_comp = HudCompFallback()
        
        # ============================================
        # StepHUDåˆæœŸåŒ–ï¼ˆCode Generationã‚¹ã‚¿ã‚¤ãƒ«ï¼‰
        # ============================================
        hud = None
        try:
            from cognix.progress_zen import StepHUD
            steps = [
                "Consistency checks",
                "Import validation",
                "Lint check & Auto-fix",
                "Quality assessment",
                "Comprehensive Review",
            ]
            _hud = StepHUD(section_title="Retry Validation", steps=steps)
            _hud.start()
            hud = _hud  # start()æˆåŠŸå¾Œã«ã®ã¿ä»£å…¥
        except ImportError:
            logger.debug("[Retry Validation] StepHUD not available, using fallback display")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚·ãƒ³ãƒ—ãƒ«ãªé–‹å§‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
            console.print(f"{Icon.GEAR.value} Retry Validation...")
        except Exception as e:
            logger.debug(f"[Retry Validation] StepHUD init error: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚·ãƒ³ãƒ—ãƒ«ãªé–‹å§‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
            console.print(f"{Icon.GEAR.value} Retry Validation...")
        
        try:
            logger.debug("[Full Validation] Starting full validation retry...")
            logger.debug(f"\n{Icon.GEAR.value} Running full validation...")
            
            # â­ Zen HUDç”¨ã‚µãƒãƒªãƒ¼åˆæœŸåŒ–ï¼ˆretryç”¨ï¼‰
            self._zen_summary = {
                "lint": {"initial": 0, "final": 0, "fixed": False},
                "review": {"initial": 0, "final": 0, "fixed": False, "issues": []}
            }
            
            # ============================================
            # Step 1: Gãƒã‚§ãƒƒã‚¯ï¼ˆG-1ã€œG-31 å…¨ã¦å†å®Ÿè¡Œï¼‰
            # ============================================
            if hud:
                hud.mark("Consistency checks")
            logger.debug(f"  {Icon.GEAR.value} Running consistency checks...")
            
            MAX_FIX_ATTEMPTS = 2
            
            # G-1: HTML-CSS ã‚¯ãƒ©ã‚¹æ•´åˆæ€§
            missing_css_classes = self._validate_html_css_class_consistency(generated_code)
            if missing_css_classes:
                logger.debug(f"[G-1] Detected {len(missing_css_classes)} missing CSS classes")
                generated_code = self._auto_complete_missing_css_classes(missing_css_classes, generated_code)
            
            # G-2: HTML-JS IDæ•´åˆæ€§
            missing_html_ids = self._validate_html_js_id_consistency(generated_code)
            if missing_html_ids:
                logger.debug(f"[G-2] Detected {len(missing_html_ids)} missing HTML IDs")
                generated_code = self._auto_complete_missing_html_ids(missing_html_ids, generated_code)
            
            # G-3: Pythonç’°å¢ƒ-ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸äº’æ›æ€§
            compatibility_issues = self._validate_python_package_compatibility(generated_code)
            if compatibility_issues:
                logger.debug(f"[G-3] Detected {len(compatibility_issues)} compatibility issues")
                generated_code = self._auto_fix_package_compatibility(compatibility_issues, generated_code)
            
            # G-4: Pythonã‚·ãƒ³ãƒœãƒ«ãƒ¬ãƒ™ãƒ«importæ•´åˆæ€§
            symbol_issues = self._validate_python_symbol_imports(generated_code)
            if symbol_issues:
                logger.debug(f"[G-4] Detected {len(symbol_issues)} symbol import issues")
                generated_code = self._auto_fix_symbol_imports(symbol_issues, generated_code)
            
            # G-5: Import vs Requirements.txt æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
            requirements_issues = self._validate_import_requirements_consistency(generated_code)
            if requirements_issues:
                logger.debug(f"[G-5] Detected {len(requirements_issues)} missing packages in requirements.txt")
                generated_code = self._auto_fix_requirements_txt(requirements_issues, generated_code)
            
            # G-6: ãƒ©ã‚¤ãƒ–ãƒ©ãƒªè¦æ±‚ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
            library_issues = self._validate_library_requirements(generated_code)
            if library_issues:
                logger.debug(f"[G-6] Detected {len(library_issues)} library requirement issues")
                generated_code = self._auto_fix_library_requirements(library_issues, generated_code)
            
            # G-7: ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ä¾å­˜æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
            frontend_issues = self._validate_frontend_dependencies(generated_code)
            if frontend_issues:
                logger.debug(f"[G-7] Detected {len(frontend_issues)} frontend dependency issues")
                generated_code = self._auto_fix_frontend_dependencies(frontend_issues, generated_code)
            
            # G-8: Modelâ†”Schemaæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
            schema_issues = self._validate_model_schema_consistency(generated_code)
            if schema_issues:
                logger.debug(f"[G-8] Detected {len(schema_issues)} model-schema consistency issues")
                generated_code = self._auto_fix_model_schema_consistency(schema_issues, generated_code)
            
            # G-9: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç’°å¢ƒä¾å­˜ãƒã‚§ãƒƒã‚¯
            doc_issues = self._validate_document_environment_commands(generated_code)
            if doc_issues:
                logger.debug(f"[G-9] Detected {len(doc_issues)} environment-dependent commands")
                generated_code = self._auto_fix_document_environment_commands(doc_issues, generated_code)
            
            # G-10: ã‚¯ãƒ©ã‚¹å®šç¾©å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
            for attempt in range(MAX_FIX_ATTEMPTS):
                class_safety_issues = self._validate_class_definition_safety(generated_code)
                if not class_safety_issues:
                    break
                logger.debug(f"[G-10] Detected {len(class_safety_issues)} issues (attempt {attempt + 1})")
                generated_code = self._auto_fix_class_definition_safety(class_safety_issues, generated_code)
            
            # G-11: ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯äºˆç´„èªãƒã‚§ãƒƒã‚¯ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
            for attempt in range(MAX_FIX_ATTEMPTS):
                framework_reserved_issues = self._validate_framework_reserved_words(generated_code)
                if not framework_reserved_issues:
                    break
                logger.debug(f"[G-11] Detected {len(framework_reserved_issues)} issues (attempt {attempt + 1})")
                generated_code = self._auto_fix_framework_reserved_words(framework_reserved_issues, generated_code)
            
            # G-12: Pythonçµ„ã¿è¾¼ã¿åã‚·ãƒ£ãƒ‰ã‚¦ã‚¤ãƒ³ã‚°ãƒã‚§ãƒƒã‚¯ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
            for attempt in range(MAX_FIX_ATTEMPTS):
                builtin_issues = self._validate_python_builtin_shadowing(generated_code)
                if not builtin_issues:
                    break
                logger.debug(f"[G-12] Detected {len(builtin_issues)} issues (attempt {attempt + 1})")
                generated_code = self._auto_fix_python_builtin_shadowing(builtin_issues, generated_code)
            
            # G-13: Flask Blueprintãƒ«ãƒ¼ãƒˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
            for attempt in range(MAX_FIX_ATTEMPTS):
                route_duplicate_issues = self._validate_flask_route_duplicates(generated_code)
                if not route_duplicate_issues:
                    break
                logger.debug(f"[G-13] Detected {len(route_duplicate_issues)} issues (attempt {attempt + 1})")
                generated_code = self._auto_fix_flask_route_duplicates(route_duplicate_issues, generated_code)
            
            # G-14: SQLAlchemyãƒ†ãƒ¼ãƒ–ãƒ«åé‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
            for attempt in range(MAX_FIX_ATTEMPTS):
                table_duplicate_issues = self._validate_sqlalchemy_table_duplicates(generated_code)
                if not table_duplicate_issues:
                    break
                logger.debug(f"[G-14] Detected {len(table_duplicate_issues)} issues (attempt {attempt + 1})")
                generated_code = self._auto_fix_sqlalchemy_table_duplicates(table_duplicate_issues, generated_code)
            
            # G-15: JavaScriptäºˆç´„èªãƒã‚§ãƒƒã‚¯ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
            for attempt in range(MAX_FIX_ATTEMPTS):
                js_reserved_issues = self._validate_javascript_reserved_words(generated_code)
                if not js_reserved_issues:
                    break
                logger.debug(f"[G-15] Detected {len(js_reserved_issues)} issues (attempt {attempt + 1})")
                generated_code = self._auto_fix_javascript_reserved_words(js_reserved_issues, generated_code)
            
            # G-16: SQLAlchemy relationshipå‚ç…§å…ˆæ¤œè¨¼ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
            for attempt in range(MAX_FIX_ATTEMPTS):
                relationship_issues = self._validate_relationship_references(generated_code)
                if not relationship_issues:
                    break
                logger.debug(f"[G-16] Detected {len(relationship_issues)} issues (attempt {attempt + 1})")
                generated_code = self._auto_fix_relationship_references(relationship_issues, generated_code)
            
            # G-17: ForeignKeyå‚ç…§å…ˆæ¤œè¨¼ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
            for attempt in range(MAX_FIX_ATTEMPTS):
                foreignkey_issues = self._validate_foreignkey_references(generated_code)
                if not foreignkey_issues:
                    break
                logger.debug(f"[G-17] Detected {len(foreignkey_issues)} issues (attempt {attempt + 1})")
                generated_code = self._auto_fix_foreignkey_references(foreignkey_issues, generated_code)
            
            # G-18: SQLAlchemy Enumå®šç¾©æ¤œè¨¼ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
            for attempt in range(MAX_FIX_ATTEMPTS):
                enum_usage_issues = self._validate_sqlalchemy_enum_usage(generated_code)
                if not enum_usage_issues:
                    break
                logger.debug(f"[G-18] Detected {len(enum_usage_issues)} issues (attempt {attempt + 1})")
                generated_code = self._auto_fix_sqlalchemy_enum_usage(enum_usage_issues, generated_code)
            
            # G-19: Mappedå‹ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ•´åˆæ€§æ¤œè¨¼ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
            for attempt in range(MAX_FIX_ATTEMPTS):
                mapped_type_issues = self._validate_mapped_type_consistency(generated_code)
                if not mapped_type_issues:
                    break
                logger.debug(f"[G-19] Detected {len(mapped_type_issues)} issues (attempt {attempt + 1})")
                generated_code = self._auto_fix_mapped_type_consistency(mapped_type_issues, generated_code)
            
            # G-20: Flaské™çš„ãƒ‘ã‚¹æ¤œè¨¼ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
            for attempt in range(MAX_FIX_ATTEMPTS):
                flask_static_issues = self._validate_flask_static_paths(generated_code)
                if not flask_static_issues:
                    break
                logger.debug(f"[G-20] Detected {len(flask_static_issues)} issues (attempt {attempt + 1})")
                generated_code = self._auto_fix_flask_static_paths(flask_static_issues, generated_code)
            
            # G-21: HTMLå‚ç…§æ•´åˆæ€§æ¤œè¨¼ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
            for attempt in range(MAX_FIX_ATTEMPTS):
                html_asset_issues = self._validate_html_asset_references(generated_code)
                if not html_asset_issues:
                    break
                logger.debug(f"[G-21] Detected {len(html_asset_issues)} issues (attempt {attempt + 1})")
                generated_code = self._auto_fix_html_asset_references(html_asset_issues, generated_code)
            
            # G-22: JavaScriptã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ—ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œè¨¼ï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
            for attempt in range(MAX_FIX_ATTEMPTS):
                animation_issues = self._validate_js_animation_loop(generated_code)
                if not animation_issues:
                    break
                logger.debug(f"[G-22] Detected {len(animation_issues)} issues (attempt {attempt + 1})")
                generated_code = self._auto_fix_js_animation_loop(animation_issues, generated_code)
            
            # G-23: APIå¥‘ç´„æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ï¼ˆè­¦å‘Šã®ã¿ã€è‡ªå‹•ä¿®æ­£ãªã—ï¼‰
            api_contract_issues = self._validate_api_contract_consistency(generated_code)
            if api_contract_issues:
                logger.debug(f"[G-23] Detected {len(api_contract_issues)} API contract issues (warning only)")
            
            # G-24: back_populatesç›¸äº’å‚ç…§ãƒã‚§ãƒƒã‚¯
            for attempt in range(MAX_FIX_ATTEMPTS):
                bp_issues = self._validate_back_populates_consistency(generated_code)
                if not bp_issues:
                    break
                logger.debug(f"[G-24] Detected {len(bp_issues)} back_populates issues (attempt {attempt + 1})")
                generated_code = self._auto_fix_back_populates_consistency(bp_issues, generated_code)
            
            # G-25: ForeignKeyå‹æ³¨é‡ˆæ¬ å¦‚ãƒã‚§ãƒƒã‚¯
            for attempt in range(MAX_FIX_ATTEMPTS):
                fk_type_issues = self._validate_foreignkey_type_annotation(generated_code)
                if not fk_type_issues:
                    break
                logger.debug(f"[G-25] Detected {len(fk_type_issues)} ForeignKey type issues (attempt {attempt + 1})")
                generated_code = self._auto_fix_foreignkey_type_annotation(fk_type_issues, generated_code)
            
            # G-26: Mappedå‹ã¨SQLAlchemyå‹ã®ä¸ä¸€è‡´ãƒã‚§ãƒƒã‚¯
            for attempt in range(MAX_FIX_ATTEMPTS):
                mapped_mismatch_issues = self._validate_mapped_sqlalchemy_type_mismatch(generated_code)
                if not mapped_mismatch_issues:
                    break
                logger.debug(f"[G-26] Detected {len(mapped_mismatch_issues)} Mapped type mismatch issues (attempt {attempt + 1})")
                generated_code = self._auto_fix_mapped_sqlalchemy_type_mismatch(mapped_mismatch_issues, generated_code)
            
            # G-27: relationship overlapsæ¤œå‡ºï¼ˆè­¦å‘Šã®ã¿ï¼‰
            overlap_issues = self._validate_relationship_overlaps(generated_code)
            if overlap_issues:
                logger.debug(f"[G-27] Detected {len(overlap_issues)} relationship overlap issues (warning only)")
            
            # G-28: å¾ªç’°ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ¤œå‡ºï¼ˆè­¦å‘Šã®ã¿ï¼‰
            circular_import_issues = self._validate_circular_imports(generated_code)
            if circular_import_issues:
                logger.debug(f"[G-28] Detected {len(circular_import_issues)} circular import issues (warning only)")
            
            # G-29: Marshmallow Nestedå¾ªç’°å‚ç…§æ¤œå‡ºï¼ˆè­¦å‘Šã®ã¿ï¼‰
            nested_circular_issues = self._validate_marshmallow_nested_circular(generated_code)
            if nested_circular_issues:
                logger.debug(f"[G-29] Detected {len(nested_circular_issues)} Marshmallow nested circular issues (warning only)")
            
            # G-30: JavaScript thiså‚ç…§å–ªå¤±ãƒã‚§ãƒƒã‚¯
            for attempt in range(MAX_FIX_ATTEMPTS):
                this_issues = self._validate_js_this_context_loss(generated_code)
                if not this_issues:
                    break
                logger.debug(f"[G-30] Detected {len(this_issues)} this context issues (attempt {attempt + 1})")
                generated_code = self._auto_fix_js_this_context_loss(this_issues, generated_code)
            
            # G-31: Marshmallow APIèª¤ç”¨æ¤œå‡ºï¼ˆå†æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ä»˜ãï¼‰
            for attempt in range(MAX_FIX_ATTEMPTS):
                marshmallow_api_issues = self._validate_marshmallow_api_usage(generated_code)
                if not marshmallow_api_issues:
                    break
                logger.debug(f"[G-31] Detected {len(marshmallow_api_issues)} Marshmallow API issues (attempt {attempt + 1})")
                generated_code = self._auto_fix_marshmallow_api_usage(marshmallow_api_issues, generated_code)
            
            logger.debug(f"  {Icon.SUCCESS.value} Consistency checks completed")
            
            if hud:
                hud.complete("Consistency checks")
            
            # ============================================
            # Step 2: Runtime Import Test
            # ============================================
            if hud:
                hud.mark("Import validation")
            logger.debug(f"  {Icon.GEAR.value} Running import test...")
            
            import_success, import_error, failing_file = self._validate_runtime_import(generated_code)
            
            if import_success:
                if import_error and import_error.startswith("EXTERNAL_PACKAGE:"):
                    package_name = import_error.replace("EXTERNAL_PACKAGE:", "")
                    logger.debug(f"[Runtime Import] Skipped - external package: {package_name}")
                else:
                    logger.debug(f"  {Icon.SUCCESS.value} Import test passed")
            else:
                logger.debug(f"  {Icon.WARNING.value} Import error: {import_error}")
                # è‡ªå‹•ä¿®æ­£ã‚’è©¦è¡Œ
                for attempt in range(2):
                    fixed_code = self._fix_import_error_with_llm(generated_code, import_error, failing_file)
                    if fixed_code:
                        retry_success, retry_error, retry_file = self._validate_runtime_import(fixed_code)
                        if retry_success:
                            generated_code = fixed_code
                            logger.debug(f"  {Icon.SUCCESS.value} Import error fixed")
                            break
                        import_error = retry_error
                        failing_file = retry_file
                        generated_code = fixed_code
            
            if hud:
                hud.complete("Import validation")
            
            # ============================================
            # Step 3: Lint check & Auto-fix
            # ============================================
            if hud:
                hud.mark("Lint check & Auto-fix")
            logger.debug(f"  {Icon.GEAR.value} Running lint check...")
            
            lint_result = None
            try:
                from cognix.linter_integration import LinterIntegration
                
                linter = LinterIntegration()
                
                if linter.available_linters:
                    lint_result = linter.lint_generated_code(generated_code)
                    
                    if lint_result['has_errors']:
                        error_count = len(lint_result['errors'])
                        warning_count = len(lint_result['warnings'])
                        
                        # â­ Zen HUD: åˆæœŸLintã‚¨ãƒ©ãƒ¼æ•°ã‚’è¨˜éŒ²
                        self._zen_summary["lint"]["initial"] = error_count
                        
                        logger.debug("")
                        logger.debug(hud_comp.lint_summary_line(0, warning_count, error_count))
                        
                        if lint_result['can_auto_fix']:
                            fixed_files = self._fix_linter_errors_with_llm(
                                generated_code,
                                lint_result['errors'],
                                lint_result.get('file_languages', {})
                            )
                            
                            if fixed_files:
                                generated_code = fixed_files
                                logger.debug(f"  {Icon.GEAR.value} Linter errors auto-fixed")
                                
                                # å†ãƒã‚§ãƒƒã‚¯
                                recheck_result = linter.lint_generated_code(generated_code)
                                lint_result = recheck_result
                                
                                error_count_after = len(recheck_result.get('errors', []))
                                warning_count_after = len(recheck_result.get('warnings', []))
                                
                                # â­ Zen HUD: Auto-fixå¾Œã®Lintã‚¨ãƒ©ãƒ¼æ•°ã‚’è¨˜éŒ²
                                self._zen_summary["lint"]["final"] = error_count_after
                                self._zen_summary["lint"]["fixed"] = (error_count_after < error_count)
                                
                                logger.debug("")
                                logger.debug("  After auto-fix:")
                                logger.debug(hud_comp.lint_summary_line(0, warning_count_after, error_count_after))
                    else:
                        logger.debug(f"  {Icon.SUCCESS.value} Lint check passed")
                else:
                    logger.debug("[Linter] No external linters available")
            except ImportError:
                logger.debug("[Linter] LinterIntegration not available")
            except Exception as e:
                logger.debug(f"[Linter] Error: {e}")
            
            if hud:
                hud.complete("Lint check & Auto-fix")
            
            # ============================================
            # Step 4: Quality check & fix
            # ============================================
            if hud:
                hud.mark("Quality assessment")
            logger.debug(f"  {Icon.GEAR.value} Checking code quality...")
            
            quality_scores = self._assess_code_quality(
                generated_code,
                "medium",
                goal,
                lint_result=lint_result
            )
            
            # Quality issuesæ¤œå‡º
            quality_issues = []
            for filename, score in quality_scores.items():
                if score < 0.70:
                    code = generated_code.get(filename, '')
                    ext = filename.split('.')[-1]
                    placeholder_count, detected_lines = self._detect_placeholders_in_comments(code, ext)
                    
                    if placeholder_count > 0:
                        quality_issues.append({
                            'filename': filename,
                            'score': score,
                            'issue_type': 'placeholder',
                            'count': placeholder_count,
                            'details': detected_lines[:3]
                        })
            
            # Qualityä¿®æ­£
            if quality_issues:
                logger.debug(f"  {Icon.WARNING.value} Found {len(quality_issues)} quality issue(s)")
                
                fixed_files = self._fix_quality_issues_with_llm(generated_code, quality_issues)
                
                if fixed_files:
                    generated_code = fixed_files
                    logger.debug(f"  {Icon.SUCCESS.value} Quality issues fixed")
                    
                    # å†è©•ä¾¡
                    quality_scores = self._assess_code_quality(
                        generated_code,
                        "medium",
                        goal,
                        lint_result=lint_result
                    )
            else:
                logger.debug(f"  {Icon.SUCCESS.value} Quality check passed")
            
            if hud:
                hud.complete("Quality assessment")
            
            # ============================================
            # Step 5: ç·åˆãƒ¬ãƒ“ãƒ¥ãƒ¼
            # ============================================
            if hud:
                hud.mark("Comprehensive Review")
            logger.debug(f"  {Icon.GEAR.value} Running comprehensive review...")
            logger.debug(f"      This may take 2-5 minutes for complex projects.")
            
            generated_code = self._final_comprehensive_review(generated_code, goal)
            
            # ğŸ†• Comprehensive Reviewå¾Œã®HTMLå‚ç…§ä¿®æ­£ï¼ˆã‚¹ã‚¿ãƒ–å†ç”Ÿæˆå¾Œã«å¿…è¦ï¼‰
            if getattr(self, '_no_framework_mode', False):
                logger.debug("[Post-Review] Checking HTML script references after comprehensive review...")
                generated_code = self._fix_html_script_reference_for_vanilla_js(generated_code)
            
            logger.debug(f"  {Icon.SUCCESS.value} Comprehensive review completed")
            
            if hud:
                hud.complete("Comprehensive Review")
            
            # ============================================
            # æœ€çµ‚Qualityå†è©•ä¾¡
            # ============================================
            quality_scores = self._assess_code_quality(
                generated_code,
                "medium",
                goal,
                lint_result=lint_result
            )
            
            logger.debug(f"\n{Icon.SUCCESS.value} Full validation completed")
            
            # StepHUDçµ‚äº†ï¼ˆæˆåŠŸï¼‰- Quality Scoresè¡¨ç¤ºå‰ã«çµ‚äº†
            if hud:
                hud.finish(success=True)
            
            # Qualityè¡¨ç¤ºï¼ˆLint/Reviewæƒ…å ±ã‚’åæ˜ ã—ãŸç·åˆè©•ä¾¡ï¼‰
            if quality_scores:
                console.print()
                
                # Lintæƒ…å ±ã‚’å–å¾—
                lint_errors = len(lint_result.get('errors', [])) if lint_result else 0
                lint_warnings = len(lint_result.get('warnings', [])) if lint_result else 0
                
                # Reviewæƒ…å ±ã‚’å–å¾—ï¼ˆ_zen_summaryã‹ã‚‰ï¼‰
                review_issues = 0
                if hasattr(self, '_zen_summary') and self._zen_summary:
                    review_issues = self._zen_summary.get("review", {}).get("final", 0)
                
                # ç·åˆè©•ä¾¡ã‚’è¨ˆç®—ãƒ»è¡¨ç¤º
                overall_score, quality_display = self._format_quality_display(
                    base_quality_scores=quality_scores,
                    lint_errors=lint_errors,
                    lint_warnings=lint_warnings,
                    review_issues=review_issues
                )
                
                # ç·åˆè©•ä¾¡ã‚’è¡¨ç¤º
                console.print(quality_display)
                console.print()  # ç©ºè¡Œ
                
                # å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¹ã‚³ã‚¢ã‚‚è¡¨ç¤ºï¼ˆè¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã®ã¿ï¼‰
                if len(quality_scores) > 1:
                    from rich.text import Text
                    console.print("Quality Scores:")
                    for filename, score in quality_scores.items():
                        score_percent = int(score * 100)
                        # è‰²ã¨ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ¤å®šï¼ˆcli_chat_workflow.pyã¨çµ±ä¸€ï¼‰
                        if score >= 0.9:
                            color = "\033[92m"  # GREEN
                            grade = "Excellent"
                        elif score >= 0.75:
                            color = "\033[92m"  # GREEN
                            grade = "Good"
                        elif score >= 0.6:
                            color = "\033[93m"  # YELLOW
                            grade = "Fair"
                        else:
                            color = "\033[93m"  # YELLOW
                            grade = "Needs Review"
                        reset = "\033[0m"
                        console.print(Text.from_ansi(f"  {color}{filename}{reset}: {score_percent}% {color}{grade}{reset}"))
                    console.print()
            
            return (generated_code, lint_result, quality_scores)
        
        except ImportError as ie:
            # ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼æ™‚ã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
            import sys
            import traceback
            
            logger.error(f"[Full Validation] ImportError details:")
            logger.error(f"  Error message: {str(ie)}")
            logger.error(f"  Module name: {getattr(ie, 'name', 'unknown')}")
            logger.error(f"  Traceback:")
            for line in traceback.format_tb(sys.exc_info()[2]):
                logger.error(f"    {line.strip()}")
            
            logger.debug(f"{Icon.WARNING.value} Full validation failed: Import error")
            logger.debug(f"[DEBUG] Import failed: {ie}")
            
            # StepHUDçµ‚äº†ï¼ˆå¤±æ•—ï¼‰
            if hud:
                hud.finish(success=False)
            
            return (generated_code, prev_lint_result, {})
        
        except Exception as e:
            # ä¸€èˆ¬çš„ãªã‚¨ãƒ©ãƒ¼æ™‚ã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
            import traceback
            
            logger.error(f"[Full Validation] Error during validation: {e}")
            logger.error(f"  Traceback:\n{traceback.format_exc()}")
            
            logger.debug(f"{Icon.ERROR.value} Full validation error: {e}")
            
            # StepHUDçµ‚äº†ï¼ˆå¤±æ•—ï¼‰
            if hud:
                hud.finish(success=False)
            
            return (generated_code, prev_lint_result, {})


# Utility functions for CLI integration
def create_engine(workspace_path: Optional[str] = None) -> SemiAutoEngine:
    """Create a new SemiAutoEngine instance"""
    return SemiAutoEngine(workspace_path)


def process_chat_message(engine: SemiAutoEngine, message: str, auto_apply: bool = False) -> Dict[str, Any]:
    """Process a chat message through the engine"""
    return engine.process_message(message, auto_apply)